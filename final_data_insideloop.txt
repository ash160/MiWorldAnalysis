{'namespace': 'common_models.common_api_lib', 'created_by': 'sys', 'source_code': 'class GenClass:\n    allow_function =[\'*\']\n    backend_func = None\n    function_dict= {}\n    network_interface = ""\n    url_type = \'ui\'\n    thread_manager_obj = None\n    loop_index = None\n    \n    def __init__(self,thread_manager_obj=None,loop_index=None):\n        #print(\'haiigen\')\n        if thread_manager_obj is not None and loop_index is not None:\n            self.thread_manager_obj = thread_manager_obj\n            self.loop_index = loop_index\n\n    def create_router(self):\n        r = router()\n        temp_url = []\n        \n        if self.allow_function[0] ==\'*\':\n            for x in self.function_dict:\n                if len(self.function_dict[x]) ==2:\n                    temp_url.append([x,self.function_dict[x][1],getattr(self,self.function_dict[x][0]),\'json\',4]) #### security layer 4 for all layer security, json type is default data type\n                else:\n                    temp_url.append([x,self.function_dict[x][1],getattr(self,self.function_dict[x][0]),self.function_dict[x][2],self.function_dict[x][3]])\n\n\n        else:\n            for x in self.allow_function:\n                if len(self.function_dict[x]) ==2:\n                    temp_url.append([x,self.function_dict[x][1],getattr(self,self.function_dict[x][0]),\'json\',4]) \n                else:\n                    temp_url.append([x,self.function_dict[x][1],getattr(self,self.function_dict[x][0]),self.function_dict[x][2],self.function_dict[x][3]])\n\n        r.urls = temp_url\n        return r\n\n\nclass router:\n    """ router class """\n    urls = [] ### [regx, [method,rout_function]|router ] {name:\\d+}\n    dict_url ={}\n    buffer_size = 1000000000000\n    #__call=False\n\n    #def __init__(selfs):\n     #   self.urls = urls\n      #  self.get_urls()\n       # __call=True\n\n    def get_urls(self,sep=\'/\'):\n        actual_url = []\n        #if self.__call:\n         #   return copy.deepcopy(self.urls)\n        u = self.urls\n        for x in u:\n            if len(x) == 2:\n                if type(x[1]) is router:\n                    temp_urls = x[1].get_urls() ### Get all url of the model\n                    for y in range(len(temp_urls)):\n                        temp_urls[y][0] = sep.join([x[0],temp_urls[y][0]]) ### Get url from map root\n                    actual_url = actual_url + temp_urls\n                else:\n                    return False\n            else:\n                actual_url.append(x)\n        self.urls = actual_url\n        #print(self.urls)\n        return actual_url\n\n    def create_dict(self):\n        """Converts url list into dictionary"""\n\n        temp_urls = self.urls\n        dict_url = {}\n        for x in temp_urls:\n            dict_url[x[0]] = {"method":x[1],"callback":x[2],"addition_parameters":[x[3],x[4]]}\n        self.dict_url = dict_url\n    \n    def get_urls_list(self):\n        urls = self.urls\n        url_list = []\n        for x in urls:\n            url_list.append(x)\n        \n        return  url_list\n\nclass BackendProcess:\n    fun_list = None\n    allow_fun = [\'*\']\n\n    def get_services(self):\n        tmp={}\n        if self.allow_fun[0]==\'*\':\n            for x in self.fun_list:\n                tmp[x]=getattr(self,x)\n        return tmp\t\n\n\ndef get_ip(request):\n        client_address = None\n        try:\n            # case server 200.000.02.001\n            client_address = request.META["HTTP_X_FORWARDED_FOR"]\n        except:\n            # case localhost ou 127.0.0.1\n            client_address = request.META["REMOTE_ADDR"]\n        return client_address\n\n', 'created_at': '2020-11-09 12:41:54.080748', 'time_stamp': 1604905914.080766, 'import_parameters': '{}'}
{'namespace': 'common_models.common_count_viewset', 'created_by': 'sys', 'source_code': '#import uuid\n\n#import datetime\n\n#from db_manager_viewset import cassandra_manager_obj, cache_manager_obj\n#from cache_manager import cache_manager_obj\n#from cassandra_manager import cassandra_manager\n#from post_gras_manager import post_gras_manager\n\nclass CommonCount(ViewSetMaster):\n    permission_class = ParmissionChacker\n    lookup_field =\'user\'\n    loop_index = None\n    pk = \'pk\'\n    serializer_class = None #Serializer\n    require_user_mapping = False\n    user_fild = None\n    model_name = ""\n    tmp_cache=None\n    db_manager_class = None #DB_client\n    db_manager_client = None\n    allow_function =[\'*\']\n    permission_list = [\'read\',\'write\',\'edit\',\'delete\']\n    function_dict= {\n            \'list\':[\'list\',\'get\'],\n            \'create\':[\'create\',\'post\'],\n            \'retrive\':[\'retrive\',\'get\'],\n            \'update\':[\'update\',\'put\'],\n            \'destroy\':[\'destroy\',\'post\'],\n            \'partial_update\':[\'partial_update\',\'put\']\n            }\n\n\n    def id_generetor(user):\n        return user+str(datetime.datetime.timestamp())\n\n    def __init__(self,thread_manager_obj,loop_index):\n        print(\'haiigen\')\n        self.permission_obj = self.permission_class(thread_manager_obj,loop_index)\n        self.thread_manager_obj = thread_manager_obj\n        self.loop_index = loop_index\n        self.db_manager_client = self.db_manager_class(thread_manager_obj,loop_index)\n        #self.permission_mape\n\n\n\n\n    async def create(self,request):\n        if self.permission_obj.has_permission(request.user, self.model_name,self.permission_mape[\'write\']):\n            data = request.data\n            serializer_data = self.serializer_class(data=data)\n            if serializer_data.is_valid():\n                rs = await self.db_manager_client.create(data)\n                return (request.data, 201)\n            else:\n                return  (request.data, 406)\n\n        else:\n            return "Invalid Authorization",400\n\n\n    async def retrive(request, pk=None):\n        if self.permission_obj.has_permission(request.user, self.model_name,self.permission_mape[\'read\']):\n            if pk is not None:\n                if self.permission_obj.has_object_permission(request.user, pk,self.permission_mape[\'read\']):\n                    data = await self.db_manager_client.retrive(request,pk)\n                    return data,201\n                else:\n                    return \'Invalid Authorization for the item\',400\n        else:\n            return "Invalid Authorization",400\n\n    async def update(self,request, pk=None):\n        if self.permission_obj.has_permission(request.user, self.model_name,self.permission_mape[\'edit\']):\n            if pk is None:\n                return  (request.data, 400)\n            else:\n                if self.permission_obj.has_object_permission(request.user, pk,self.permission_mape[\'edit\']):\n                    data = request.data\n                    serializer_data = self.serializer_class(data=data)\n                    if serializer_data.is_valid():\n                        rs = await self.db_manager_client.update(request,pk)\n                        return (rs, 201)\n                else:\n                    return "Invalid Authorization for the item",400\n                else:\n                    return (rs, 406)\n\n        else:\n            return "Invalid Authorization",400\n\n    async def partial_update(self,request, pk=None):\n        return await self.update(request,pk)\n\n    async def destroy(request, pk=None):\n        if self.permission_obj.has_permission(request.user, self.model_name,self.permission_mape[\'delete\']):\n            if self.permission_obj.has_object_permission(request.user, pk,self.permission_mape[\'delete\']):\n                if pk is None:\n                    return  (request.data, 400)\n                else:\n                    rs = self.db_manager_client.destroy(request,pk)\n                    res1 = rs[\'row_effected\']\n                    if res1 > 0:\n                        return (request.data, 201)\n                    else:\n                        return (request.data, 406)\n\n            else:\n                return "Invalid Authorization for the item",400\n\n        else:\n            return "Invalid Authorization",400\n\n   # def create_router():\n   #     r = router()\n   #     temp_url = []\n   #     for x in self.function_dict:\n   #         temp_url.append(self.function_dict[x][0],self.function_dict[x][1],getattr(self,x))\n   #     r.urls = temp_url\n   #     return r\n', 'created_at': '2020-11-09 12:41:54.083914', 'time_stamp': 1604905914.0839314, 'import_parameters': '{"common_api_lib": ["router,GenClass", "custom"], "db_manager_client": ["DBClient", "custom"], "permission_model.permission_checker": ["ParmissionChacker", "custom"], "datetime": ["sys"], "serializers": ["Serializer", "custom"], "common_model.async_viewset": ["ViewSetMaster", "custom"]}'}
{'namespace': 'common_models.common_id_generator', 'created_by': 'sys', 'source_code': "def id_generetor(parameters,joint_str = ''): #### []\n        return joint_str.join(parameters)#user+str(datetime.datetime.timestamp())\n", 'created_at': '2020-11-09 12:41:54.086604', 'time_stamp': 1604905914.0866241, 'import_parameters': '{}'}
{'namespace': 'common_models.async_viewset', 'created_by': 'sys', 'source_code': '#import uuid\n\n#import datetime\n\n#from db_manager_viewset import cassandra_manager_obj, cache_manager_obj\n#from cache_manager import cache_manager_obj\n#from cassandra_manager import cassandra_manager\n#from post_gras_manager import post_gras_manager\n\n\n\nclass ViewSetMaster(GenClass):\n    permission_class = ParmissionChacker\n    lookup_field =\'user\'\n    loop_index = None\n    pk = \'pk\'\n    serializer_class = None #Serializer\n    require_user_mapping = False\n    user_fild = None\n    model_name = ""\n    tmp_cache=None\n    db_manager_class = None #DB_client\n    db_manager_client = None\n    allow_function =[\'*\']\n    permission_list = [\'read\',\'write\',\'edit\',\'delete\']\n    function_dict= {\n            \'list\':[\'list\',\'get\'],\n            \'create\':[\'create\',\'post\'],\n            \'retrive\':[\'retrive\',\'get\'],\n            \'update\':[\'update\',\'put\'],\n            \'destroy\':[\'destroy\',\'post\'],\n            \'partial_update\':[\'partial_update\',\'put\']\n            }\n\n\n    def id_generetor(user):\n        return user+str(datetime.datetime.timestamp())\n\n    def __init__(self,thread_manager_obj,loop_index):\n        self.permission_obj = self.permission_class(thread_manager_obj,loop_index)\n        self.thread_manager_obj = thread_manager_obj\n        self.loop_index = loop_index\n        self.db_manager_client = self.db_manager_class(thread_manager_obj,loop_index)\n        #self.permission_mape\n\n    async def list(self, request):\n\n        if await self.permission_obj.has_permission(request.user, self.model_name,\'read\'):\n            data = await self.db_manager_client.list(request)\n            return data,200\n        else:\n            return "Invalid Authorization",400\n\n\n    async def create(self,request):\n        user = request.user\n\n        if await self.permission_obj.has_permission(user, self.model_name,\'write\'):\n            id = self.id_generetor(user)\n            data = request.data\n            data[self.pk] = id\n            serializer_data = self.serializer_class(data=data)\n            if serializer_data.is_valid():\n\n                self.permission_obj.assing_permission(user,self.model,id,\'*\',\'*\',True)\n                rs = await self.db_manager_client.create(request[\'data\'])\n                return (request.data, 201)\n            else:\n                return  (request.data, 406)\n\n        else:\n            return "Invalid Authorization",400\n\n\n    async def retrive(request, pk=None):\n        if self.permission_obj.has_permission(request.user, self.model_name,\'read\'):\n            if pk is not None:\n                if self.permission_obj.has_object_permission(request.user, pk,\'read\'):\n                    data = await self.db_manager_client.retrive(request,pk)\n                    return data,201\n                else:\n                    return \'Invalid Authorization for the item\',400\n        else:\n            return "Invalid Authorization",400\n\n    async def update(self,request, pk=None):\n        if self.permission_obj.has_permission(request.user, self.model_name,\'edit\'):\n            if pk is None:\n                return  (request.data, 400)\n            else:\n                if self.permission_obj.has_object_permission(request.user, pk,\'edit\'):\n                    data = request.data\n                    serializer_data = self.serializer_class(data=data)\n                    if serializer_data.is_valid():\n                        rs = await self.db_manager_client.update(request,pk)\n                        return (rs, 201)\n                else:\n                    return "Invalid Authorization for the item",400\n                else:\n                    return (rs, 406)\n\n        else:\n            return "Invalid Authorization",400\n\n    async def partial_update(self,request, pk=None):\n        return await self.update(request,pk)\n\n    async def destroy(request, pk=None):\n        if self.permission_obj.has_permission(request.user, self.model_name,\'delete\'):\n            if self.permission_obj.has_object_permission(request.user, pk,\'delete\'):\n                if pk is None:\n                    return  (request.data, 400)\n                else:\n                    rs = self.db_manager_client.destroy(request,pk)\n                    res1 = rs[\'row_effected\']\n                    if res1 > 0:\n                        return (request.data, 201)\n                    else:\n                        return (request.data, 406)\n\n            else:\n                return "Invalid Authorization for the item",400\n\n        else:\n            return "Invalid Authorization",400\n\n   # def create_router():\n   #     r = router()\n   #     temp_url = []\n   #     for x in self.function_dict:\n   #         temp_url.append(self.function_dict[x][0],self.function_dict[x][1],getattr(self,x))\n   #     r.urls = temp_url\n   #     return r\n', 'created_at': '2020-11-09 12:41:54.089435', 'time_stamp': 1604905914.0894487, 'import_parameters': '{"common_api_lib": ["router,GenClass", "custom"], "db_manager_client": ["DBClient", "custom"], "permission_model.permission_checker": ["ParmissionChecker", "custom"], "datetime": ["sys"], "serializers": ["Serializer", "custom"]}'}
{'namespace': 'network_models.socket_manager', 'created_by': 'sys', 'source_code': '#from event_loop_manager import event_loop_manager_obj\n\n#CLIENT_TIMEOUT = 60\nclass SocketManagerClass:\n    #session_list =None\n    \n    timeout = aiohttp.ClientTimeout(total=NETWORK_CLIENT_TIME_OUT)\n\n    session_obj_list = {}\n    session_obj_count = {}\n    current_port = 8888\n    \n    loop_list = []\n    network_interface = {} ### {\'ip_address\':}\n    \n\n    MAXIMUM_NUM_SESSION = 3\n    MINIMUM_NUM_SESSION = 1\n\n    def __init__(self,thread_obj=None):\n        if thread_obj is not None:\n            self.loop_list = thread_obj.loop_list\n\n\n\n    \n    def get_session_obj(self,loop_index,interface): ## interface  = ["KAFKA","kafka_br01"]\n\n        if self.session_obj_list[loop_index] is None:\n            #print(\'haii\')\n\n            self.session_obj_list[loop_index] = {interface:[]}\n\n        try:\n            self.session_obj_list[loop_index][interface]\n        except:\n            self.session_obj_list[loop_index][interface] =[]\n\n            \n\n        if self.session_obj_list[loop_index][interface] == []: \n            if self.session_obj_count[loop_index] < self.MAXIMUM_NUM_SESSION:\n                if interface==\'*\':\n                    conn = aiohttp.TCPConnector( loop=self.loop_list[loop_index])\n                else:\n                    conn = aiohttp.TCPConnector(local_addr=(interface, self.current_port), loop=self.loop_list[loop_index])\n                self.current_port +=1\n                session_obj = aiohttp.ClientSession(loop = self.loop_list[loop_index], timeout = self.timeout,connector=conn )\n                self.session_obj_count[loop_index] += 1\n                return session_obj\n            else:\n                return False\n        else:\n            m =self.session_obj_list[loop_index][interface].pop() \n            if m.closed:\n                return self.get_session_obj(loop_index,interface)\n            else:\n                return m\n\n\n\n\n    def register_loop(self,loop_index):\n        self.session_obj_list[loop_index] = None\n        self.session_obj_count[loop_index] = 0\n\n    async def send_single_request(self,loop_index,interface,request_type, request_format, request_url, request_parameter = None, return_function = None, return_parameter = None, buffer_size = None,is_socket=True):\n        so = self.get_session_obj(loop_index,interface)\n        r = self.register_request(request_type, request_format, request_url, request_parameter = request_parameter, return_function = return_function, return_parameter = return_parameter, buffer_size = buffer_size,is_socket=is_socket)\n        d = await self.run_request(so,r) \n        \n        self.release_session_obj(loop_index,interface,so)\n        return d\n\n\n\n    def register_request(self, request_type, request_format, request_url, request_parameter = None, return_function = None, return_parameter = None, buffer_size = None,is_socket=True):\n\n        ### Here user is going to resister the request ###\n        ### it may be get or post ###\n        request_json_meta_data = {"request_type" : request_type, "request_format" : request_format, "request_parameter" : request_parameter,\n                "request_url" : request_url,\n                "return_function" : return_function, "buffer_size" : buffer_size,\n                "return_parameter" : return_parameter, "status" : "PENDING", "functionality_type" : "socket" if is_socket else "UI"}\n        return request_json_meta_data\n        \n\n\n    def request_obj(self, session_obj, request_type):\n        if request_type == "put":\n            return session_obj.put\n        elif request_type == "delete":\n            return session_obj.delete\n        elif request_type == "head":\n            return session_obj.head\n        elif request_type == "options":\n            return session_obj.options\n        elif request_type == "patch":\n            return session_obj.patch\n        elif request_type == "post":\n            return session_obj.post\n        elif request_type == "get":\n            return session_obj.get\n        elif request_type =="socket":\n            return session_obj.ws_connect\n\n    def request_method(self, return_obj, request_format):\n        if request_format == "json":\n            return return_obj.json\n        elif request_format == "text":\n            return return_obj.text\n        elif request_format == "byte":\n            return return_obj.read\n        elif request_format == "buffer":\n            return return_obj.content.read\n\n    \n\n\n\n\n    async def run_request(self, session_ob, metadata):\n        \n        """ \n        At starting the request status of the user request is PENDING\n        At processing time the request status is set to RUNNING\n        After complete request status is set to DONE\n        """\n        #print(\'haiii run request\')\n        \n        metadata["status"] = "RUNNING"\n        method  = metadata["request_type"]\n        if metadata["buffer_size"] is not None:\n            return await buffer_call_func(self,session_ob,metadata)\n        else:\n            result = []\n            #print(\'in socket\')\n            result = await call_func(self,session_ob,metadata)\n            #async for x in call_func(self,session_ob,metadata):\n            #    result.append(x)\n\n            return result\n\n                  \n      \n\n    def release_session_obj(self,loop_index,interface,session):\n        self.session_obj_list[loop_index][interface].append(session)\n        \n#socket_manager_obj = SocketManagerClass()\n\n', 'created_at': '2020-11-09 12:41:54.092369', 'time_stamp': 1604905914.0923848, 'import_parameters': '{"config": ["NETWORK_INTERFACE,NETWORK_CLIENT_TIME_OUT", "custom"], "network_models.buffer_call_process": ["buffer_call_func", "custom"], "network_models.call_process": ["call_func", "custom"], "aiohttp": ["sys"]}'}
{'namespace': 'network_models.socket_manager (copy 1)', 'created_by': 'sys', 'source_code': '#from event_loop_manager import event_loop_manager_obj\n\n#CLIENT_TIMEOUT = 60\nclass SocketManagerClass:\n    #session_list =None\n    \n    timeout = aiohttp.ClientTimeout(total=NETWORK_CLIENT_TIME_OUT)\n\n    session_obj_list = {}\n    session_obj_count = {}\n    current_port = 0\n    \n    loop_list = []\n    \n\n    MAXIMUM_NUM_SESSION = 3\n    MINIMUM_NUM_SESSION = 1\n\n    def __init__(self,thread_obj=None):\n        if thread_obj is not None:\n            self.loop_list = thread_obj.loop_list\n\n\n\n    \n    def get_session_obj(self,loop_index,interface): ## interface  = ["KAFKA","kafka_br01"]\n\n        if self.session_obj_list[loop_index] is None:\n            #print(\'haii\')\n\n            self.session_obj_list[loop_index] = {interface[0]:{interface[1]:[]}}\n            \n\n        if self.session_obj_list[loop_index][interface[0]][interface[1]] == []: \n            if self.session_obj_count[loop_index] < self.MAXIMUM_NUM_SESSION:\n                \n                conn = aiohttp.TCPConnector(local_addr=(NETWORK_INTERFACE[interface[0]][interface[1]], self.current_port), loop=self.loop_list[loop_index])\n                self.current_port +=1\n                session_obj = aiohttp.ClientSession(loop = self.loop_list[loop_index], timeout = self.timeout,connector=conn )\n                self.session_obj_count[loop_index] += 1\n                return session_obj\n            else:\n                return False\n        else:\n            return self.session_obj_list[loop_index][interface[0]][interface[1]].pop() \n\n\n\n\n    def register_loop(self,loop_index):\n        self.session_obj_list[loop_index] = None\n        self.session_obj_count[loop_index] = 0\n\n    def register_request(self, request_type, request_format, request_url, request_parameter = None, return_function = None, return_parameter = None, buffer_size = None,is_socket=False):\n\n        ### Here user is going to resister the request ###\n        ### it may be get or post ###\n        request_json_meta_data = {"request_type" : request_type, "request_format" : request_format, "request_parameter" : request_parameter,\n                "request_url" : request_url,\n                "return_function" : return_function, "buffer_size" : buffer_size,\n                "return_parameter" : return_parameter, "status" : "PENDING", "functionality_type" : "socket" if is_socket else "UI"}\n        return request_json_meta_data\n        \n\n\n    def request_obj(self, session_obj, request_type):\n        if request_type == "put":\n            return session_obj.put\n        elif request_type == "delete":\n            return session_obj.delete\n        elif request_type == "head":\n            return session_obj.head\n        elif request_type == "options":\n            return session_obj.options\n        elif request_type == "patch":\n            return session_obj.patch\n        elif request_type == "post":\n            return session_obj.post\n        elif request_type == "get":\n            return session_obj.get\n        elif request_type =="socket":\n            return session_obj.ws_connect\n\n    def request_method(self, return_obj, request_format):\n        if request_format == "json":\n            return return_obj.json\n        elif request_format == "text":\n            return return_obj.text\n        elif request_format == "byte":\n            return return_obj.read\n        elif request_format == "buffer":\n            return return_obj.content.read\n\n    \n\n\n\n\n    async def run_request(self, session_ob, metadata):\n        \n        """ \n        At starting the request status of the user request is PENDING\n        At processing time the request status is set to RUNNING\n        After complete request status is set to DONE\n        """\n        print(\'haiii run request\')\n        \n        metadata["status"] = "RUNNING"\n        method  = metadata["request_type"]\n\n        async with session_ob:\n            if metadata["status"] == "DONE":\n                return\n            if method in ["post", "put", "patch"]:\n                if metadata["request_format"] == "json":\n                    if metadata["functionality_type"] != "socket":\n                        if metadata["buffer_size"] is None:\n                            v = await self.request_obj(session_ob, method)(metadata["request_url"], json = metadata["request_parameter"])\n                        else:\n                            data = json.dumps(metadata["request_parameter"])\n                            if len(data) > metadata["buffer_size"]:\n                                    for x in range(len(metadata)/metadata["buffer_size"]):\n                                        v = await self.request_obj(session_ob, method)(metadata["request_url"], data = data[x*metadata["buffer_size"]:(x+1)*metadata["buffer_size"]])\n                            else:\n                                v = await self.request_obj(session_ob, method)(metadata["request_url"], data = data[x*metadata["buffer_size"]:(x+1)*metadata["buffer_size"]])\n                    else:\n                        data = ujson.dumps(metadata["request_parameter"])\n                        data = data.encode()\n                        if metadata["buffer_size"] is None:\n                            async with request_obj(session_ob, method)(metadata["request_url"]) as ws:\n                                await ws.send_str(data)\n                                await ws.send_str("in_close")\n                                async for msg in ws:\n                                    if msg.type == aiohttp.WSMsgType.ERROR:\n                                        break\n\n                                    if msg.type == aiohttp.WSMsgType.TEXT:\n                                        if msg.data == \'out_close\':\n                                            await ws.close()\n                                            break\n                                    v = msg.data\n\n                                    \n                        else:\n                            if len(data) <= metadata["buffer_size"]:\n                                async with request_obj(session_ob, method)(metadata["request_url"],max_msg_size = metadata["buffer_size"]) as ws:\n                                    await ws.send_str(data)\n                                    await ws.send_str("in_close")\n                                    await ws.close()\n                            else:\n                                async with request_obj(session_ob, method)(metadata["request_url"],max_msg_size = metadata["buffer_size"]) as ws:\n                                    for x in range(len(data)/metadata["buffer_size"]):\n                                        await ws.send_str(data)\n                                        await ws.send_str("in_close")\n                                        async for msg in ws:\n                                            if msg.type == aiohttp.WSMsgType.ERROR:\n                                                break\n\n                                            if msg.type == aiohttp.WSMsgType.TEXT:\n                                                if msg.data == \'out_close\':\n                                                    await ws.close()\n                                                    break\n                                            v = msg.data\n\n                                        try:\n                                            await ws.close()\n                                        except:\n                                            pass\n                \t\t\n                else:\n                    if metadata["functionality_type"] != "socket":\n                        if metadata["buffer_size"] is None:\n                            v = await self.request_obj(session_ob, method)(metadata["request_url"], data = metadata["request_parameter"])\n                        else:\n                            if len(metadata["request_parameter"])< metadata["buffer_size"]:\n                                    v = await self.request_obj(session_ob, method)(metadata["request_url"], data = metadata["request_parameter"])\n                            else:\n                                for x in range(len(metadata["request_parameter"])/metadata["buffer_size"]):\n                                    v = await self.request_obj(session_ob, method)(metadata["request_url"], data = metadata["request_parameter"][x*metadata["buffer_size"]:(x+1)*metadata["buffer_size"]])\n\n                    else:\n                        data = (metadata["request_parameter"]).encode()\n\n                        if metadata["buffer_size"] is None:\n                            async with request_obj(session_ob, method)(metadata["request_url"]) as ws:\n                                await ws.send_str(data)\n                                await ws.send_str("in_close")\n                                async for msg in ws:\n                                    if msg.type == aiohttp.WSMsgType.ERROR:\n                                        break\n\n                                    if msg.type == aiohttp.WSMsgType.TEXT:\n                                        if msg.data == \'out_close\':\n                                            await ws.close()\n                                            break\n                                    v = msg.data\n\n                                try:\n                                    await ws.close()\n                                except:\n                                    pass\n                        else:\n                            if len(data) < metadata["buffer_size"]:\n                                async with request_obj(session_ob, method)(metadata["request_url"],max_msg_size = metadata["buffer_size"]) as ws:\n                                    await ws.send_str(data)\n                                    await ws.send_str("in_close")\n                                    async for msg in ws:\n                                        if msg.type == aiohttp.WSMsgType.ERROR:\n                                            break\n\n                                        if msg.type == aiohttp.WSMsgType.TEXT:\n                                            if msg.data == \'out_close\':\n                                                await ws.close()\n                                                break\n                                        v = msg.data\n\n                                    try:\n                                        await ws.close()\n                                    except:\n                                        pass\n                            else:\n\n                                async with request_obj(session_ob, method)(metadata["request_url"],max_msg_size = metadata["buffer_size"]) as ws:\n                                    for x in range(len(data)/metadata["buffer_size"]):\n                                        await ws.send_str(data[x*metadata["buffer_size"]:(x+1)*metadata["buffer_size"]])\n                                    await ws.send_str("in_close")\n                                    async for msg in ws:\n                                        if msg.type == aiohttp.WSMsgType.ERROR:\n                                            break\n\n                                        if msg.type == aiohttp.WSMsgType.TEXT:\n                                            if msg.data == \'out_close\':\n                                                await ws.close()\n                                                break\n                                        v = msg.data\n\n                                    try:\n                                        await ws.close()\n                                    except:\n                                        pass\n                            \n                if metadata["return_function"] is not None:\n                    metadata["return_function"]()\n                await v\n            else:\n                if metadata[\'functionality_type\'] !="socket":\n                    async with self.request_obj(session_ob, method)(metadata["request_url"], params = metadata["request_parameter"]) as resp:\n                        \n                        if metadata["return_function"] is None:\n                            if metadata["buffer_size"] is None:\n                                #print("haii")\n                                data =  await self.request_method(resp, metadata["request_format"])()\n                                yield data\n                                \n                                return\n                            else:\n                                #print(\'haii\')\n                                #help(self.request_method(resp, metadata["request_format"]))\n                                while True:\n                                    data = await self.request_method(resp, metadata["request_format"])(metadata["buffer_size"])\n                                    if not data:\n                                        break\n\n                                    yield data\n                                \n                                \n                                 \n                                \n                        else:\n                            if metadata["buffer_size"] is None:\n                                data = await self.request_method(resp, metadata["request_type"])()\n                                yield metadata["return_function"](data, metadata["return_parameter"])\n                                return\n                            else:\n                                while True:\n                                    data = await self.request_method(resp, metadata["request_type"])(metadata["buffer_size"])\n                                    if not data:\n                                        break\n                                    yield metadata["return_function"](data ,metadata["return_parameter"])\n                       # \n\n                else:\n                    data =None \n                    if metadata["buffer_size"] is None:\n                        async with session.ws_connect(metadata["request_url"]) as ws:\n                            async for msg in ws:\n                                if msg.type == aiohttp.WSMsgType.ERROR:\n                                    break\n\n                                if msg.type == aiohttp.WSMsgType.TEXT:\n                                    if msg.data == \'in_close\':\n                                        await ws.close()\n                                        break\n                                elif data is None:\n                                    if msg.type is bytes:\n                                        data = b\'\'\n                                    else:\n                                        data = ""\n                                            \n                                data += msg.data\n                        if metadata["return_function"] is None:\n                            if metadata["request_type"] == "json":\n                                await ujson.loads(data)\n                            await data\n                        else:\n                            if metadata["request_type"] == "json":\n                                await metadata["return_function"](ujson.loads(data))\n                            await metadata["return_function"](data)\n                    else:\n                        async with session.ws_connect(metadata["request_url"],max_msg_size = metadata["buffer_size"])  as ws:\n                            async for msg in ws:\n                                if msg.type == aiohttp.WSMsgType.ERROR:\n                                    break\n\n                                if msg.type == aiohttp.WSMsgType.TEXT:\n                                    if msg.data == \'close\':\n                                        await ws.close()\n                                        break\n                                elif metadata["return_function"] is None :\n                                    yield msg.data\n                                else:\n                                    yield metadata["return_function"](msg.data)\n\n        metadata["status"] = "DONE"\n\n    def release_session_obj(self,loop_index,interface,session):\n        self.session_obj_list[loop_index][interface[0]][interface[1]].append(session)\n        \n#socket_manager_obj = SocketManagerClass()\n\n', 'created_at': '2020-11-09 12:41:54.096169', 'time_stamp': 1604905914.0961869, 'import_parameters': '{"config": ["NETWORK_INTERFACE,NETWORK_CLIENT_TIME_OUT", "custom"], "aiohttp": ["sys"]}'}
{'namespace': 'network_models.call_process', 'created_by': 'sys', 'source_code': '\nasync def call_func(obj,session_obj,metadata):\n    v = \'\'\n    method  = metadata["request_type"]\n     #async with session_obj:\n    if metadata["status"] == "DONE":\n        return\n    if method in ["post", "put", "patch"]:\n        if metadata["request_format"] == "json":\n            if metadata["functionality_type"] != "socket":\n                v = await obj.request_obj(session_obj, method)(metadata["request_url"], json = metadata["request_parameter"])\n                \n            else:\n                data = ujson.dumps(metadata["request_parameter"])\n                data = data.encode()\n                async with obj.request_obj(session_obj, method)(metadata["request_url"]) as ws:\n                    await ws.send_str(data)\n                    await ws.send_str("in_close")\n                    async for msg in ws:\n                        if msg.type == aiohttp.WSMsgType.ERROR:\n                            break\n    \n                        if msg.type == aiohttp.WSMsgType.TEXT:\n                            if msg.data == \'out_close\':\n                                await ws.close()\n                                break\n                            v = msg.data\n                    \n              \t\t\n    else:\n        if metadata["functionality_type"] != "socket":\n            v = await obj.request_obj(session_obj, method)(metadata["request_url"], data = metadata["request_parameter"])\n                      \n        else:\n            #print(\'haiii inside socket\')\n            \n\n            data = ujson.dumps(metadata["request_parameter"])#.encode()\n        \n            async with obj.request_obj(session_obj, method)(metadata["request_url"]) as ws:\n                await ws.send_str(data)\n                await ws.send_str("in_close")\n                #temp = None\n                async for msg in ws:\n                    if msg.type == aiohttp.WSMsgType.ERROR:\n                        break\n                    \n                    if msg.type == aiohttp.WSMsgType.TEXT:\n                        if msg.data == \'out_close\':\n                            await ws.close()\n                            break\n                    \n                    v = msg.data\n                 #   print(v)\n    \n            if metadata["return_function"] is None:\n                return v \n            else:\n                if metadata[\'return_parameter\'] is None:\n                    return metadata[\'return_function\'](v)\n                else:\n                    return metadata[\'return_function\'](v,**metadata[\'return_parameter\'])\n\n        \n                  \n    metadata["status"] = "DONE"\n                   \n    if metadata["return_function"] is None:\n        return v \n    else:\n        if metadata[\'return_parameter\'] is None:\n            return metadata[\'return_function\'](v)\n        else:\n            return metadata[\'return_function\'](v,**metadata[\'return_parameter\'])\n', 'created_at': '2020-11-09 12:41:54.100081', 'time_stamp': 1604905914.1001015, 'import_parameters': '{"ujson": ["sys"], "aiohttp": ["sys"]}'}
{'namespace': 'network_models.buffer_call_process', 'created_by': 'sys', 'source_code': '\nasync def buffer_call_func(obj,session_obj,metadata):\n    method  = metadata["request_type"]\n    async with session_obj:\n        if metadata["status"] == "DONE":\n            return\n        if method in ["post", "put", "patch"]:\n            if metadata["request_format"] == "json":\n                if metadata["functionality_type"] != "socket":\n                    data = json.dumps(metadata["request_parameter"])\n                    if len(data) > metadata["buffer_size"]:\n                        for x in range(len(metadata)/metadata["buffer_size"]):\n                            v = await self.request_obj(session_obj, method)(metadata["request_url"], data = data[x*metadata["buffer_size"]:(x+1)*metadata["buffer_size"]])\n                    else:\n                        v = await self.request_obj(session_obj, method)(metadata["request_url"], data = data[x*metadata["buffer_size"]:(x+1)*metadata["buffer_size"]])\n                else:\n                    if len(data) <= metadata["buffer_size"]:\n                        async with request_obj(session_obj, method)(metadata["request_url"],max_msg_size = metadata["buffer_size"]) as ws:\n                            await ws.send_str(data)\n                            await ws.send_str("in_close")\n                            await ws.close()\n                    else:\n                        async with request_obj(session_obj, method)(metadata["request_url"],max_msg_size = metadata["buffer_size"]) as ws:\n                            for x in range(len(data)/metadata["buffer_size"]):\n                                await ws.send_str(data)\n                            await ws.send_str("in_close")\n                            async for msg in ws:\n                                if msg.type == aiohttp.WSMsgType.ERROR:\n                                    break\n                                if msg.type == aiohttp.WSMsgType.TEXT:\n                                    if msg.data == \'out_close\':\n                                        await ws.close()\n                                        break\n                                v = msg.data ###return_parameter needed\n\n                                if metadata["return_function"] is not None:\n                                    if metadata["return_parameter"] is not None:\n                                        yield metadata["return_function"](v,**metadata["return_parameter"])\n                                    else:\n                                        yield metadata["return_function"](v)\n                                else:\n                                    yield v  \n            else:\n                if metadata["functionality_type"] != "socket":\n                    if len(metadata["request_parameter"])< metadata["buffer_size"]:\n                        v = await self.request_obj(session_obj, method)(metadata["request_url"], data = metadata["request_parameter"])\n                    else:\n                        for x in range(len(metadata["request_parameter"])/metadata["buffer_size"]):\n                            v = await self.request_obj(session_obj, method)(metadata["request_url"], data = metadata["request_parameter"][x*metadata["buffer_size"]:(x+1)*metadata["buffer_size"]])\n                else:\n                    data = (metadata["request_parameter"]).encode()\n                    if len(data) < metadata["buffer_size"]:\n                        async with request_obj(session_obj, method)(metadata["request_url"],max_msg_size = metadata["buffer_size"]) as ws:\n                            await ws.send_str(data)\n                            await ws.send_str("in_close")\n                            async for msg in ws:\n                                if msg.type == aiohttp.WSMsgType.ERROR:\n                                    break\n                                if msg.type == aiohttp.WSMsgType.TEXT:\n                                    if msg.data == \'out_close\':\n                                        await ws.close()\n                                        break\n                                v = msg.data ###return_parameter needed\n                    else:\n                        async with request_obj(session_obj, method)(metadata["request_url"],max_msg_size = metadata["buffer_size"]) as ws:\n                            for x in range(len(data)/metadata["buffer_size"]):\n                                await ws.send_str(data[x*metadata["buffer_size"]:(x+1)*metadata["buffer_size"]])\n                            await ws.send_str("in_close")\n                            async for msg in ws:\n                                if msg.type == aiohttp.WSMsgType.ERROR:\n                                    break\n                                if msg.type == aiohttp.WSMsgType.TEXT:\n                                    if msg.data == \'out_close\':\n                                        await ws.close()\n                                        break\n                                v = msg.data\n                                    \n                    if metadata["return_function"] is not None:\n                        if metadata["return_parameter"] is not None:\n                            yield metadata["return_function"](v,**metadata["return_parameter"])\n                        else:\n                            yield metadata["return_function"](v)\n                    else:\n                        yield v\n            \n        else:\n            if metadata[\'functionality_type\'] !="socket":\n                async with self.request_obj(session_obj, method)(metadata["request_url"], params = metadata["request_parameter"]) as resp:\n                    if metadata["return_function"] is None:\n                        while True:\n                            data = await self.request_method(resp, metadata["request_format"])(metadata["buffer_size"])\n                            if not data:\n                                break\n                            yield data\n                    \n                    else:\n                        while True:\n                            data = await self.request_method(resp, metadata["request_type"])(metadata["buffer_size"])\n                            if not data:\n                                break\n                            yield metadata["return_function"](data,**metadata["return_parameter"])\n\n            else:\n                async with session.ws_connect(metadata["request_url"],max_msg_size = metadata["buffer_size"])  as ws:\n                    async for msg in ws:\n                        if msg.type == aiohttp.WSMsgType.ERROR:\n                            break\n\n                        if msg.type == aiohttp.WSMsgType.TEXT:\n                            if msg.data == \'close\':\n                                await ws.close()\n                                break\n                        elif metadata["return_function"] is None :\n                            yield msg.data\n                        else:\n                            yield metadata["return_function"](msg.data)\n\n        metadata["status"] = "DONE"\n        \n', 'created_at': '2020-11-09 12:41:54.103417', 'time_stamp': 1604905914.1034572, 'import_parameters': '{"ujson": ["sys"], "aiohttp": ["sys"]}'}
{'namespace': 'network_models.socket_try', 'created_by': 'sys', 'source_code': "server\n\nasync def websocket_handler(request):\n\n    ws = web.WebSocketResponse()\n    await ws.prepare(request)\n\n    async for msg in ws:\n        if msg.type == aiohttp.WSMsgType.TEXT:\n            if msg.data == 'close':\n                await ws.close()\n            else:\n                await ws.send_str(msg.data + '/answer')\n        elif msg.type == aiohttp.WSMsgType.ERROR:\n            print('ws connection closed with exception %s' %\n                  ws.exception())\n\n    print('websocket connection closed')\n\n    return ws\napp.add_routes([web.get('/ws', websocket_handler)])\n        \n        \n    async with session.ws_connect('http://example.org/ws') as ws:\n    async for msg in ws:\n        if msg.type == aiohttp.WSMsgType.TEXT:\n            if msg.data == 'close cmd':\n                await ws.close()\n                break\n            else:\n                await ws.send_str(msg.data + '/answer')\n        elif msg.type == aiohttp.WSMsgType.ERROR:\n            break)\n", 'created_at': '2020-11-09 12:41:54.108403', 'time_stamp': 1604905914.1084833, 'import_parameters': '{}'}
{'namespace': 'network_models.buffer_call_process_1', 'created_by': 'sys', 'source_code': 'async def buffer_call_func(obj,session_obj,metadata):\n    #async with session_ob:\n    if metadata["status"] == "DONE":\n          return\n    if method in ["post", "put", "patch"]:\n        if metadata["request_format"] == "json":\n            if metadata["functionality_type"] != "socket":\n                data = json.dumps(metadata["request_parameter"])\n                if len(data) > metadata["buffer_size"]:\n                    for x in range(len(metadata)/metadata["buffer_size"]):\n                        v = await self.request_obj(session_ob, method)(metadata["request_url"], data = data[x*metadata["buffer_size"]:(x+1)*metadata["buffer_size"]])\n                else:\n                    v = await self.request_obj(session_ob, method)(metadata["request_url"], data = data[x*metadata["buffer_size"]:(x+1)*metadata["buffer_size"]])\n            else:\n                if len(data) <= metadata["buffer_size"]:\n                    async with request_obj(session_ob, method)(metadata["request_url"],max_msg_size = metadata["buffer_size"]) as ws:\n                        await ws.send_str(data)\n                        await ws.send_str("in_close")\n                        await ws.close()\n                else:\n                    async with request_obj(session_ob, method)(metadata["request_url"],max_msg_size = metadata["buffer_size"]) as ws:\n                        for x in range(len(data)/metadata["buffer_size"]):\n                            await ws.send_str(data)\n                        await ws.send_str("in_close")\n                        async for msg in ws:\n                            if msg.type == aiohttp.WSMsgType.ERROR:\n                                break\n                            if msg.type == aiohttp.WSMsgType.TEXT:\n                                if msg.data == \'out_close\':\n                                    await ws.close()\n                                    break\n                            v = msg.data ###return_parameter needed\n\n                            if metadata["return_function"] is not None:\n                                if metadata["return_parameter"] is not None:\n                                    yield metadata["return_function"](v,**metadata["return_parameter"])\n                                else:\n                                    yield metadata["return_function"](v)\n                            else:\n                                yield v  \n        else:\n            if metadata["functionality_type"] != "socket":\n                if len(metadata["request_parameter"])< metadata["buffer_size"]:\n                    v = await self.request_obj(session_ob, method)(metadata["request_url"], data = metadata["request_parameter"])\n                else:\n                    for x in range(len(metadata["request_parameter"])/metadata["buffer_size"]):\n                        v = await self.request_obj(session_ob, method)(metadata["request_url"], data = metadata["request_parameter"][x*metadata["buffer_size"]:(x+1)*metadata["buffer_size"]])\n            else:\n                data = (metadata["request_parameter"]).encode()\n                    if len(data) < metadata["buffer_size"]:\n                        async with request_obj(session_ob, method)(metadata["request_url"],max_msg_size = metadata["buffer_size"]) as ws:\n                            await ws.send_str(data)\n                            await ws.send_str("in_close")\n                            async for msg in ws:\n                                if msg.type == aiohttp.WSMsgType.ERROR:\n                                    break\n                                if msg.type == aiohttp.WSMsgType.TEXT:\n                                    if msg.data == \'out_close\':\n                                        await ws.close()\n                                        break\n                                v = msg.data ###return_parameter needed\n                    else:\n                        async with request_obj(session_ob, method)(metadata["request_url"],max_msg_size = metadata["buffer_size"]) as ws:\n                            for x in range(len(data)/metadata["buffer_size"]):\n                                await ws.send_str(data[x*metadata["buffer_size"]:(x+1)*metadata["buffer_size"]])\n                            await ws.send_str("in_close")\n                            async for msg in ws:\n                                if msg.type == aiohttp.WSMsgType.ERROR:\n                                    break\n                                if msg.type == aiohttp.WSMsgType.TEXT:\n                                    if msg.data == \'out_close\':\n                                        await ws.close()\n                                        break\n                                v = msg.data\n                                \n                if metadata["return_function"] is not None:\n                    if metadata["return_parameter"] is not None:\n                        yield metadata["return_function"](v,**metadata["return_parameter"])\n                    else:\n                        yield metadata["return_function"](v)\n                else:\n                    yield v\n        \n    else:\n        if metadata[\'functionality_type\'] !="socket":\n            async with self.request_obj(session_ob, method)(metadata["request_url"], params = metadata["request_parameter"]) as resp:\n                if metadata["return_function"] is None:\n                    while True:\n                        data = await self.request_method(resp, metadata["request_format"])(metadata["buffer_size"])\n                        if not data:\n                            break\n                        yield data\n                \n                else:\n                    while True:\n                        data = await self.request_method(resp, metadata["request_type"])(metadata["buffer_size"])\n                        if not data:\n                            break\n                        yield metadata["return_function"](data,**metadata["return_parameter"])\n\n        else:\n            async with session.ws_connect(metadata["request_url"],max_msg_size = metadata["buffer_size"])  as ws:\n                async for msg in ws:\n                    if msg.type == aiohttp.WSMsgType.ERROR:\n                        break\n\n                    if msg.type == aiohttp.WSMsgType.TEXT:\n                        if msg.data == \'close\':\n                            await ws.close()\n                            break\n                    elif metadata["return_function"] is None :\n                        yield msg.data\n                    else:\n                        yield metadata["return_function"](msg.data)\n\n    metadata["status"] = "DONE"\n    \n', 'created_at': '2020-11-09 12:41:54.115070', 'time_stamp': 1604905914.115096, 'import_parameters': '{}'}
{'namespace': 'root_models.container_manage', 'created_by': 'sys', 'source_code': '#from thread_loop import thread_manager\n#from coruting_manager import corouting_mager\n#from common_api_lib import router\n#from base_model.thread_models.thread_loop import thread_manager\n#from base_model.suport_thread.import_model import ImportManager\n\n\nMAXIMUM_NO_COROUTING =3*10**3\nMINIMUM_NO_COROUTING = 10\nclass container_suppervisor(thread_manager,GenClass):\n    function_dict = {\n            \'/get_task_from_monitor\':[\'get_task_from_monitor\',\'socket\'],\n            \'/reset\':[\'reset\',\'socket\'],\n            \'/restart_all_microservices\':[\'restart_all_microservices\',\'socket\'],\n            \'/shutdown\':[\'shutdown\',\'socket\'],\n            \'/shutdown_container\':[\'shutdown_container\',\'socket\'],\n            \'/restart_microservice\':[\'restart_microservice\',\'socket\'],\n            \'/delete_microservice\':[\'delete_microservice\',\'socket\'],\n            \'/delete_microservice_function\':[\'delete_microservice_function\',\'socket\'],\n            \'/list\':[\'list\',\'socket\'],\n            \'/stop_debugging_to_the_micrservice\':[\'stop_debugging_to_the_micrservice\',\'socket\'],\n            \'/start_debugging_to_the_microservice\':[\'start_debugging_to_the_microservice\',\'socket\'],\n            \'/stop_debugging_all_micrservices\':[\'stop_debugging_all_micrservices\',\'socket\'],\n            \'/generate_microservice_debugging_info\':[\'generate_microservice_debugging_info\',\'socket\'],\n            \'/start_debugging_all_microservices\':[\'start_debugging_all_microservices\',\'socket\'],\n            \'/report_all_services\':[\'report_all_services\',\'socket\'],\n            \'/report_total_no_of_corouting\':[\'report_total_no_of_corouting\',\'socket\']\n            }\n\n    function_list=[\'request_monitor_for_delete_service\']\n    #dic = {\n    #        \'default\':\'initializer\',\n    #        \'pre_process\':\'hallo_monitor\',\n    #        \'ask_monitor_for_help\':\'ask_monitor_for_help\'\n    #        }\n    socket_manager_obj = None\n    is_running = False\n    is_shutdown = False\n    is_close = False\n    current_thread_id = 0\n    #MAXIMUM_NO_COROUTING =3*10**3\n    default_co_routing_task = None\n    current_no_of_corouting = 0\n    current_service_status =[]\n    is_debugging_service = False\n    host = NETWORK_INTERFACE[\'IN_NET\']##\'127.0.0.1\'\n\n    def __init__(self,import_manager_obj):\n\n        #l = [thread_manager,GenClass]\n        for x in type(self).__bases__:\n        #      print(x)\n        \tx.__init__(self)\n        \n        self.thread_manager_obj = self\n        self.register_thread()\n        self.import_manager_obj = import_manager_obj\n        task_meta = {\'service\':type(self).__name__,\'service_fun\':\'request_monitor_for_delete_service\'}\n        self.corouting_manager_obj.register_service(type(self).__name__,\'request_monitor_for_delete_service\',self.request_monitor_for_delete_service)\n        self.thread_manager_obj = self\n        self.socket_object.register_loop(0)\n        loop = self.loop_list[self.loop_index]\n        loop.run_until_complete(self.hallo_monitor(0))\n        self.initialize_supervise_services(0)\n\n\n    def initialize_supervise_services(self,thread_id):\n        try:\n            m = MicroService(self.next_port(),0,self)\n            self.services[\'supervise_microservice\']=[]\n            self.services[\'supervise_microservice\'].append(m)\n\n            r = self.create_router()\n            loop = self.loop_list[thread_id]\n            m.register_url(r.urls)\n            m.start_microservice_server(r,loop)\n            runner = m.runner\n            loop.run_until_complete(runner.setup())\n\n            site = web.TCPSite(runner, \'0.0.0.0\', self.current_port)\n            loop.create_task(site.start())\n        except Exception as e:\n            print(e )\n        \n    def supervisor_run(self):\n        #print(\'haiii\')\n        self.assign_loop()\n\n    async def supervisor_shutdown(self):\n        m = self.services[\'supervise_microservice\'][0]\n        loop = self.loop_list[self.loop_index]\n        t = loop.create_task(m.runner.cleanup())\n        while not t.done():\n            await asyncio.sleep(0.5)\n        \n        self.is_close = True\n        loop.stop()\n        SYSTEM_STATUS[\'is_shutdown\'] = True\n\n    async def hallo_monitor(self,thread_id):\n        #print(self.loop_index,NETWORK_INTERFACE[\'MONITOR_NET\'],\'socket\',"json","http://"+MONITOR_NETWORK_INTERFACE[\'IP_ADDRESS\']+":1000/initialize_suppervizor/")\n        try:\n\n            global NETWORK_INTERFACE\n            global MONITOR_NETWORK_INTERFACE\n            global DEFAULT\n            #print("http://"+MONITOR_NETWORK_INTERFACE[\'IP_ADDRESS\']+":9999/initialize_suppervizor/")\n        \n            remsg = ujson.loads(await self.socket_object.send_single_request(self.loop_index,\'*\',\'socket\',"json","http://"+MONITOR_NETWORK_INTERFACE[\'IP_ADDRESS\']+":10/initialize_suppervizor",\'haii\',is_socket=True))\n            NETWORK_INTERFACE = remsg[\'NETWORK_INTERFACE\']\n            NGINX = remsg[\'NGINX\']\n            DEFAULT["CASSANDRA"]["IP"]= NGINX[\'OUT_NET\']\n            DEFAULT["POSTGRESQL"]["IP"]= NGINX[\'OUT_NET\']\n            DEFAULT["REDIS"]["IP"]=NGINX[\'OUT_NET\']\n            DEFAULT["KAFKA"]["BROKER"]=[NGINX[\'OUT_NET\']+\':9092\']\n            print(remsg)\n            \n        except Exception as e: \n            print(type(remsg),remsg)\n            print("http://"+MONITOR_NETWORK_INTERFACE[\'IP_ADDRESS\']+":9999/initialize_suppervizor")\n            print(e)\n            \n        \n        #NETWORK_INTERFACE = remsg.data[\'NETWORK_INTERFACE\']\n       # NGINX = remsg.data[\'NGINX\']\n        #print(\'in hallo monitor\')\n\n    ################ monitor related task assaign to corouting  ##########################\n\n    async def request_monitor_for_delete_service(self,loop_index,meta_data): ##(self,thread_id,namespace)\n        while self.REDUCE_FLAG[loop_index] is not True:\n            await asyncio.sleep(10)\n            self.current_service_status = [] ### need lock acurring\n            try:\n            \tself.services[\'microservice\']\n            except:\n            \tcontinue\n\n            if self.services[\'microservice\'] ==[]:\n                continue\n            temp_max = 0\n            temp_url = None\n            temp_url_dir = {}\n            total =0\n            for x in self.services[\'microservice\']:\n                self.current_service_status.append(x.no_request_processing_per_url)\n                for y in x.no_request_processing_per_url:\n                    if temp_max < x.no_request_processing_per_url[y]:\n                        temp_url_dir[\'namespace\'] = x.namespace\n                        temp_url_dir[\'class\'] = x.class_name\n                        temp_url_dir[\'url\'] = y\n                    total+=x.no_request_processing_per_url[y]\n            if not total < MAXIMUM_NO_COROUTING:\n                await self.ask_monitor_for_help(loop_index,temp_url_dir)\n            if total < MINIMUM_NO_COROUTING:\n                await self.request_for_delete_service(loop_index,temp_url_dir)\n\n            self.current_no_of_corouting = total\n            #### need to lock relish\n\n            temp_max = 0\n            temp_url = None\n            temp_url_dir = {}\n            total =0\n\n    async def inform_monitor(urls):\n        #l = self.loop_list[0]\n        return await self.socket_object.send_single_request(self.loop_index,NETWORK_INTERFACE[\'OUT_NET\'],\'socket\',"json","http://"+MONITOR_NETWORK_INTERFACE[\'IP_ADDRESS\']+"/update_container_url",urls)\n\n    async def ask_monitor_for_help(self,thread_id,task):\n        await self.socket_object.send_single_request(self.loop_index,NETWORK_INTERFACE[\'OUT_NET\'],\'socket\',"json","http://"+MONITOR_NETWORK_INTERFACE[\'IP_ADDRESS\']+"/monitor/add_threads/",task)\n        return response\n\n    async def request_for_delete_service(self,thread_id,namespace):\n    \t#print(namespace)\n    \tawait self.socket_object.send_single_request(self.loop_index,NETWORK_INTERFACE[\'OUT_NET\'],\'socket\',"json","http://"+MONITOR_NETWORK_INTERFACE[\'IP_ADDRESS\']+"/start_delete_microservice_from_container",{\'namespace\':namespace})\n\n############################################### Supervisor server service #########################################\n\n    async def current_microservice_info(self,request):\n        return ujson(self.current_microservice_iformation)\n\n    async def report_total_no_of_corouting(self,request):\n        return ujson.dumps({\'no_of_co_routing\':self.current_no_of_corouting})\n    async def report_all_services(self,request):\n        return ujson.dumps(self.current_service_status)\n\n    async def start_debugging_all_microservices(self,request):\n        for x in self.services[\'microservice\']:\n            x.is_debuging = True\n\n        return \'Ok\'\n\n    async def generate_microservice_debugging_info(self,loop_index,metadata):\n        \'\'\' This is a corouting service for back end process \'\'\'\n        temp_data = []\n        session_obj = self.socket_obj.get_session_obj(thread_id)\n        while self.is_debugging_service:\n\n            for x in self.services[\'microservice\']:\n                if x.is_debuging:\n                    temp_data.append(x.replay_time_per_sec)\n            await self.socket_object.send_single_request(self.loop_index,NETWORK_INTERFACE[\'OUT_NET\'],\'socket\',"json","http://"+MONITOR_NETWORK_INTERFACE[\'IP_ADDRESS\']+"/get_all_debugging_info/",temp_data)\n\n    async def stop_debugging_all_micrservices(self,request):\n\n        for x in self.services[\'microservice\']:\n            x.is_debuging = False\n\n        return \'Ok\'\n\n    async def start_debugging_to_the_microservice(self,request):\n        microservice = request.data[\'namespace\']\n        microservice.is_debuging = True\n\n        return \'Ok\'\n\n    async def stop_debugging_to_the_micrservice(self,request):\n        microservice = request.data[\'namespace\']\n        microservice.is_debuging = False\n\n        return \'Ok\'\n    async def get_task_from_monitor(self,request):\n        task = request.data\n        """Creates new thread and appends to thread list"""\n        print(\'get task from monitor \',task)\n        return ujson.dumps({\'port\':await self.call_micro_service(self.loop_index,task)})\n\n        \n\n    async def reset(self,request):\n    \t\'\'\'clear all microservices\'\'\'\n    \tfor x in self.microservice_list:\n    \t\tx.close_microservice()\n    \tfor x in range(len(self.REDUCE_FLAG)):\n    \t\tself.REDUCE_FLAG[x] = True\n    \treturn \'Ok\'\n    async def restart_all_microservices(self,request):\n        for x in self.microservice_list:\n            self.import_manager_obj.update_namespace(x.namespace)\n            x.restart_microservice()\n        return \'Ok\'\n\n    async def shutdown(self,request):\n        self.REDUCE_FLAG[self.current_thread_id] = True\n        self.is_shutdown = True\n        return \'Ok\'\n\n    async def shutdown_container(self,request):\n    \tawait self.shutdown(request)\n    \tSYSTEM_STATUS[\'is_shutdown\'] = True\n    \treturn \'Ok\'\n\n    async def restart_microservice(self,request):\n        data = request.data\n        url = data[\'namespace\']\n        for x in microservice_list:\n            if x.namespace == url:\n                self.import_manager_obj.update_namespace(namespace)\n                x.restart_microservice()\n                break\n\n        return \'Ok\'\n    \n    async def update_namespace(self,namespace):\n       data = request.data\n       namespace = data["namespace"]\n       self.import_manager_obj.update_namespace(namespace)\n       return \'Ok\'\n    #\n\n    async def delete_microservice(self,request):\n        data = request.data\n        url = data[\'namespace\']\n        for x in microservice_list:\n            if x.namespace == url:\n                x.close_microservice()\n                break\n        return \'Ok\'\n\n    async def delete_microservice_function(self,request):\n        data = request.data\n        url = data[\'namespace\']\n        func_url = data[\'url\']\n        for x in microservice_list:\n            if x.namespace == url:\n                url_list = list(x.no_request_processing_per_url.keys())\n                url_list.remove(func_url)\n\n                x.restart_microservice(url_list)\n                break\n        return \'Ok\'\n\n\n#    async def kafka_model_reset(request):\n#        try:\n#            self.services["Kafka"].STOP = True\n#            self.import_manager_obj.reset_model("kafka")\n#            self.thread_pending_task.append({\'type\':\'kafka\'})\n#        except:\n#            return \'Fail\'\n    async def list(request):\n        #data = request.json()\n        urls = []\n        for x in self.thread_info:\n            for y in self.thread_info[x]:\n                if \'UI\' == y[\'service_type\']:\n                    urls.append(y[\'url\'])\n        return ujson.dumps(urls)\n', 'created_at': '2020-11-09 12:41:54.119531', 'time_stamp': 1604905914.119568, 'import_parameters': '{"thread_models.one_only_thread": ["thread_manager", "custom"], "thread_models.coroutine_manager_only_one_thread": ["coroutine_manager", "custom"], "common_models.common_api_lib": ["GenClass", "custom"], "config": ["NETWORK_INTERFACE,MONITOR_NETWORK_INTERFACE,SYSTEM_STATUS,NGINX,SYSTEM_STATUS,DEFAULT", "custom"], "thread_models.support_only_one_thread": ["MicroService", "custom"], "ujson": ["sys"], "asyncio": ["sys"], "aiohttp": ["web", "sys"], "threading": ["sys"]}'}
{'namespace': 'root_models.nginx_config_generator', 'created_by': 'sys', 'source_code': '\n#request = {\n#    "url": "new_list",\n#    "url_type": "mn_con",\n#    "ip": "127.0.0.1",\n#    "port": "9998",\n#    "weight": "5",\n#    "protocol":"http",\n#    "method": "RedisViewSet/viewset/Cassandra/list"\n#    }\n#\n\nclass NginxConfig:\n    \n    url_type= {\n        "db_con": {\n            "container_id": "nginx_database_connector",\n            "config_path": "/home/miworld_monitor/db_con/nginx.conf"\n            },\n        "mn_con": {\n            "container_id": "manager",\n            "config_path": "/home/miworld_monitor/mn_con/nginx.conf"\n            },\n        "ui_con": {\n            "container_id": "ui",\n            "config_path": "/home/miworld_monitor/ui_con/nginx.conf"\n        }\n    }\n    local_var = {\n            "db_con": {"default_redis","default_cassandra","default_postgres"},\n            "mn_con": {},\n            "ui_con": {}\n            }\n\n    def comm_op(self, request):\n        new = {\n                request["url"]: {\n                    "upstream": {\n                        request["ip"]: {\n                            request["port"]: {\n                                "weight":request["weight"]\n                                }   \n                            }\n                        },\n                    "method": request["method"]\n                    }\n                }\n        try:\n            if request["url"] in self.local_var[request["url_type"]]:\n                self.local_var[request["url_type"]][request["url"]].update(new[request["url"]]["upstream"])\n                \n            else:\n                self.local_var[request["url_type"]].update(new)\n        except:\n            print(\'Invalid url type\')\n\n\n\n            \n\n    def conf_create(self, request):\n       # new = {\n       #         request["url"]: {\n       #             "upstream": {\n       #                 request["ip"]: {\n       #                     request["port"]: {\n       #                         "weight":request["weight"]\n       #                         }   \n       #                     }\n       #                 },\n       #             "method": request["method"]\n       #             }\n       #         }\n       # try:\n       #     if request["url"] in self.local_var[request["url_type"]]:\n       #         self.local_var[request["url_type"]][request["url"]].update(new[request["url"]]["upstream"])\n       #         \n       #     else:\n       #         self.local_var[request["url_type"]].update(new)\n       # except:\n       #     print(\'Invalid url type\')\n\n       self.comm_op(request)\n       self.conf_write(request["url_type"])\n\n\n    def conf_multiple_create(self, multi_config): ####  {\'configes\':[],\'url_type\':\'\'}\n        ### array of configuration file for a single url type or container type ####\n        #new = {\n        #        request["url"]: {\n        #            "upstream": {\n        #                request["ip"]: {\n        #                    request["port"]: {\n        #                        "weight":request["weight"]\n        #                        }   \n        #                    }\n        #                },\n        #            "method": request["method"]\n        #            }\n        #        }\n        #for x in multi_config[\'configes\']:\n        #    try:\n        #        if x["url"] in self.local_var[x["url_type"]]:\n        #            self.local_var[x["url_type"]][x["url"]].update(new[x["url"]]["upstream"])\n        #        else:\n        #            self.local_var[x["url_type"]].update(new)\n        #    except:\n        #        print(\'Invalid url type\')\n\n        for x in multi_config[\'configes\']:\n            self.comm_op(x)\n\n        self.conf_write(multi_config["url_type"])\n\n    \n    def conf_list(self, url_type, url):\n        return list(self.local_var[url_type][url]["upstream"].keys())\n\n    \n    def conf_update(self, request):\n        new_weight = request["weight"]    \n        self.local_var[request["url_type"]][request["url"]]["upstream"][request["ip"]][request["port"]]["weight"] = new_weight\n        \n        self.conf_write(request["url_type"])\n\n    \n    def conf_delete_url(self, request):\n        del self.local_var[request["url_type"]][request["url"]]\n        \n        self.conf_write(request["url_type"])\n    \n    \n    def search_url_for_ip_address(self, ip):\n        url_list = []\n        conf = {x: nginxconfig_obj.local_var[x] for x in nginxconfig_obj.local_var if x not in "db_con"}\n        for url_type in conf:\n            for url in conf[url_type]:\n                if ip in (x for x in conf[url_type][url]["upstream"]):\n                    url_list.append(url)\n        return url_list\n\n    \n    def return_ip_list_for_url(self, url):\n        ip_list = []\n        conf = {x: nginxconfig_obj.local_var[x] for x in nginxconfig_obj.local_var if x not in "db_con"}\n        for url_type in conf:\n            if url in (x for x in conf[url_type]):\n                return list(conf[url_type][url]["upstream"].keys())\n\n\n    def conf_write(self, url_type):\n        if url_type == "db_con":\n            s = "resolver 127.0.0.11 valid=5s\\n"\n            for x in self.local_var["db_con"]:\n                s = s+"\\nserver {\\n\\tlisten 80;\\n\\tset $"+x+" http://"+x+";\\n\\tlocation / {\\n\\t\\tproxy_pass $"+x+";\\n\\t}\\n}"\n\n        else:\n            s = "events{}\\nhttp {\\n\\tmap $http_upgrade $connection_upgrade {\\n\\t\\tdefault upgrade;\\n\\t\\t\'\'      close;\\n\\t}"\n            for url in self.local_var[url_type]:\n                s = s+"\\n\\t"+"upstream "+url+" {"\n                temp = self.local_var[url_type][url]["upstream"]\n                for ip in temp:\n                    for port in temp[ip]:\n                        s = s+"\\n\\t\\tserver "+ip+":"+str(port)+" weight="+str(temp[ip][port]["weight"])+";"\n                s = s+"\\n\\t}\\n\\tserver {\\n\\\n                listen 80;\\n\\\n                location ~ ^/("+self.local_var[url_type][url]["method"]+"|socket\\.io){\\n\\\n                \\tproxy_http_version 1.1;\\n\\\n                \\tproxy_set_header Upgrade $http_upgrade;\\n\\\n                \\tproxy_set_header Connection $connection_upgrade;\\n\\\n                \\tproxy_pass http://"+url+";\\n\\\n                }\\n\\t}"\n            s = s+"\\n}"\n        \n        #os.system(\'ssh -o StrictHostKeyChecking=no -i my_secreat -tt miworld_monitor@172.17.0.1 "echo maa@12345|sudo -S python3 file_write.py \'+self.url_type[url_type]["config_path"]+\' \'+s)\n\n        f = open(\'temp_file.txt\',\'wt+\')\n        f.write(s)\n        f.close()\n        str01 = \'scp -o StrictHostKeyChecking=no -i my_secreat temp_file.txt miworld_monitor@172.17.0.1:~/nginx_config_01.txt\'\n        print(str01)\n        str02 = \'ssh -o StrictHostKeyChecking=no -i my_secreat -tt miworld_monitor@172.17.0.1 "echo maa@12345|sudo -S cp nginx_config_01.txt \'+self.url_type[url_type]["config_path"]+\'"\'\n        print(str02)\n        str03 = \'ssh -o StrictHostKeyChecking=no -i my_secreat -tt miworld_monitor@172.17.0.1 "echo maa@12345|sudo -S python3 nginx_changer.py \'+self.url_type[url_type]["container_id"]+\'"\'\n        print(str03)\n\n\n        os.system(str01)\n        os.system(str02)\n        os.system(str03)\n        #f = open(self.url_type[url_type]["config_path"], "a")\n        #f.truncate(0)\n        #f.write(s)\n        #f.close()\n        #os.system("docker-compose exec"+self.url_type[url_type]["container_id"]+"nginx -s reload")\n        #os.system(\'ssh -o StrictHostKeyChecking=no -i my_secreat -tt miworld_monitor@172.17.0.1 "echo maa@12345|sudo -S python3 nginx_changer.py \'+self.url_type[url_type]["container_id"])\n\n\nnginxconfig_obj = NginxConfig()\n\n', 'created_at': '2020-11-09 12:41:54.126985', 'time_stamp': 1604905914.1270082, 'import_parameters': '{"os": ["sys"]}'}
{'namespace': 'root_models.root_manage_services', 'created_by': 'sys', 'source_code': 'class RootManagementServices:\n\n    # async def upload_source_code(self,request):\n    #     str_data = request.data\n    #     file_id = uuid.uuid4().hex\n    #     file_name = file_id+\'.zip\'\n    #     file_path = MONITOR_TEMP_ZIPE_STORE+\'/\'+file_name\n    #     fp =open(file_path,\'bw+\')\n    #     fp.write(str_data)\n    #     extract_zip_file(file_path,MONITOR_TEMP_FOLDER+\'/\'+file_id)\n    #     r_path= MONITOR_TEMP_FOLDER+\'/\'+file_id\n    #     file_name =os.listdir(r_path)\n    #     ff = r_path+\'/\'+filename\n    #     p = pathlib.Path(ff)\n    #     if p.is_dir():\n    #         namespace_list = insert_package(file_name,r_path) ### model insertion is done ####\n    #         ########## model url mapping #############\n    #         self.name_space_url_mapping(file_name)\n\n    #         return \'Done\'\n    #     else:\n    #         return \'Fail\'\n\n\n    # async def source_code_update(self,meta_data,loop_index):\n    #     while True:\n    #         file_name =os.listdir(r_path)\n    #         ff = r_path+\'/\'+filename\n    #         p = pathlib.Path(ff)\n    #         if p.is_dir():\n    #             namespace_list = insert_package(file_name,r_path) ### model insertion is done ####\n    #             ########## model url mapping #############\n    #             self.name_space_url_mapping(file_name)\n\n    #             return \'Done\'\n    #         else:\n    #             return \'Fail\'\n\n\n    async def search_container_for_namespace_restart(self,url_type,url,namespace):\n        ip_list = nginxconfig_obj.conf_list(url_type,url) ### [ip1,ip2,....]\n\n        for x in ip_list:\n            await self.start_restart_microservice_of_container(self.current_loop_index,x,namespace)\n\n    async def search_container_for_namespace_delete(self,url_type,url,namespace):\n        ip_list = nginxconfig_obj.conf_list(url_type,url) ### [ip1,ip2,....]\n\n        for x in ip_list:\n            await self.start_delete_microservice_from_container(self.current_loop_index,x,namespace)\n\n    async def search_container_for_start_namespace_debugging(self,url_type,url,namespace):\n        ip_list = nginxconfig_obj.conf_list(url_type,url) ### [ip1,ip2,....]\n\n        for x in ip_list:\n            await  self.start_debugging_to_container_microservices(self.current_loop_index,x,namespace)\n\n    async def search_container_for_stop_namespace_debugging(self,url_type,url,namespace):\n        ip_list = nginxconfig_obj.conf_list(url_type,url) ### [ip1,ip2,....]\n\n        for x in ip_list:\n            await self.stop_debugging_to_container_microservices(self.current_loop_index,x,namespace)\n\n    async def search_urls_for_microservices_restart_against_namespace(self,namespace):\n        for x in search_url_for_namespace(namespace):\n            self.search_container_for_namespace_restart(url_type,url,namespace) ##### need to check\n\n    async def name_space_url_mapping(self,app_name):\n        temp_namespace_dir = {}\n        import_manager_obj.add_namespace(app_name+\'.urls\')\n        app_url_info = import_manager_obj.walk(app_name+\'.urls\')\n        for x in app_url_info.microservice_class_list:\n            import_manager_obj.add_namespace(app_name+\'.\'+x)\n            temp_class = import_manager_obj.walk(app_name+\'.\'+x+\'.\'+app_url_info.microservice_class_list[x])\n            url_type = temp_class.url_type\n            source_url = "/"+app_name+\'/\'+x\n            microservice_class_name = app_url_info.microservice_class_list[x]\n            sub_url = ujson.dumps(list(temp_class.function_dict.keys()))\n            temp_namespace_dir[app_name+\'.\'+x] = {\'source_url\':source_url,\'sub_url\':sub_url,"microservice_class_name":microservice_class_name}\n\n         ### {\'namespace_name\':{\'source_url\':"",\'sub_url\':"","microservice_class_name":""},.....}\n\n        mapper_obj = UrlMapper()\n        mapper_obj.namespace_url_mape = temp_namespace_dir\n        mapper_obj.mape_url()\n', 'created_at': '2020-11-09 12:41:54.131147', 'time_stamp': 1604905914.1311677, 'import_parameters': '{"uuid": ["sys"]}'}
{'namespace': 'root_models.monitor_init_tasks', 'created_by': 'sys', 'source_code': '##### monitor initial tasks list ######\n\n#### "container_type01":[task01,...taskn],"container_type02":[task01,...taskn] ####\n\ninit_task = {\n        "db_manager":[\n\n              {\n                "service_models":"UI",\n                "class_name":\'PermissionPermissionBitMapperPostgresqlViewset\',\n                \'name_space\':\'PermissionPermissionBitMapper.Postgresql_viewset\'\n                },\n           # {\n           #     "service_models":"UI",\n           #     "class_name":\'PermissionPermissionBitMapperRedisViewset\',\n           #     \'name_space\':\'PermissionPermissionBitMapper.Redis_viewset\'\n           #     }#,\n\n          #  {\n          #      "service_models":"UI",\n          #      "class_name":\'PermissionPermission.Redis_viewset\',\n          #      \'name_space\':\'PermissionPermission.Redis_viewset\'\n          #      },\n   \n          #  {\n          #      "service_models":"UI",\n          #      "class_name":\'PermissionGroupPermissionForItemRedisViewset\',\n          #      \'name_space\':\'PermissionGroupPermissionForItem.Redis_viewset\'\n          #      },\n\n          #  {\n          #      "service_models":"UI",\n          #      "class_name":\'PermissionPermissionForModelRedisViewset\',\n          #      \'name_space\':\'PermissionPermissionForModel.Redis_viewset\'\n          #      },\n\n          #  {\n          #      "service_models":"UI",\n          #      "class_name":\'PermissionGroupBitsMapperRedisViewset\',\n          #      \'name_space\':\'PermissionGroupBitsMapper.Redis_viewset\'\n          #      },\n     \n          #  {\n          #      "service_models":"UI",\n          #      "class_name":\'PermissionGroupRedisViewset\',\n          #      \'name_space\':\'PermissionGroup.Redis_viewset\'\n          #      },\n       \n          #  {\n          #      "service_models":"UI",\n          #      "class_name":\'PermissionUserGroupRedisViewset\',\n          #      \'name_space\':\'PermissionUserGroup.Redis_viewset\'\n          #      },\n          #           {\n          #      "service_models":"UI",\n          #      "class_name":\'ChattingRedisViewset\',\n          #      \'name_space\':\'Chatting.Redis_viewset\'\n          #      },\n   \n          #  {\n          #      "service_models":"UI",\n          #      "class_name":\'UserFollowerRedisViewset\',\n          #      \'name_space\':\'UserFollower.Redis_viewset\'\n          #      },\n          #  {\n          #      "service_models":"UI",\n          #      "class_name":\'UserIsAFollowerCassandraViewset\',\n          #      \'name_space\':\'UserIsAFollower.Cassandra_viewset\'\n          #      },\n          #  {\n          #      "service_models":"UI",\n          #      "class_name":\'UserIsAFollowerPostgresqlViewset\',\n          #      \'name_space\':\'UserIsAFollower.Postgresql_viewset\'\n          #      },\n          #  {\n          #      "service_models":"UI",\n          #      "class_name":\'UserIsAFollowerRedisViewset\',\n          #      \'name_space\':\'UserIsAFollower.Redis_viewset\'\n          #      },\n      \n          #  {\n          #      "service_models":"UI",\n          #      "class_name":\'NotificationRedisViewset\',\n          #      \'name_space\':\'Notification.Redis_viewset\'\n          #      },\n         \n          #  {\n          #      "service_models":"UI",\n          #      "class_name":\'PostCommentRedisViewset\',\n          #      \'name_space\':\'PostComment.Redis_viewset\'\n          #      },\n        \n          #  {\n          #      "service_models":"UI",\n          #      "class_name":\'LikePostRedisViewset\',\n          #      \'name_space\':\'LikePost.Redis_viewset\'\n          #      },\n         \n          #  {\n          #      "service_models":"UI",\n          #      "class_name":\'PostLikedByUserRedisViewset\',\n          #      \'name_space\':\'PostLikedByUser.Redis_viewset\'\n          #      },\n        \n          #  {\n          #      "service_models":"UI",\n          #      "class_name":\'SavePostRedisViewset\',\n          #      \'name_space\':\'SavePost.Redis_viewset\'\n          #      },\n        \n          #  {\n          #      "service_models":"UI",\n          #      "class_name":\'PostRedisViewset\',\n          #      \'name_space\':\'Post.Redis_viewset\'\n          #      },\n        \n          #  {\n          #      "service_models":"UI",\n          #      "class_name":\'UserPostRedisViewset\',\n          #      \'name_space\':\'UserPost.Redis_viewset\'\n          #      },\n   \n          #  {\n          #      "service_models":"UI",\n          #      "class_name":\'PostForUserRedisViewset\',\n          #      \'name_space\':\'PostForUser.Redis_viewset\'\n          #      },\n       \n          #  {\n          #      "service_models":"UI",\n          #      "class_name":\'UserViewRedisViewset\',\n          #      \'name_space\':\'UserView.Redis_viewset\'\n          #      },\n         \n          #  {\n          #      "service_models":"UI",\n          #      "class_name":\'PostViewRedisViewset\',\n          #      \'name_space\':\'PostView.Redis_viewset\'\n          #      },\n        \n          #  {\n          #      "service_models":"UI",\n          #      "class_name":\'CustomUserRedisViewset\',\n          #      \'name_space\':\'CustomUser.Redis_viewset\'\n          #      },\n         \n          #  {\n          #      "service_models":"UI",\n          #      "class_name":\'ProfileRedisViewset\',\n          #      \'name_space\':\'Profile.Redis_viewset\'\n          #      },\n      \n          #  {\n          #      "service_models":"UI",\n          #      "class_name":\'ItemStatusRedisViewset\',\n          #      \'name_space\':\'ItemStatus.Redis_viewset\'\n          #      },\n        \n          #  {\n          #      "service_models":"UI",\n          #      "class_name":\'PersonalRedisViewset\',\n          #      \'name_space\':\'Personal.Redis_viewset\'\n          #      },\n     \n          #  {\n          #      "service_models":"UI",\n          #      "class_name":\'VideoRedisViewset\',\n          #      \'name_space\':\'Video.Redis_viewset\'\n          #      },\n         \n          #  {\n          #      "service_models":"UI",\n          #      "class_name":\'WatchTogetherRedisViewset\',\n          #      \'name_space\':\'WatchTogether.Redis_viewset\'\n          #      }\n\n\n\n          # \n            \n           \n\n            ]\n        }\n\nm =    {\n        "db_manager":[\n            {\n                "service_models":"UI",\n                "class_name":\'PermissionPermissionBitMapperCassandraViewset\',\n                \'name_space\':\'PermissionPermissionBitMapper.Cassandra_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'PermissionPermissionBitMapperPostgresqlViewset\',\n                \'name_space\':\'PermissionPermissionBitMapper.Postgresql_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'PermissionPermissionBitMapperRedisViewset\',\n                \'name_space\':\'PermissionPermissionBitMapper.Redis_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'PermissionPermissionCassandraViewset\',\n                \'name_space\':\'PermissionPermission.Cassandra_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'PermissionPermissionPostgresqlViewset\',\n                \'name_space\':\'PermissionPermission.Postgresql_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'PermissionPermission.Redis_viewset\',\n                \'name_space\':\'PermissionPermission.Redis_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'PermissionGroupPermissionForItemCassandraViewset\',\n                \'name_space\':\'PermissionGroupPermissionForItem.Cassandra_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'PermissionGroupPermissionForItemPostgresqlViewset\',\n                \'name_space\':\'PermissionGroupPermissionForItem.Postgresql_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'PermissionGroupPermissionForItemRedisViewset\',\n                \'name_space\':\'PermissionGroupPermissionForItem.Redis_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'PermissionPermissionForModelCassandraViewset\',\n                \'name_space\':\'PermissionPermissionForModel.Cassandra_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'PermissionPermissionForModelPostgresqlViewset\',\n                \'name_space\':\'PermissionPermissionForModel.Postgresql_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'PermissionPermissionForModelRedisViewset\',\n                \'name_space\':\'PermissionPermissionForModel.Redis_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'PermissionGroupBitsMapperCassandraViewset\',\n                \'name_space\':\'PermissionGroupBitsMapper.Cassandra_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'PermissionGroupBitsMapperPostgresqlViewset\',\n                \'name_space\':\'PermissionGroupBitsMapper.Postgresql_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'PermissionGroupBitsMapperRedisViewset\',\n                \'name_space\':\'PermissionGroupBitsMapper.Redis_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'PermissionGroupCassandraViewset\',\n                \'name_space\':\'PermissionGroup.Cassandra_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'PermissionGroupPostgresqlViewset\',\n                \'name_space\':\'PermissionGroup.Postgresql_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'PermissionGroupRedisViewset\',\n                \'name_space\':\'PermissionGroup.Redis_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'PermissionUserGroupCassandraViewset\',\n                \'name_space\':\'PermissionUserGroup.Cassandra_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'PermissionUserGroupPostgresqlViewset\',\n                \'name_space\':\'PermissionUserGroup.Postgresql_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'PermissionUserGroupRedisViewset\',\n                \'name_space\':\'PermissionUserGroup.Redis_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'ChattingCassandraViewset\', #--------------------------------------------------------------------------------------\n                \'name_space\':\'Chatting.Cassandra_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'ChattingPostgresqlViewset\',\n                \'name_space\':\'Chatting.Postgresql_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'ChattingRedisViewset\',\n                \'name_space\':\'Chatting.Redis_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'UserFollowerCassandraViewset\',\n                \'name_space\':\'UserFollower.Cassandra_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'UserFollower.Postgresql_viewset\',\n                \'name_space\':\'UserFollower.Postgresql_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'UserFollowerRedisViewset\',\n                \'name_space\':\'UserFollower.Redis_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'UserIsAFollowerCassandraViewset\',\n                \'name_space\':\'UserIsAFollower.Cassandra_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'UserIsAFollowerPostgresqlViewset\',\n                \'name_space\':\'UserIsAFollower.Postgresql_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'UserIsAFollowerRedisViewset\',\n                \'name_space\':\'UserIsAFollower.Redis_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'NotificationCassandraViewset\',\n                \'name_space\':\'Notification.Cassandra_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'NotificationPostgresqlViewset\',\n                \'name_space\':\'Notification.Postgresql_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'NotificationRedisViewset\',\n                \'name_space\':\'Notification.Redis_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'PostCommentCassandraViewset\',\n                \'name_space\':\'PostComment.Cassandra_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'PostComment.PostgresqlViewset\',\n                \'name_space\':\'PostComment.Postgresql_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'PostCommentRedisViewset\',\n                \'name_space\':\'PostComment.Redis_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'LikePostCassandraViewset\',\n                \'name_space\':\'LikePost.Cassandra_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'LikePostPostgresqlViewset\',\n                \'name_space\':\'LikePost.Postgresql_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'LikePostRedisViewset\',\n                \'name_space\':\'LikePost.Redis_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'PostLikedByUserCassandraViewset\',\n                \'name_space\':\'PostLikedByUser.Cassandra_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'PostLikedByUserPostgresqlViewset\',\n                \'name_space\':\'PostLikedByUser.Postgresql_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'PostLikedByUserRedisViewset\',\n                \'name_space\':\'PostLikedByUser.Redis_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'SavePostCassandraViewset\',\n                \'name_space\':\'SavePost.Cassandra_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'SavePostPostgresqlViewset\',\n                \'name_space\':\'SavePost.Postgresql_viewset\' #-----------------------done------------------------------------\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'SavePostRedisViewset\',\n                \'name_space\':\'SavePost.Redis_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'PostCassandraViewset\',\n                \'name_space\':\'Post.Cassandra_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'PostPostgresqlViewset\',\n                \'name_space\':\'Post.Postgresql_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'PostRedisViewset\',\n                \'name_space\':\'Post.Redis_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'UserPostCassandraViewset\',\n                \'name_space\':\'UserPost.Cassandra_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'UserPostPostgresqlViewset\',\n                \'name_space\':\'UserPost.Postgresql_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'UserPostRedisViewset\',\n                \'name_space\':\'UserPost.Redis_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'PostForUserCassandraViewset\',\n                \'name_space\':\'PostForUser.Cassandra_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'PostForUserPostgresqlViewset\',\n                \'name_space\':\'PostForUser.Postgresql_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'PostForUserRedisViewset\',\n                \'name_space\':\'PostForUser.Redis_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'UserViewCassandraViewset\',\n                \'name_space\':\'UserView.Cassandra_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'UserViewPostgresqlViewset\',\n                \'name_space\':\'UserView.Postgresql_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'UserViewRedisViewset\',\n                \'name_space\':\'UserView.Redis_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'PostViewCassandraViewset\',\n                \'name_space\':\'PostView.Cassandra_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'PostViewPostgresqlViewset\',\n                \'name_space\':\'PostView.Postgresql_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'PostViewRedisViewset\',\n                \'name_space\':\'PostView.Redis_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'CustomUserCassandraViewset\',\n                \'name_space\':\'CustomUser.Cassandra_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'CustomUserPostgresqlViewset\',\n                \'name_space\':\'CustomUser.Postgresql_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'CustomUserRedisViewset\',\n                \'name_space\':\'CustomUser.Redis_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'ProfileCassandraViewset\',\n                \'name_space\':\'Profile.Cassandra_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'ProfilePostgresqlViewset\',\n                \'name_space\':\'Profile.Postgresql_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'ProfileRedisViewset\',\n                \'name_space\':\'Profile.Redis_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'ItemStatusCassandraViewset\',\n                \'name_space\':\'ItemStatus.Cassandra_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'ItemStatusPostgresqlViewset\',\n                \'name_space\':\'ItemStatus.Postgresql_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'ItemStatusRedisViewset\',\n                \'name_space\':\'ItemStatus.Redis_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'PersonalCassandraViewset\',\n                \'name_space\':\'Personal.Cassandra_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'PersonalPostgresqlViewset\',\n                \'name_space\':\'Personal.Postgresql_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'PersonalRedisViewset\',\n                \'name_space\':\'Personal.Redis_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'VideoCassandraViewset\',\n                \'name_space\':\'Video.Cassandra_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'VideoPostgresqlViewset\',\n                \'name_space\':\'Video.Postgresql_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'VideoRedisViewset\',\n                \'name_space\':\'Video.Redis_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'WatchTogetherCassandraViewset\',\n                \'name_space\':\'WatchTogether.Cassandra_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'WatchTogetherPostgresqlViewset\',\n                \'name_space\':\'WatchTogether.Postgresql_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":\'WatchTogetherRedisViewset\',\n                \'name_space\':\'WatchTogether.Redis_viewset\'\n                }\n\n            ],### manager_container\n        "ui":[\n\n\n            {\n                "service_models":"UI",\n                "class_name":\'LikeViewSet\',\n                \'name_space\':\'likes_api.like_api_viewset\'\n                },\n            {\n                "service_models":"UI",\n                "class_name":"PostViewset",\n                "name_space":"posts_api.post_viewset"\n            },\n            {   \n                "service_models":"UI",\n                "class_name":"SaveViewset",\n                "name_space":"saves_api.save_api_viewset"\n            },\n            {   \n                "service_models":"UI",\n                "class_name":"ShareViewSet",\n                "name_space":"shares_api.share_api_viewset"\n            },\n            {   \n                "service_models":"UI",   \n                "class_name":"StoryViewset",\n                "name_space":"stories_api.stories_viewset"\n            },\n            {   \n                "service_models":"UI",\n                "class_name":"ViewViewset",\n                "name_space":"views_api.views_viewset"\n            },\n            {   \n                "service_models":"UI",\n                "class_name":"CommentViewset",\n                "name_space":"comments_api.comments_viewset"\n            },\n            {\n                "service_models":"UI",\n                "class_name":"FollowerViewset",\n                "name_space":"followers_api.follower_viewset"\n            }\n            ]### ui_container\n        }\n\n\n', 'created_at': '2020-11-09 12:41:54.136417', 'time_stamp': 1604905914.136435, 'import_parameters': '{}'}
{'namespace': 'root_models.namespace', 'created_by': 'sys', 'source_code': '\n#def abc(namespace):\n#    records = (x for x in mr.objects.all() if x.source_url is not None)\n#    url_list = []\n#    for rec in records:\n#        for param in ujson.loads(rec.import_parameters):\n#            if namespace == param:\n#                #print(rec.source_url)\n#                url_list.append(rec.source_url)\n#                break\n#        else:\n#            for param in ujson.loads(rec.import_parameters):\n#                for url in abc(param):\n#                    url_list.append(url)\n#    return url_list\n#\ndef search_for_url(namespace):\n    records = mr.objects.filter(namespace = namespace).allow_filtering()\n    return recods[\'source_urls\']#,recods[]\n\n\ndef filter_core(search_namespace):#### namespace in string #######\n    records =  mr.objects.filter(is_core = False).allow_filtering() \n    for x in records:\n        if x.source_url is None:\n            continue\n        temp_url = (x.source_url,x.namespace,x.sub_url,x.container_type)\n        if search_urls_of_import_namespace(search_namespace,x.import_parameters):\n            yield temp_url\n\n\ndef search_urls_of_import_namespace(search_namespace,current_namespace): ###### search_namespace =\'value\', current_namespace = {\'input01\':{\'impot_namecpace\',[\'imput_parameter\',\'custom\'/\'sys\']}\n    \n    temp_namespace = []\n    for i in ujson.loads(current_namespace):\n        if ujson.loads(current_namespace)[i][1] == "custom" and i != "config":\n            temp_namespace.append(i)\n\n    if search_namespace in temp_namespace:\n        return True\n    else:\n        for x in temp_namespace:\n            records =  mr.objects().filter(namespace = x).allow_filtering().first()\n            if records.is_core:\n                continue\n            temp_imports = records.import_parameters\n            if temp_imports == \'{}\':\n                continue\n\n            if search_urls_of_import_namespace(search_namespace,records.import_parameters):\n                return True\n\n        return False\n\n', 'created_at': '2020-11-09 12:41:54.139692', 'time_stamp': 1604905914.1397085, 'import_parameters': '{"meta_model.container_db": ["MicroserviceResource", "custom"], "ujson": ["sys"]}'}
{'namespace': 'root_models.url_mapper', 'created_by': 'sys', 'source_code': 'class UrlMapper:\n    namespace_url_mape = {} ### {\'namespace_name\':{\'source_url\':"",\'sub_url\':"","microservice_class_name":""},.....}\n\n    def mape_url(self):\n        for x in self.namespace_url_mape:\n            data = MicroserviceResource.objects().get(namespace=x)\n            dd = dict(data)\n            dd["source_url"] = self.namespace_url_mape[x][\'source_url\']\n            dd["microservice_class_name"] = self.namespace_url_mape[x]["microservice_class_name"]\n            dd["sub_url"]= self.namespace_url_mape[x][\'sub_url\']\n            MicroserviceResource(**dd).save()\n\n    def create_mape(self,namespace,source_url,sub_url,microservice_class_name):\n        self.namespace_url_mape[namespace] ={"source_url":source_url,"sub_url":sub_url,"microservice_class_name":microservice_class_name}\n', 'created_at': '2020-11-09 12:41:54.141519', 'time_stamp': 1604905914.141535, 'import_parameters': '{"meta_model.container_db": ["MicroserviceResource", "custom"]}'}
{'namespace': 'root_models.make_file_dir.autodata_insert', 'created_by': 'sys', 'source_code': '\nimp = r\'^import\\ *\'\nfrp = r\'^from\\ *\'\nROOT_PATH =\'base_model\'\n\nsys = [\'ipaddress\',\'asyncio\',\'aiohttp\', \'aiokafka\', \'asn1crypto\', \'async-timeout\', \'attrs\', \'cassandra\', \'catfish\', \'certifi\', \'chardet\', \'command-not-found\',\'copy\', \'cryptography\', \'cupshelpers\', \'defer\', \'distro-info\', \'dlib\', \'enum34\', \'httplib2\', \'idna\', \'idna-ssl\', \'kafka-python\', \'keyring\', \'keyrings.alt\', \'language-selector\', \'launchpadlib\', \'lazr.restfulclient\', \'lazr.uri\', \'lightdm-gtk-greeter-settings\', \'macaroonbakery\', \'menulibre\', \'mugshot\', \'multidict\', \'netifaces\', \'nginx-config-builder\', \'oauth\', \'olefile\', \'onboard\', \'pexpect\', \'Pillow\', \'protobuf\', \'psutil\', \'pycairo\', \'pycrypto\', \'pycups\', \'pycurl\', \'pygobject\', \'pymacaroons\', \'PyNaCl\', \'pyRFC3339\', \'python-apt\', \'python-dateutil\', \'python-debian\', \'python-debianbts\', \'python-magic\', \'pytz\', \'pyxdg\', \'PyYAML\', \'redis\',\'reportlab\', \'requests\', \'requests-unixsocket\', \'SecretStorage\', \'sgt-launcher\', \'simplejson\', \'six\', \'sqlalchemy\', \'ssh-import-id\', \'system-service\', \'systemd-python\', \'threading\',\'typing-extensions\', \'ubuntu-drivers-common\', \'ufw\', \'ujson\', \'unattended-upgrades\', \'urllib3\', \'usb-creator\', \'uuid\', \'wadllib\', \'xcffib\', \'xkit\', \'yarl\', \'zipfile\', \'zope.interface\',\'os\',\'pathlib\',\'json\',\'re\',\'time\',\'datetime\',\'uuid\',\'validator_collection\']\n\ndef import_seperator(file_path):\n    imstr = []\n    rimstr =""\n    fn = file_path.split(\'/\')[1:]\n    fn[len(fn)-1] = fn[len(fn)-1].split(\'.\')[0]\n    fn = \'.\'.join(fn)\n    #print(fn)\n\n    with open(file_path,\'r\') as f:\n        s = f.readline()\n        while s:\n            if re.match(imp,s) is not None:\n                imstr.append(s.strip())\n            elif re.match(frp,s) is not None:\n                imstr.append(s.strip())\n            else:\n                rimstr+=s\n            s = f.readline()\n    \n    return imstr,rimstr,fn\n\n\ndef package_reader(path):\n    for filename in os.listdir(path):\n        ff = path+\'/\'+filename\n        p = pathlib.Path(ff)\n        if not p.is_dir():\n            ss = filename.split(\'.\')\n            if len(ss) == 2:\n                if ss[0] !=\'__init__\':\n                    if ss[1] ==\'py\':\n                        \n                        yield import_seperator(path+\'/\'+filename)\n        else:\n            #print(ff+\'  \'+filename)\n            yield from package_reader(ff)\n\ndef insert_package(package_name,root_path=ROOT_PATH):\n    for x in package_reader(root_path+\'/\'+package_name):\n        #print(x)\n        model_list = {}\n        for y in x[0]:\n            ss = y.split(\' \')\n            lib = ss[1].split(\'.\')\n            flag = "custom" \n            if lib[0] in sys:\n                flag = "sys"\n            if ss[0] =="from":\n                if lib[0]=="base_model":\n                    ss[1] = ss[1].replace("base_model.","")\n                model_list[ss[1]]=[ss[3],flag]\n            else:\n                model_list[ss[1]]=[flag]\n        data = {\'namespace\':x[2],\'created_by\':\'sys\',\'source_code\':x[1],\'created_at\':str(datetime.now()),\'time_stamp\':time.time(),\'import_parameters\':json.dumps(model_list)}\n        ob = MicroserviceResource(**data)\n        ob.save()\n\n', 'created_at': '2020-11-09 12:41:54.143644', 'time_stamp': 1604905914.1436605, 'import_parameters': '{"re": ["sys"], "os": ["sys"], "pathlib": ["sys"], "time": ["sys"], "meta_model.container_db": ["MicroserviceResource", "custom"], "datetime": ["datetime", "sys"], "json": ["sys"]}'}
{'namespace': 'root_models.zip_creater', 'created_by': 'sys', 'source_code': '# import required modules\n \n \n# Declare the function to return all file paths of a particular directory\ndef retrieve_file_paths(dirName):\n \n  # setup file paths variable\n  filePaths = []\n   \n  # Read all directory, subdirectories and file lists\n  for root, directories, files in os.walk(dirName):\n    for filename in files:\n      # Create the full filepath by using os module.\n      filePath = os.path.join(root, filename)\n      filePaths.append(filePath)\n       \n  # return all paths\n  return filePaths\n \n \n# Declare the main function\ndef zipe_maker(dir_name):\n \n  # Set the zip file name\n  zipFileName = dir_name + ".zip"\n   \n  # Call the function to retrieve all files and folders of the assigned directory\n  filePaths = retrieve_file_paths(dir_name)\n   \n  # print the list of files to be zipped\n  print(\'The following list of files will be zipped:\')\n  for fileName in filePaths:\n    print(fileName)\n   \n  # write files and folders to a zipfile\n  zip_file = zipfile.ZipFile(zipFileName, \'w\')\n  with zip_file:\n    # write each file seperately\n    for file in filePaths:\n      zip_file.write(file)\n       \n  print(zipFileName+\' file is created successfully!\')\n   \ndef extract_zip_file(source,destination):\n    with zipfile.ZipFile(source, \'r\') as zip_ref:\n        zip_ref.extractall(destination)\n', 'created_at': '2020-11-09 12:41:54.146259', 'time_stamp': 1604905914.1462822, 'import_parameters': '{"os": ["sys"], "sys": ["sys"], "zipfile": ["sys"]}'}
{'namespace': 'root_models.autodata_insert', 'created_by': 'sys', 'source_code': '\nimp = r\'^import\\ *\'\nfrp = r\'^from\\ *\'\nROOT_PATH =\'project_file_system/base_model\'\n\nsys = [\'ipaddress\',\'asyncio\',\'aiohttp\', \'aiokafka\', \'asn1crypto\', \'async-timeout\', \'attrs\', \'cassandra\', \'catfish\', \'certifi\', \'chardet\', \'command-not-found\',\'copy\', \'cryptography\', \'cupshelpers\', \'defer\', \'distro-info\', \'dlib\', \'enum34\', \'httplib2\', \'idna\', \'idna-ssl\', \'kafka-python\', \'keyring\', \'keyrings.alt\', \'language-selector\', \'launchpadlib\', \'lazr.restfulclient\', \'lazr.uri\', \'lightdm-gtk-greeter-settings\', \'macaroonbakery\', \'menulibre\', \'mugshot\', \'multidict\', \'netifaces\', \'nginx-config-builder\', \'oauth\', \'olefile\', \'onboard\', \'pexpect\', \'Pillow\', \'protobuf\', \'psutil\', \'pycairo\', \'pycrypto\', \'pycups\', \'pycurl\', \'pygobject\', \'pymacaroons\', \'PyNaCl\', \'pyRFC3339\', \'python-apt\', \'python-dateutil\', \'python-debian\', \'python-debianbts\', \'python-magic\', \'pytz\', \'pyxdg\', \'PyYAML\', \'redis\',\'reportlab\', \'requests\', \'requests-unixsocket\', \'SecretStorage\', \'sgt-launcher\', \'simplejson\', \'six\', \'sqlalchemy\', \'ssh-import-id\', \'system-service\', \'systemd-python\', \'threading\',\'typing-extensions\', \'ubuntu-drivers-common\', \'ufw\', \'ujson\', \'unattended-upgrades\', \'urllib3\', \'usb-creator\', \'wadllib\', \'xcffib\', \'xkit\', \'yarl\', \'zope.interface\',\'os\',\'pathlib\',\'json\',\'re\',\'time\',\'datetime\',\'uuid\',\'validator_collection\']\n\nclass NamespaceFileMapper:\n    \n\n    def __init__(self):\n        pass\n       # print("haiiii")\n        # self.generate_file_modify_time(ROOT_PATH)\n        # print(self.modified_file_structure)\n        # self.file_structure = self.modified_file_structure\n        # self.modified_file_structure = {}\n\n    modified_file_structure = {}\n    file_structure = {}\n\n\n    \n        \n    def import_seperator(self,file_path):\n        imstr = []\n        rimstr =""\n        fn = file_path.split(\'/\')[1:]\n        fn[len(fn)-1] = fn[len(fn)-1].split(\'.\')[0]\n        fn = \'.\'.join(fn)\n        #print(fn)\n    \n        with open(file_path,\'r\') as f:\n            s = f.readline()\n            while s:\n                if re.match(imp,s) is not None:\n                    imstr.append(s.strip())\n                elif re.match(frp,s) is not None:\n                    imstr.append(s.strip())\n                else:\n                    rimstr+=s\n                s = f.readline()\n        \n        return imstr,rimstr,fn\n    \n    \n    def package_reader(self,path):\n        for filename in os.listdir(path):\n            ff = path+\'/\'+filename\n            p = pathlib.Path(ff)\n            if not p.is_dir():\n                if (ff not in self.updated_file_list) and (ff not in self.new_file_list):\n                    return\n                ss = filename.split(\'.\')\n                if len(ss) == 2:\n                    if ss[0] !=\'__init__\':\n                        if ss[1] ==\'py\':\n                            \n                            yield self.import_seperator(path+\'/\'+filename)\n            else:\n                #print(ff+\'  \'+filename)\n                yield from self.package_reader(ff)\n    \n    def insert_package(self,package_name):\n        namespace_list = []\n        for x in self.package_reader(ROOT_PATH+\'/\'+package_name):\n            #print(x)\n            model_list = {}\n            for y in x[0]:\n                ss = y.split(\' \')\n                lib = ss[1].split(\'.\')\n                flag = "custom" \n                if lib[0] in sys:\n                    flag = "sys"\n                if ss[0] =="from":\n                    if lib[0]=="base_model":\n                        ss[1] = ss[1].replace("base_model.","")\n                    model_list[ss[1]]=[ss[3],flag]\n                else:\n                    model_list[ss[1]]=[flag]\n            data = {\'namespace\':x[2],\'created_by\':\'sys\',\'source_code\':x[1],\'created_at\':str(datetime.now()),\'time_stamp\':time.time(),\'import_parameters\':json.dumps(model_list)}\n            namespace_list.append(x[2])\n            ob = MicroserviceResource(**data)\n            ob.save()\n        return namespace_list\n\n    def generate_file_modify_time(self,path):\n        #print(self.modified_file_structure)\n\n        for f in os.listdir(path):\n            ff = path+\'/\'+f\n            #print(ff)\n            self.modified_file_structure[ff] = os.stat(ff).st_mtime\n            if pathlib.Path(ff).is_dir():\n                self.generate_file_modify_time(ff)\n\n    def generate_updated_file_list(self):\n        self.updated_file_list = []\n        self.new_file_list = []\n        for x in self.modified_file_structure:\n            if x not in self.file_structure.keys():\n                self.new_file_list.append(x) ### for new file models\n            else:\n                if (self.modified_file_structure[x]  - self.file_structure[x]) >0.0: ### findoiut latest file or folder\n                    self.updated_file_list.append(x)\n\n    def generate_delete_file_list(self):\n        m_keys = list(self.modified_file_structure.keys())\n        self.delete_file_list = []\n        for x in self.file_structure:\n            if x not in m_keys:\n                self.delete_file_list.append(x)\n\n\n\n    async def check_all_files(self): \n        \'\'\'  Note: that it  is a backen task \'\'\'\n        \n        root_path = ROOT_PATH\n        \'\'\'This function will check for any update in file structure and if there is any update in fiel structure it will reflact all this changes to the containers of the system.\'\'\'\n        while not self.REDUCE_FLAG[0]:\n            #print(\'haii\')\n            self.generate_file_modify_time(root_path)\n            #print(self.file_structure)\n            if self.file_structure == {}:\n                self.file_structure = self.modified_file_structure.copy()\n            self.generate_updated_file_list()\n            self.generate_delete_file_list()\n            if len(self.updated_file_list) != 0 or len(self.new_file_list) != 0: \n                 file_list =os.listdir(root_path)\n                 for x in file_list: \n                     p = pathlib.Path(x)\n                     if p.is_dir():\n                        self.insert_package(x)\n                 await self.update_namespace_in_system()\n                 await self.insert_namespace_to_system()\n                 self.file_structure = self.modified_file_structure.copy()\n                 \n            if len(self.delete_file_list) !=0:\n                await self.delete_namespace_from_system()\n\n\n\n\n    async def delete_namespace_from_system(self):\n        temp_delete_urls = []\n        temp_update_urls = []\n        for x in self.delete_file_list:\n            \n            r = x.split(\'.py\')\n            if len(r) ==2: #### so it is a python file \n                ##### generate the namespace ####\n                namespace = \'.\'.join(r[0].split(\'/\')[2:]) ### \'.\'.join([\'rf01/rf02/file\',\'\'][0].split(\'/\')[1:])\n                if \'__pycache__\' in namespace:\n                    continue\n                ob = MicroserviceResource.objects().filter(namespace = namespace).allow_filtering().first()\n                if not ob.is_core:\n                    if ob.source_url !=\'\':\n                        temp_delete_url.append((ob.source_url,ob.namespace,ob.container_type))\n                        await self.search_container_for_namespace_delete(ob.container_type,ob.source_url,ob.namespace)\n\n                for x in filter_core(ob.namespace):\n                    \n                    await self.search_container_for_namespace_restart(x[3],x[0],x[1]) ## return tuple (x.source_url,x.namespace,x.sub_url,x.container_type)\n\n\n##### we need to find out the container type and urls #####\n                    \n\n                \n    async def update_namespace_in_system(self):\n        temp_delete_urls = []\n        temp_update_urls = []\n        for x in self.updated_file_list:\n            \n            r = x.split(\'.py\')\n            if len(r) ==2: #### so it is a python file \n                ##### generate the namespace ####\n                namespace = \'.\'.join(r[0].split(\'/\')[2:]) ### \'.\'.join([\'rf01/rf02/file\',\'\'][0].split(\'/\')[1:])\n                if \'__pycache__\' in namespace:\n                    continue\n                ob = MicroserviceResource.objects().filter(namespace = namespace).allow_filtering().first()\n                if not ob.is_core:\n                    if ob.source_url !=\'\':\n                        temp_delete_url.append((ob.source_url,ob.namespace,ob.container_type))\n                        await self.search_container_for_namespace_restart(ob.container_type,ob.source_url,ob.namespace)\n\n                for x in filter_core(ob.namespace):\n\n                    await self.search_container_for_namespace_restart(x[3],x[0],x[1])\n\n        \n    async def insert_namespace_to_system(self):\n        \n        temp_delete_urls = []\n        temp_update_urls = []\n        for x in self.new_file_list:\n            \n            r = x.split(\'.py\')\n            if len(r) ==2: #### so it is a python file \n                ##### generate the namespace ####\n                namespace = \'.\'.join(r[0].split(\'/\')[2:]) ### \'.\'.join([\'rf01/rf02/file\',\'\'][0].split(\'/\')[1:])\n                #print(namespace)\n                if \'__pycache__\' in namespace:\n                    continue\n                if \'__init__\' in namespace:\n                    continue\n                #print(namespace)\n                ob = MicroserviceResource.objects().filter(namespace = namespace).allow_filtering().first()\n\n                \n                if not ob.is_core:\n                    if ob.source_url !=\'\':\n                        task = {\n                          "class_name":ob.microservice_class_name ,\n                          \'name_space\':ob.namespace\n                          }\n                        await self.assing_container(task)\n\n\n    async def filter_applications_from_update_insert_list(self):\n        total_changed_files = self.new_file_list + self.updated_file_list\n        temp_file_apps = []\n        for x in total_changed_files: ### all files list \n            file_apps_name = x.splite(\'/\')[2] #### [base_model,apps_name,namespace.py]\n            if file_apps_name == \'config.py\':\n                continue\n            ############# find all apps name ##########\n            try:\n                temp_file_apps.index(file_apps_name)\n            except:\n                temp_file_apps.append(file_apps_name)\n\n        for x in temp_file_apps:\n            self.insert_package(x)\n\n        total_namespace = []\n\n        for x in total_changed_files:\n            temp_file_parts = x.splite(\'.py\')\n            if len(temp_file_parts) == 2:\n                namespace=\'.\'.join(temp_file_parts[0].splite(\'/\')[2:])\n                #total_namespace.append()\n                for x in filter_core(namespace):\n                    total_namespace.append(x)\n\n            else:\n                total_namespace = total_namespace + url_mapper(x)\n\n        \n                \n        total_namespace = list(dict.fromkeys(total_namespace))\n        for x in total_namespace:\n            await self.search_container_for_namespace_restart(x[3],x[0],x[1])\n\n\n#url_mapper(model_folder,root_path = \'\')\n', 'created_at': '2020-11-09 12:41:54.149528', 'time_stamp': 1604905914.1495435, 'import_parameters': '{"re": ["sys"], "os": ["sys"], "pathlib": ["sys"], "time": ["sys"], "meta_model.container_db": ["MicroserviceResource", "custom"], "datetime": ["datetime", "sys"], "json": ["sys"], "root_models.namespace": ["filter_core", "custom"]}'}
{'namespace': 'root_models.monitor_client_microservices', 'created_by': 'sys', 'source_code': '\nclass MonitorAnalysisModels(GenClass):\n\tfunction_dict_da= {\n\t"/generat_debugging_info":["generat_debugging_info",\'socket\'],\n\t"/start_debugging":["start_debugging",\'socket\'],\n\t\'/stop_debugging\':[\'stop_debugging\',\'socket\'],\n\t#\'/upload_source_code\':[\'upload_source_code\',\'socket\']\n\t}\n\t#function_dict_internal_communication= {\n\t#"/get_all_debugging_info":["get_all_debugging_info",\'socket\']\n\t#}\n\tasync def get_all_debugging_info(self,request): ### not here \n\t\tip = get_ip(request)\n\t\tself.thread_manager_obj.service_debugging_info[ip] = request.data\n\t\treturn \'Done\'\n\n\tasync def generat_debugging_info(self,request):\n\t\treturn ujson.loads(self.thread_manager_obj.service_debugging_info)\n\n\n\n\tasync def search_container_for_start_namespace_debugging(self,url_type,url,namespace):\n\t\tip_list = self.thread_manager_obj.nginxconfig_obj.conf_list(url_type,url) ### [ip1,ip2,....]\n\t\tfor x in ip_list:\n\t\t\tawait  self.thread_manager_obj.start_debugging_to_container_microservices(self.loop_index,x,namespace)\n\n\tasync def search_container_for_stop_namespace_debugging(self,url_type,url,namespace):\n\t\tip_list = nginxconfig_obj.conf_list(url_type,url) ### [ip1,ip2,....]\n\t\tfor x in ip_list:\n\t\t\tawait self.thread_manager_obj.stop_debugging_to_container_microservices(self.current_loop_index,x,namespace)\n\n\n\tasync def start_debugging(self,request):\n\t\tnamespace = request.data[\'namespace\']\n\t\ttry:\n\t\t\turls = request.data[\'urls\']\n\t\texcept:\n\t\t\tpass\n\n\t\tawait self.thread_manager_obj.search_container_for_start_namespace_debugging(url_type,urls,namespace)\n\n\n\n\n\tasync def stop_debugging(self,request):\n\t\tnamespace = request.data[\'namespace\']\n\t\ttry:\n\t\t\turls = request.data[\'urls\']\n\t\texcept:\n\t\t\tpass\n\n\t\tawait self.thread_manager_obj.search_container_for_stop_namespace_debugging(url_type,urls,namespace)\n', 'created_at': '2020-11-09 12:41:54.152586', 'time_stamp': 1604905914.1526036, 'import_parameters': '{"ujson": ["sys"], "common_models.common_api_lib": ["GenClass", "custom"]}'}
{'namespace': 'root_models.myconfig', 'created_by': 'sys', 'source_code': '#im = import_model.import_manager_obj\n#im.add_namespace(\'root_models.nginx_config_generator\')\nob = NginxConfig()\n\n\n\n\n\n#request = {\n#    "url": "new_list",\n#    "url_type": "mn_con",\n#    "ip": "127.0.0.1",\n#    "port": "9998",\n#    "weight": "5",\n#    "protocol":"http\'/\'stream",\n#    "method": "RedisViewSet/viewset/Cassandra/list"\n#    }\n#\n#r = {}\n#\n#data_dir = {\n#        "model_name":"RedisViewSet",\n#        "table_info":{\n#            "name":{\n#                "type":"string"\n#                },\n#            "age":{\n#                \'type\':"int"\n#                },\n#            \'index\':[\'name\',\'age\']\n#            },\n#        "redis_type":"list",\n#        "data_base_type":["*"],\n#        "data_server":\'default\',\n#        \'backup_db\':[\'*\'],\n#        \'operation\':[\'*\']\n#        }\n#\ndef db_structure_to_nginx_mapping(data_dir):\n    global ob\n    op = ["list","update",\'retrieve\',\'delete\',\'create\',\'partial_update\',\'destroy\']\n    db_type = [\'Cassandra\',\'Redis\',\'Postgresql\']\n    if data_dir[\'data_base_type\'] != ["*"]:\n        db_type = data_dir[\'data_base_type\']\n    if data_dir[\'operation\'] !=[\'*\']:\n        op = data_dir[\'operation\']\n\n    i = 0\n    \n    for x in db_type:\n        for y in op:\n            temp_str = data_dir[\'model_name\']+\'/viewset/\'+x+\'/\'+y\n            temp_url = data_dir[\'model_name\']+\'_viewset_\'+x+\'_\'+y\n    \n            r = {\n            "url": temp_url,\n            "url_type": "mn_con",\n            "ip": data_dir["ip"],\n            "port": str(data_dir["port"]+i),\n            "weight": \'5\',\n            "protocol":"http",\n            "method": temp_str\n            }\n            \n            ob.conf_create(r)\n        i+=1\n\n#data_dir[\'ip\'] = \'127.0.0.1\'\n#data_dir[\'port\'] = 9998\n#db_structure_to_nginx_mapping(data_dir)\n\n\ndef app_struture_to_nginx_mapping(data_dir):\n    global ob\n    op = ["list","update",\'retrieve\',\'delete\',\'create\',\'partial_update\',\'destroy\']\n    class_name = data_dir[\'class_name\']\n    if data_dir[\'operation\'] !=[\'*\']:\n        op = data_dir[\'operation\']\n    for y in op:\n        temp_str = data_dir[\'model_name\']+"/"+class_name+"/"+y\n        temp_url = data_dir[\'model_name\']+"_"+class_name+"_"+y\n        r= {\n                "url":temp_url,\n                "url_type":data_dir[\'url_type\'],\n                "ip":data_dir["ip"],\n                "port":data_dir["port"],\n                "weight":data_dir["weight"],\n                "protocol":data_dir["protocol"],\n                "method":temp_str\n            }\n        ob.conf_create(r)\n\n\n', 'created_at': '2020-11-09 12:41:54.155031', 'time_stamp': 1604905914.1550493, 'import_parameters': '{"suport_thread": ["import_model", "custom"], "root_models.nginx_config_generator": ["NginxConfig", "custom"]}'}
{'namespace': 'root_models.monitor_suppervisor_microservices', 'created_by': 'sys', 'source_code': 'class RootServiceForMannageSupervisor:\n\n    async def assing_container(self,task):\n        """This function select container which has minimum number of corouting"""\n\n        #server_info = ServerInfo.objects().all()\n        #mini_threads = 2333\n        print(\'Try to assing a container\')\n        no_container =  ContainerObject.objects().filter(container_type=task[\'container_type\']).count()\n\n        selected_server = None\n        if no_container >0:\n            ip_address,selected_server,no_corouting = await self.short_conatiner_by_no_of_running_corouting(self.loop_index,task[\'container_type\']) ### return {\'container_ip\':"","no_of_corouting":}\n            if no_corouting >= MAXIMUM_COROUTING-LAMDA_DISTANCE:\n                ip_address = self.create_container(task[\'container_type\'])\n                self.add_container_pending_work(ip_address,task)\n\n            else:\n                await self.start_assing_task_to_container(ip_address,task)\n                microservice_info = await self.get_current_microservice_information(self.loop_index,ip_address).data\n                task[\'port\'] = microservice_info[\'port\']\n                self.nginx_mapping(task)\n        else:\n            ip_address = self.create_container(task[\'container_type\'])\n            self.add_container_pending_work(ip_address,task)\n\n\n\n        return \'Done\'\n\n    async def manage_container(self,request):\n        """This function will assign the function to a selected container"""\n        ip = get_ip(request)\n        task = request.data\n        ob = ContainerObjectNetworking.objects().get(ip_address = ip)\n        task[\'container_type\'] = ob.container_type\n        # url = request.data[\'task\'][\'url\']\n\n        # request ={\n        #     "url":task[\'url\'],\n        #     "ip":task[\'ip\'],\n        #     "port":task[\'port\'],\n        #     "weight":1,\n        #     }\n        #nginx_conf.update(request)\n        await self.assing_container(task)\n        return \'Done\'\n\n    async def get_all_debugging_info(self,request): ### not here\n                ip = get_ip(request)\n                self.service_debugging_info[ip] = request.data\n                return \'Done\'\n    def get_ip(self,request):\n        client_address = None\n        try:\n            # case server 200.000.02.001\n            client_address = request.META["HTTP_X_FORWARDED_FOR"]\n        except:\n            # case localhost ou 127.0.0.1\n            try:\n                client_address = request.META["REMOTE_ADDR"]\n            except:\n                client_address = str(request.remote)\n        return client_address\n\n\n    async def initialize_suppervizor(self,request):\n            ip = self.get_ip(request)\n            container_info =ContainerObjectNetworking.objects.filter(ip_address = ip).allow_filtering().first()#ContainerObject.objects.get(ip_address = ip)\n            container_name = container_info.container_name\n            container_info = ContainerObjectNetworking.objects.filter(container_name = container_name).allow_filtering()\n            network_info = {}\n            container_type = container_info.first().container_type\n            for x in container_info:\n                temp_key = x.inter_face_code.split(container_type+\'_\')\n                if len(temp_key)==2:\n                    network_info[temp_key[1]] = x.ip_address ###     MANAGER_IN_NET\n                else:\n                    network_info[x.inter_face_code] = x.ip_address ###     MONITER_INTERFACE\n\n            try:\n                task = self.container_panding_task[ip]\n                s = {\'service\':\'monitor\',\'service_fun\':\'assing_task_to_container\'}\n                s[\'task\'] = task\n                s[\'ip\'] = ip\n\n                self.corouting_manager_obj.add_pending_task_list(s)\n\n            except:\n                pass\n\n            total_info = {}\n\n            total_info[\'NETWORK_INTERFACE\'] = network_info\n            total_info[\'NGINX\'] = ContanerTypeToNginxMapping[container_type.split(\'_\')[1].upper()]\n            print(total_info)\n\n            return ujson.dumps(total_info)\n\n\n#    async def get_all_debugging_info(self,request): ### not here\n#                ip = get_ip(request)\n#                self.service_debugging_info[ip] = request.data\n#                return \'Done\'\n#\n\n    def nginx_mapping(self,task):\n        \n        ob = MicroserviceResource.objects().filter(namespace = task[\'name_space\']).allow_filtering().first()\n\n\n        #info= UrlMethodMapping.objects().get(url=resource_url)\n        #url_type=info.url_type\n\n        #protocal = UrlTypeProtocalMapping.objects().get(url_type = url_type).protocal\n        urls = []\n        request = {\'configes\':[],\'url_type\':\'\'}\n\n        try:\n            urls = task[\'urls\']\n\n        except:\n            urls = []\n            for x in ujson.loads(ob.sub_url):\n                urls.append(ob.source_url+x)\n\n        for x in urls:\n            request[\'configes\'].append({\n                    "url":x,\n                    "url_type":\'mn_con\' if ob.container_type is None else ContanerTypeToNginxMapping[ob.container_type.split(\'_\')[1].upper()][\'url_type\'],\n                    "ip":task[\'ip\'],\n                    "port":task[\'port\'],\n                    "weight":5,\n                   # "server":x,\n                    "protocol":\'http\',\n                    "method":x\n                    }\n                    )\n        request[\'url_type\'] = \'mn_con\' if ob.container_type is None else ContanerTypeToNginxMapping[ob.container_type.split(\'_\')[1].upper()][\'url_type\']\n        print(request)\n\n\n        self.nginx_conf.conf_multiple_create(request)\n        #self.nginx_conf.conf_create(request)\n\n\n    def find_out_container(self,con_type):\n        \'\'\' This function find the container from the database which is shutdowned. self.find_out_container(con_type)\'\'\'\n        for x in  ContainerObject.objects().filter(container_type=con_type).allow_filtering():\n            if x.status == "stop":\n                return x.container_name\n\n    def create_container(self,con_type = \'mn_con\'):\n        """This function creates new container according to con_type(db_manager/monitor/ui) as per defined in db.\n        \'req_master\' is for the supervisor information such as Port and Ip address\n        as common container requires it where as db_manager/ui does not require it."""\n        container_name = None\n        container_name = self.find_out_container(con_type)\n        if container_name is None:\n\n            container_info = ContainerIso.objects.get(container_type = con_type)\n            container_name = container_info.container_type+str(datetime.now().timestamp())\n            image_name = container_info.container_iso\n            dir_name = container_info.container_dir\n            print(\'haiiiiiiii \',image_name,container_name)\n            print(\'make IMAGE_NAME=\'+image_name+\' CONTAINER_NAME=\'+container_name+\' create_container\')\n            #os.system(\'make IMAGE_NAME=\'+image_name+\' CONTAINER_NAME=\'+container_name+\' create_container\')\n            os.system(\'ssh -o StrictHostKeyChecking=no -i my_secreat -tt miworld_monitor@172.17.0.1 "echo maa@12345|sudo -S make IMAGE_NAME=\'+image_name+\' CONTAINER_NAME=\'+container_name+\' create_container"\')\n        ips, codes = self.ip_generator(con_type)\n        print(ips)\n\n        #os.system(\'sudo make IMAGE_NAME=\'+image_name+\' DIR_NAME=\'+dir_name+\' compile\')\n        #os.system(\'sudo make IMAGE_NAME=\'+image_name+\' CONTAINER_NAME=\'+container_name+\' run\')\n\n        for ip,code in zip (ips,codes):\n            # data = {\'ip_address\':ips[ip],\'container_type\':con_type,\'container_name\':container_name,\'threads\':1,\'inter_face_code\':code}\n            data = {\'ip_address\':ips[ip],\'container_type\':con_type,\'container_name\':container_name,\'inter_face_code\':code}\n            os.system(\'ssh -o StrictHostKeyChecking=no -i my_secreat -tt miworld_monitor@172.17.0.1 "echo maa@12345|sudo -S make IP_ADDRESS=\'+ips[ip]+\' CONTAINER_NAME=\'+container_name+\' INTERFACE=\'+ip+\' net_connect"\')\n#            os.system(\'make IP_ADDRESS=\'+ips[ip]+\' CONTAINER_NAME=\'+container_name+\' INTERFACE=\'+ip+\' net_connect\')\n            print(data)\n            #ContainerObject(**data).save()\n            ContainerObjectNetworking(**data).save()\n        ContainerObject(container_name=container_name,container_type=con_type,status=\'running\').save()\n        print(print(image_name,container_name))\n#        os.system(\'make IMAGE_NAME=\'+image_name+\' CONTAINER_NAME=\'+container_name+\' run\')            \n        os.system(\'ssh -o StrictHostKeyChecking=no -i my_secreat -tt miworld_monitor@172.17.0.1 "echo maa@12345|sudo -S make IMAGE_NAME=\'+image_name+\' CONTAINER_NAME=\'+container_name+\' run"\')\n\n        return (ips[\'monitor_bridge\'])\n        #else:\n        #    ip_address = list(ips.values())[0]\n        #    data = {\'ip_address\':ip_address,\'container_type\':con_type,\'container_name\':container_name,\'inter_face_code\':codes[0]}\n        #    ContainerObject(**data).save()\n        #    os.system(\'sudo make IP_ADDRESS=\'+ip_address+\' CONTAINER_NAME=\'+container_name+\' INTERFACE=\'+code[0]+\' net_connect\')\n        #    os.system(\'sudo make IMAGE_NAME=\'+image_name+\' CONTAINER_NAME=\'+container_name+\' run\')\n        #    return ip_address[0]\n\n    def ip_generator(self,container_type):\n        """ Generate Ip addresses by container type where container type is str. """\n    \n        ps = ContainerNetworking.objects.filter(container_type = container_type).allow_filtering()\n        ip_list = {}\n        code_list = []\n        for x in ps:\n            code = x.inter_face_type\n            nm = NetworkInterface.objects.filter(inter_face_type = code).allow_filtering().first()\n            count = ContainerObject.objects.filter(container_type = container_type).allow_filtering().count()\n            count +=10\n            netmask = nm.netmask\n            base_ip = nm.base_ip\n            max_limit = (2**(32-netmask)) - 3\n            if count <= max_limit:\n                ip = IPv4Address(base_ip) + (count+2)\n                ip_list[nm.inter_face_name] = str(ip)\n                code_list.append(code)\n        return ip_list, code_list\n\n\n\n    def add_container_pending_work(self,ip_address,task):\n        #s = {\'service\':\'monitor\',\'service_fun\':\'assing_task_to_container\'}\n        self.container_panding_task[ip_address] = task\n        task[\'ip\'] = ip_address\n        #self.corouting_manager_obj.add_pending_task_list(s)\n\n\n\n\n################################################# backend task ##########################################################\n    async def assing_task_to_container(self,loop_index,meta_coro_work):\n        task = meta_coro_work[\'task\']\n        await asyncio.sleep(20)\n\n        port_info = await self.start_assing_task_to_container(self.loop_index,meta_coro_work[\'ip\'],task)\n        task[\'port\'] = ujson.loads(port_info)[\'port\']\n        \n\n        self.nginx_mapping(task)\n################################################ web service #############################################################\n', 'created_at': '2020-11-09 12:41:54.157964', 'time_stamp': 1604905914.157981, 'import_parameters': '{"os": ["sys"], "datetime": ["datetime", "sys"], "ipaddress": ["IPv4Address", "sys"], "meta_model.container_db": ["ContainerObject,ContainerIso,NetworkInterface,ContainerObjectNetworking,ContainerNetworking,UrlMethodMapping,UrlTypeProtocalMapping,ContanerTypeToNginxMapping,MicroserviceResource", "custom"], "ujson": ["sys"], "asyncio": ["sys"]}'}
{'namespace': 'root_models.root_process', 'created_by': 'sys', 'source_code': '\n#from base_model.thread_models.thread_loop import thread_manager\n\n#from base_model.suport_thread.import_model import import_manager_obj\n#from models import *\n#from common_api_lib import router\n#from socket_manager import socket_manager_obj\n\n#from base_model.root_models.root_services import RootServiceForMannageSupervisor\n\n\nMONITOR_INITIAL_TASKS = init_task\n\n\n#,MONITOR_INITIAL_TASKS\nNGINX_UNIT_URL  = \'http://localhost/config/listeners/*:8000\'\nMAXIMUM_THREADs = 4\nMASTER_PORT = 99\nMASTER_IP_INDEX = 0\nsocket_obj = SocketManagerClass()\nLAMDA_DISTANCE = 15\nclass monitor(NamespaceFileMapper,GenClass,thread_manager,RootManagementServices,RootToSupervisorCommunication,RootServiceForMannageSupervisor):\n    """This is the root point of the system.\n    The monitor checks system status and allocate resources according their requirement"""\n    monitor_client_service = None\n    function_dict= {\n            "/mannager_container":["manage_container",\'socket\'],\n            #"/register_fun":["register_microservice",\'socket\'],\n            "/initialize_suppervizor":["initialize_suppervizor",\'socket\'],\n            \'/get_all_debugging_info\':[\'get_all_debugging_info\',\'socket\'],\n           # \'/upload_source_code\':[\'upload_source_code\',\'socket\']\n            }\n    #function_list =[\'assing_task_to_container\']\n\n   # function_dict = {\n   #         \'/create\':[\'create\',\'post\'],\n   #         \'/update\':[\'update\',\'post\'],\n   #         \'/list\':[\'list\',\'post\'],\n   #         \'/delete\':[\'delete\',\'post\'],\n   #         \'/showdown\':[\'showdown\',\'post\'],\n   #         \'/kafka_reset\':[\'kafka_model_reset\',\'post\'],\n   #         \'/restart\':[\'restart\',\'post\']\n   #         }\n    #dic = {\'default\':\'initializer\',\'pre_process\':\'hallo_monitor\',\'ask_monitor_for_help\':\'ask_monitor_for_help\'}\n    socket_manager_obj = None\n    is_shutdown = False\n    is_running = False\n    is_closed = False\n    container_panding_task = {}  ### when cotainer is created but could not initilize. So, its task is stored. ###\n    service_debugging_info = {}\n    host = MONITER_BRIDGE_NAME\n    client_interface = MONITER_BRIDGE_NAME\n    nginx_conf=nginxconfig_obj\n\n    def __init__(self,import_manager_obj):\n        \n        #self.loop_status= {0:{\'status\':\'start\'}}\n\n        for x in type(self).__bases__:\n                x.__init__(self)\n        super(NamespaceFileMapper,self).__init__()\n        self.host = MONITER_BRIDGE_NAME\n        self.thread_manager_obj = self\n        self.register_thread()\n        l = self.loop_list[self.loop_index]\n\n        self.monitor_client_service =  MonitorAnalysisModels(self,self.loop_index)\n        self.import_manager_obj = import_manager_obj\n        #loop.create_task(self.client_initializer())\n        self.nginxconfig_obj = nginxconfig_obj\n        self.initialize_monitor_service(0)\n        self.initilize_monitor_client_communication_service(0)\n        self.corouting_manager_obj.register_service(type(self).__name__,\'assing_task_to_container\',self.assing_task_to_container)\n        l.create_task(self.initialize_monitor())\n       \n\n    async def initialize_monitor(self):\n        task_list = MONITOR_INITIAL_TASKS\n        for x in self.services[\'monitor_microservice\']:\n            while not x.run_status.done():\n                await asyncio.sleep(1)\n\n            \n        for x in task_list:\n            ### get container_type\n            #container_ip = ContainerObject.objects().filter(container_type=x).allow_filtering()\n            for y in task_list[x]:\n                #### get container task #######\n                #self.start_assing_task_to_container(0,container_ip,y)\n                y[\'container_type\'] = x\n                print(\'task \',y)\n                await self.assing_container(y)\n                await asyncio.sleep(1) \n\n  \n\n    def initilize_monitor_client_communication_service(self,loop_index):\n        \'\'\' Start monitor client communication service\'\'\'\n\n        m = MicroService(self.next_port(),self.loop_index,self)\n        try:\n            self.services[\'monitor_microservice\']\n        except:\n            self.services[\'monitor_microservice\']=[]\n        self.services[\'monitor_microservice\'].append(m)\n        r = self.monitor_client_service.create_router()\n        #print(r)\n        print(\'+++++++++++++++++++++++++ Monitor Client services is starting ++++++++++++++++++++++++++++++++\')\n        loop = self.loop_list[self.loop_index]\n        m.register_url(r.urls)\n        m.start_microservice_server(r,loop)\n        runner = m.runner \n        loop.run_until_complete(runner.setup())   \n        site = web.TCPSite(runner, \'0.0.0.0\', self.current_port) #self.client_interface\n\n        print("kjbviadifbdiugfia---------------------------------------------")\n        m.run_status =loop.create_task(site.start())\n        #loop.run_until_complete(site.start())\n        #loop.stop()\n        \n    def initialize_monitor_service(self,thread_id):\n        \'\'\' monitor services which will be run when monitor starrt \'\'\'\n        m = MicroService(self.next_port(),0,self)\n        self.services[\'monitor_microservice\']=[]\n        self.services[\'monitor_microservice\'].append(m)\n        r = self.create_router()\n        #print(r)\n        print(\'+++++++++++++++++++++++++ Monitor starting ++++++++++++++++++++++++++++++++\')\n        loop = self.loop_list[thread_id]\n        m.register_url(r.urls)\n        m.start_microservice_server(r,loop)\n        runner =m.runner\n        loop.run_until_complete(runner.setup())\n        site = web.TCPSite(runner,"0.0.0.0", self.current_port) ##self.host\n        m.run_status = loop.create_task(site.start())\n        #print(\'hasadasd\')\n        #loop.run_until_complete(site.start())\n       \n    #### shutdown the thread in container ######\n        #loop.run_until_complete(runner.cleanup())\n        #loop.stop()\n        #self.is_close = True\n\n    def monitor_run(self):\n        self.assign_loop()\n\n    async def monitor_shutdown(self):\n        for x in self.services[\'monitor_microservice\']:\n            l = self.loop_list[0]\n            await x.stop_microservice(l)\n        self.REDUCE_FLAG[0] = True\n        while self.corouting_manager_obj.num_of_corouting_in_loop[0] > 0:\n            await asyncio.sleep(0.5)\n\n        SYSTEM_STATUS[\'is_shutdown\'] = True\n        \n        l.stop()\n \n', 'created_at': '2020-11-09 12:41:54.161868', 'time_stamp': 1604905914.161933, 'import_parameters': '{"re": ["sys"], "os": ["sys"], "asyncio": ["sys"], "datetime": ["sys"], "uuid": ["sys"], "time": ["sys"], "threading": ["sys"], "common_models.common_api_lib": ["GenClass,get_ip", "custom"], "root_models.nginx_config_generator": ["nginxconfig_obj", "custom"], "network_models.socket_manager": ["SocketManagerClass", "custom"], "thread_models.one_only_thread": ["thread_manager", "custom"], "thread_models.support_only_one_thread": ["MicroService", "custom"], "root_models.zip_creater": ["extract_zip_file", "custom"], "root_models.url_mapper": ["UrlMapper", "custom"], "root_models.autodata_insert": ["NamespaceFileMapper", "custom"], "meta_model.container_db": ["MicroserviceResource,ContainerObject,ContainerObjectNetworking", "custom"], "aiohttp": ["web", "sys"], "ipaddress": ["IPv4Address", "sys"], "config": ["MONITOR_NETWORK_INTERFACE,MONITOR_TEMP_ZIPE_STORE,MONITOR_TEMP_FOLDER,MONITER_BRIDGE_NAME,SYSTEM_STATUS", "custom"], "root_models.root_manage_services": ["RootManagementServices", "custom"], "root_models.root_to_supperviser_communication": ["RootToSupervisorCommunication", "custom"], "root_models.monitor_client_microservices": ["MonitorAnalysisModels", "custom"], "root_models.monitor_suppervisor_microservices": ["RootServiceForMannageSupervisor", "custom"], "root_models.monitor_init_tasks": ["init_task", "custom"], "ujson": ["sys"]}'}
{'namespace': 'root_models.final', 'created_by': 'sys', 'source_code': '\nrequest = {\n    "url": "new_list",\n    "ip": "192.168.0.1",\n    "port": 92,\n    "weight": 5,\n    "server": "s1",\n    "method": "put"\n}\n\nclass nginx_conf:\n    \n    local_var = {\n        "stream": {\n            "url1": {\n                "ip1": {\n                    "url1_ip1_port1": {"weight": 5}, \n                    "url1_ip1_port2": {"weight": 0}\n                }, \n                "ip2": {\n                    "url1_ip2_port1": {"weight": 5}, \n                    "url1_ip2_port2": {"weight": 0}\n                }\n            },\n            "url2": {\n                "ip1": {\n                    "url2_ip1_port1": {"weight": 5}, \n                    "url2_ip1_port2": {"weight": 0}\n                }, \n                "ip2": {\n                    "url2_ip2_port1": {"weight": 5}, \n                    "url2_ip2_port2": {"weight": 0}\n                }\n            }\n        }\n    }\n    \n    local_var2 = {\n        "url1": ("server1", "post"),\n        "url2": ("server2", "put"),\n    }    \n    \n    def is_duplicate_pair(self,request):\n        """\n        This function will return True for existence of same ip/port pair in local var\n        """\n        find_pair = False    \n        for x in self.local_var["stream"]:        ### for each upstream resource \n            for y in self.local_var["stream"][x]: ### for each ip address\n                for z in self.local_var["stream"][x][y]:\n                    if request[\'ip\'] == y and request[\'port\'] == z:\n                        find_pair = True\n                        #return find_pair,x\n                        return True, x, y, z\n        return False\n                   \n\n    def create(self, request):\n        if self.is_duplicate_pair(request) == False:\n            v = list(request.values())\n            new = {v[0]:{v[1]:{v[2]:{"weight":v[3]}}}}\n\n            if request["url"] in self.local_var["stream"].keys():\n                new = {v[1]:{v[2]:{"weight":v[3]}}}\n                self.local_var["stream"][request["url"]][v[1]] = {v[2]:{"weight":v[3]}}\n            else:\n                self.local_var["stream"].update(new)\n                new_server_method = {v[0]:(v[4],v[5])}\n                self.local_var2.update(new_server_method)\n        else:\n            return False\n        file_write()\n    \n    def retrieve_by_url(self, request):\n        list_info = self.local_var["stream"].get(request[\'url\'])\n        return list_info\n\n    def retrieve_by_ip(self, request):\n        pass\n    \n    def retrieve_by_ip(self, request):\n        pass\n\n    def update(self, request):\n        new_weight = request["weight"]    \n        if self.is_duplicate_pair(request) == True:\n            self.local_var["stream"][request["url"]][request["ip"]][request["port"]]["weight"] = weight\n        file_write()\n\n    def delete_by_url(self, request):\n        del self.local_var["stream"][request["url"]]\n        del self.local_var2[request["url"]]\n        file_write()\n    \n    def delete_by_url_ip(self, request):\n        del self.local_var["stream"][request["url"]][request["ip"]]\n        file_write()\n\n    def delete_by_ip_port(self, request):\n        b, url, ip, port = self.is_duplicate_pair(request)\n        del self.local_var["stream"][url][ip]\n        file_write()\n\n    def delete_by_ip(self, request):\n        for x in self.local_var["stream"].keys():\n            if request["ip"] in self.local_var["stream"][x].keys():\n                del self.local_var["stream"][x][request["ip"]]\n        file_write()\n    \n    def file_write(self):\n        s = "stream {\\n\\t"\n        for url in self.local_var["stream"]:\n            s = s+"upstream "+url+" {\\n\\t\\t"\n            for ip in self.local_var["stream"][url]:\n                for port in self.local_var["stream"][url][ip]:\n                    s = s+"server "+ip+":"+port+" weight = "+str(self.local_var["stream"][url][ip][port]["weight"])+";\\n\\t\\t"\n            s = s+"\\n\\t}"\n            s = s+"\\n\\tserver {\\n\\t\\tlisten 80;\\n\\t\\tproxy_pass "+url+";\\n\\t\\tserver_name "+self.local_var2[url][0]+";\\n\\t\\tmethod "+self.local_var2[url][1]+"\\n\\t}\\n\\t"\n        s = s+"\\n}"\n\n        f = open("nginx_conf.txt", "a")\n        f.truncate(0)\n        f.write(s)\n        f.close()\n        os.system("service nginx restart")', 'created_at': '2020-11-09 12:41:54.164929', 'time_stamp': 1604905914.1649446, 'import_parameters': '{"os": ["sys"]}'}
{'namespace': 'root_models.class_urls_mapper', 'created_by': 'sys', 'source_code': '\ndef search_for_model_url(model_path):\n\tfor filename in os.listdir(model_path):\n\t\ttemp_file = model_path+\'/\'+filename\n\t\tp = pathlib.Path(p)\n\t\tif p.is_dir():\n\t\t\tyield from search_for_model_url(temp_file)\n\t\telse:\n\t\t\tif filename == \'urls.py\':\n\t\t\t\tyield temp_file\n\n\ndef url_mapper(model_folder,root_path = \'\'): ### model folder path  \n\t#importlib.import_module(root_path+\'.\'+model_folder+\'.urls\') \n\t#importlib.import_module(root_path+\'.\'+model_folder+\'.db_model\')\n\t#related_namespace_list = []\n\tcall_namespace_list = []\n\tfor filename in search_for_model_url(model_folder):\n\t\tfile_header_info = filename.split(\'.py\')[0] #### url.py file \n\t\tfile_path = file_header_info.split(\'/\') ### [root,apps,model,urls]\n\t\tmodule = file_header_info.replace(\'/\',\'.\')  ### root.apps.model.urls\n\t\tmodule_data = importlib.import_module(module) \n\t\tfor model in  module_data.urls:####{\'model01\':{\'class\':\'\',\'container_type\':\'mm_con\'}}\n\t\t\ttemp_file_path = file_path[:-1] + model #### [root,apps,model,model01]\n\t\t\tob = importlib.import_module(\'.\'join(temp_file_path)) ### model\n\t\t\tob_class = ob.get_attr(module_data.urls[model][\'class\'])  ### class of the model\n\t\t\tob_class_fucntion_keys = list(ob_class.function_dict.keys()) #### \n\t\t\tclass_funct_str = ujson.loads(ob_class_fucntion_keys)\n            temp_namespace = \'.\'join(temp_file_path)\n\t\t\tdb_obj = dict(MicroserviceResource.objects().fileter(namespace = temp_namespace).allow_filtering().first())\n            call_namespace_list.append(temp_namespace)\n\t\t\tdb_obj[\'sub_url\']  = class_funct_str\n\t\t\tdb_obj[\'microservice_class_name\'] = module_data.urls[\'class\']\n\t\t\tdb_obj[\'container_type\']= module_data.urls[\'container_type\']\n\t\t\tdb_obj[\'is_core\'] = False\n\t\t\tMicroserviceResource(**db_obj).save()\n\n\t\tfor db_model in module_data.db_urls: ##[\'db_models.py\']\n\t\t\ttemp_file_path = file_path[:-1] + db_model #### [root,apps,model,db_models]\n\t\t\tob = importlib.import_module(\'.\'.join(temp_file_path)) ### root.apps.model.db_models\n\t\t\tob_tables = ob.databases  ### class of the model \n\t\t\tfor table in ob_tables: ### table install to each and everey data table s\n\t\t\t\trelated_namespace_list = []\n\t\t\t\tdg.db_generator(table)\n\t\t\t\t#temp_db_models = table[\'model\']\n\t\t\t\tfor x in table[\'data_base_type\']: #### ["Cassandra","Redis"]\n\t\t\t\t    related_namespace_list.append(table[\'model_name\']+\'.\'+x+\'_viewset\') ##### Model.Cassandra_viewset\n\n\t\t\t\trelated_namespace_list.append(table[\'model_name\']+\'.DBClient\')  ##### Model.DBClient\n\t\t\t\tfor x in related_namespace_list:\n\t\t\t\t\tfor y in filter_core(namespace):\n\t\t\t\t\t\tcall_namespace_list.append(y)\n\n\t\ttry:\n\t\t\tmodule_data.kafaka_models\n\t\t\t\n\n\t\t\t#class KafkaMsgMeta(Model):\n    \n\t\t\tfor kafaka_model in module_data.kafaka_models: ### kafka_models = {\'kafka_model\':{\'classes\':[\'KafkaClass\'], \'cluster\':{\'cluster_name\':[\'brokers_array\']}}}\n\t\t\t\ttemp_data = {}\n\n\t\t\t\ttemp_data[\'fun_name_list\'] = ujson.dumps(module_data.kafaka_models[kafaka_model][\'classes\'])\n\t\t\t\ttemp_data[\'namespace\'] = kafaka_model\n\t\t\t\ttry:\n\t\t\t\t\ttemp_data[\'kafka_broker_cluster\'] =module_data.kafaka_models[kafaka_model][\'cluster\'][\'cluster_name\']\n\t\t\t\t\ttemp_data[\'kafka_broker_list\'] = module_data.kafaka_models[kafaka_model][\'cluster\'][\'brokers\']\n\t\t\t\texcept:\n\t\t\t\t\ttemp_data[\'kafka_broker_cluster\'] = \'Default\'\n\t\t\t\tKafkaMsgMeta(**temp_data).save()\n\n\n\t\texcept:\n\t\t\tpass\n\t\n\treturn call_namespace_list\n', 'created_at': '2020-11-09 12:41:54.167354', 'time_stamp': 1604905914.167371, 'import_parameters': '{"os": ["sys"], "pathlib": ["sys"], "importlib": ["custom"], "ujson": ["sys"], "base_models.meta_model.container_db": ["MicroserviceResource", "custom"], "service_models.data_base.generator.db_models_generator": ["DBGenerator", "custom"], "root_models.namespace": ["filter_core", "custom"]}'}
{'namespace': 'root_models.root_to_supperviser_communication', 'created_by': 'sys', 'source_code': 'MAXIMUM_COROUTING = 300000\n\n############################ corouting work ###################################################################\n\nclass RootToSupervisorCommunication:\n\n    async def start_assing_task_to_container(self,loop_index,ip_address,task):\n        #get_task_from_monitor. it should return port id of the service #####\n        print(self.loop_index,NETWORK_INTERFACE[\'OUT_NET\'],\'socket\',"json","http://"+ip_address+":10/get_task_from_monitor",task)\n        return await self.socket_object.send_single_request(self.loop_index,NETWORK_INTERFACE[\'OUT_NET\'],\'socket\',"json","http://"+ip_address+":10/get_task_from_monitor",task) \n\n    async def get_current_microservice_information(self,loop_index,ip_address):\n        \n        return await self.socket_object.send_single_request(self.loop_index,NETWORK_INTERFACE[\'OUT_NET\'],\'socket\',"json","http://"+ip_address+"/current_microservice_info",\'\') \n\n\n    async def start_reset_container(self,loop_index,ip_address):\n        #reset\n        return await self.socket_object.send_single_request(self.loop_index,NETWORK_INTERFACE[\'OUT_NET\'],\'socket\',"json","http://"+ip_address+"/reset",\'\')\n\n    async def start_restart_all_microservices_of_the_container(self,loop_index,ip_address):\n        #restart_all_microservices\n        return await self.socket_object.send_single_request(self.loop_index,NETWORK_INTERFACE[\'OUT_NET\'],\'socket\',"json","http://"+ip_address+"/restart_all_microservices",\'\')\n\n    async def start_shutdown_container_suppervisor(self,loop_index,ip_address):\n        #shutdown\n        return await self.socket_object.send_single_request(self.loop_index,NETWORK_INTERFACE[\'OUT_NET\'],\'socket\',"json","http://"+ip_address+"/shutdown",\'\')\n\n    async def start_shutdown_conatiner(self,loop_index,ip_address):\n        #shutdown_container\n        return await self.socket_object.send_single_request(self.loop_index,NETWORK_INTERFACE[\'OUT_NET\'],\'socket\',"json","http://"+ip_address+"/shutdown_container",\'\')\n\n    async def start_restart_microservice_of_container(self,loop_index,ip_address,namespace):\n        #restart_microservice\n        data = {\'namespace\':namespace}\n        return await self.socket_object.send_single_request(self.loop_index,NETWORK_INTERFACE[\'OUT_NET\'],\'socket\',"json","http://"+ip_address+"/restart_microservice",ujson.dumps(data))\n\n    async def start_delete_microservice_from_container(self,loop_index,ip_address,namespace):\n        data={\'namespace\':namespace}\n        return await self.socket_object.send_single_request(self.loop_index,NETWORK_INTERFACE[\'OUT_NET\'],\'socket\',"json","http://"+ip_address+"/delete_microservice",ujson.dumps(data))\n\n    async def start_delete_microservice_function_from_container(self,loop_index,ip_address,namespace,func_url):\n            #delete_microservice_function\n        data = {\'namespace\':namespace,\'url\':func_url}\n        urls = nginx_conf.search_url_for_ip_address(ip_address) ### [url1,url2,....]\n        if urls[0] == \'/\'+namespace +\'/\'+func_url:\n                return await self.start_shutdown_conatiner(self.loop_index,ip_address)\n        else:\n            return await self.socket_object.send_single_request(loop_index,NETWORK_INTERFACE[\'OUT_NET\'],\'socket\',"json","http://"+ip_address+"/delete_microservice_function",ujson.dumps(data))\n\n    async def start_debugging_to_container_microservices(self,loop_index,ip_address):\n            #start_debugging_all_microservices\n        return await self.socket_object.send_single_request(self.loop_index,NETWORK_INTERFACE[\'OUT_NET\'],\'socket\',"json","http://"+ip_address+"/start_debugging_all_microservices",\'\')\n\n    async def stop_debugging_to_container_microservices(self,loop_index,ip_address):\n        return await self.socket_object.send_single_request(self.loop_index,NETWORK_INTERFACE[\'OUT_NET\'],\'socket\',"json","http://"+ip_address+"/stop_debugging_all_micrservices",\'\')\n\n    async def start_debugging_to_container_microservice(self,loop_index,ip_address,namespace):\n            #start_function_debugging\n        data = {\'namespace\':namespace}\n        return await self.socket_object.send_single_request(self.loop_index,NETWORK_INTERFACE[\'OUT_NET\'],\'socket\',"json","http://"+ip_address+"/start_debugging_to_the_microservice",ujson.dumps(data))\n\n    async def stop_debugging_to_container_microservice(self,loop_index,ip_address,namespace):\n        data = {\'namespace\':namespace}\n        return await self.socket_object.send_single_request(self.loop_index,NETWORK_INTERFACE[\'OUT_NET\'],\'socket\',"json","http://"+ip_address+"/stop_debugging_to_the_micrservice",ujson.dumps(data))\n\n    async def get_container_ip_by_type_and_namespace(self,container_type,namespace):\n\n        container_info = ContainerObject.objects().filter(container_type = container_type).allow_filtering()\n\n        so = self.socket_object.get_session_obj(self.loop_index,NETWORK_INTERFACE[\'OUT_NET\'])#self.db_interface_list[\'REDIS\'])\n        temp_container_ip = None\n        temp_container_name =None\n\n\n        for x in container_info:\n            r = self.socket_object.register_request(\'socket\',"json","http://"+x.ip_address+"/report_all_service",\'\')\n            d = await self.socket_obj.run_request(so,r)\n            d = ujson.loads(d)\n            if d[\'namespace\'] == namespace:\n                yield x.ip_address\n        self.socket_object.release_session_obj(self.loop_index,NETWORK_INTERFACE[\'OUT_NET\'],so)\n\n    async def short_conatiner_by_no_of_running_corouting(self,loop_index,container_type):\n            #report_total_no_of_corouting\n        container_info = ContainerObject.objects().filter(container_type = container_type).allow_filtering()\n        container_net_info = ContainerObjectNetworking.objects().filter(container_type = container_type,inter_face_code =\'MONITOR_NET\').allow_filtering()\n\n        so = self.socket_object.get_session_obj(self.loop_index,NETWORK_INTERFACE[\'OUT_NET\'])#self.db_interface_list[\'REDIS\'])\n      \n        temp_container_ip = None\n        temp_no_corouting = MAXIMUM_COROUTING\n        temp_container_name =None\n\n\n        for x,y in zip(container_info,container_net_info):\n            r = self.socket_object.register_request(\'socket\',"json","http://"+y.ip_address+"/report_total_no_of_corouting",\'\')\n            d = await self.socket_object.run_request(so,r)\n            d = ujson.loads(d)\n            if d[\'no_of_co_routing\'] < temp_no_corouting:\n                temp_no_corouting = d[\'no_of_co_routing\']\n                temp_container_ip = y.ip_address\n                temp_container_name = x.container_name\n        self.socket_object.release_session_obj(self.loop_index,NETWORK_INTERFACE[\'OUT_NET\'],so)\n\n        return temp_container_ip, temp_container_name,temp_no_corouting\n', 'created_at': '2020-11-09 12:41:54.169601', 'time_stamp': 1604905914.1696174, 'import_parameters': '{"meta_model.container_db": ["ContainerObject,ContainerObjectNetworking", "custom"], "config": ["NETWORK_INTERFACE", "custom"]}'}
{'namespace': 'thread_models.support_only_one_thread', 'created_by': 'sys', 'source_code': '#import aiohttp\n#async def preprocess_json_request(request):\n#        urls = router_obj.dict_url\n#        urls_patarn = request.match_info.get_info()[\'formatter\']\n#        args = dict(request.match_info)\n#        request.data= await request.json()\n#        rs = urls[urls_patarn]["callback"](request,**args)\n#        if len(rs) == 2:\n#            return await preprocess_return_json_obj(rs[0],rs[1])\n#        else:\n#            return await preprocess_return_json_obj(rs)\n\nclass MicroService:\n    is_close = False\n    port = None\n    is_stop = False\n    lock = None\n    #no_request_processing = 0\n    no_request_processing_per_url = {}\n    current_url = None\n    import_manager_obj= None\n    current_time = 0\n    request_per_sec = 0\n    replay_time_per_sec ={} \n    is_debuging = False\n    test_re = None\n    STOPING = False\n    status = \'NotDone\'\n    class_name = None\n    name_space = None\n    urls = None\n    host = None\n\n    \n\n    def __init__(self,port,loop_index,thread_manager_obj):\n        self.port = port\n        self.loop_list = thread_manager_obj.loop_list\n        loop = thread_manager_obj.loop_list[loop_index]\n        self.lock = asyncio.Lock(loop = loop)\n        self.thread_manager_obj = thread_manager_obj\n        self.loop_index = loop_index\n\n\n    def close_microservice(self):\n        self.STOPING = True\n\n\n    def restart_microservice(self,urls=[\'*\']):\n        self.close_microservice()\n        while not self.is_stop:\n            continue\n        self.is_stop = False\n        self.microservice(self.class_name,self.name_space,self.host,urls)\n\n    #async def aiorestart_microservice(self,urls=[\'*\']):\n\n\n\n    \n    def start_debuging(self):\n        self.is_debuging= True\n\n    async def microservice(self,class_name,name_space,host,urls=[\'*\']): #microservice(self,loop,parent_url,name_space=None,urls=[\'*\']):\n        self.class_name = class_name\n        self.name_space = name_space\n        self.urls = urls\n        self.host = host\n        loop = self.loop_list[self.loop_index]\n\t\n\t#code,parent_url,model = self.preprocess_source_code(url)\n\t#self.current_url = url\n        #print(\'haii\')\n        try:\n            #print(class_name,name_space,host,urls)\n            import_manager_obj.add_namespace(name_space)\n            #print(\'haii\')\n            name_space_obj = import_manager_obj.walk(name_space)\n            obj = getattr(name_space_obj,class_name)(self.thread_manager_obj,self.loop_index)\n            self.class_obj = obj\n            parent_url = import_manager_obj.addition_info[name_space][\'source_url\']\n \n            url = [[parent_url, obj.create_router()]]\n           # print(url)\n            router_obj = router()\n\n            router_obj.urls = url\n            #print(router_obj.urls)\n            router_obj.get_urls()\n            #print(\'1221243123\')\n           # print(router_obj.urls)\n            \n            router_obj = self.pre_process_router(router_obj,urls)\n            #print(router_obj.urls)\n\n            self.register_url(router_obj.urls)\n           \n            #self.run_microserver(loop,router_obj,host)\n            await self.start_microservice(loop,router_obj,\'0.0.0.0\')\n            #self.start_microservice_server(router_obj,loop,obj.network_interface)\n            return True,\'request successfully complited\'\n        except Exception as e:\n            print(e)\n            return False, str(e)#\'compile time error\'\n\n    def register_url(self,urls):\n\n        for x in urls:\n            self.no_request_processing_per_url[x[0]] =0\n\n\n    def start_microservice_server(self,router_obj,loop,init_work = None,end_work = None):###network_interface ="IP_address"\n        router_obj.create_dict()\n        urls = router_obj.dict_url\n        print(urls)\n        app = web.Application()\n        rout_method = []\n        lock = self.lock\n        self.thread_manager_obj.current_microservice_iformation = {\'urls\': list(urls.keys()),\'port\':self.port}\n        #print(\'Haloo\')\n\n        \n        async def lock_accuring():\n            #print(\'haii\')\n            #print(str(self.no_request_processing))\n\n            \n            while sum(self.no_request_processing_per_url.values())>0:\n                #print(str(self.no_request_processing))\n                continue\n            if self.class_obj.backend_fun is not None:\n                self.class_obj.backend_fun()\n\n\n            await asyncio.sleep(1)\n                #if lock.locked():\n                #return True\n\n        async def exception_handler(app):\n            await lock_accuring()\n            if end_work is not None:\n                await end_work()\n            #print("hallo world")\n    \n        async def preprocess_json_request(request):\n            current_time = 0\n            \n            #print(str(self.no_request_processing))\n\n            if self.STOPING:\n                loop.stop()\n\n\n            if self.is_debuging:\n                current_time = time.time()\n                #self.request_per_sec+=1\n                #global lock\n            \n            #lock.release()\n            urls = router_obj.dict_url\n            self.test_re = request\n            #print(urls)\n            try:\n                urls_patarn = request.match_info.get_info()[\'path\']\n            except:\n                urls_patarn = request.match_info.get_info()[\'formatter\']\n            async with lock:\n                self.no_request_processing_per_url[urls_patarn]+=1\n\n            #print(urls)\n            \n            #print(urls_patarn )\n            \n            #return web.json_response(data = \'sadsad\')\n\n            #raise MicroServiceExit(\'go to haii\')\n            #return self.test_re\n\n            if urls[urls_patarn]["method"]!="socket":\n                if urls[urls_patarn]["method"] in ["post","put","delete","patch","trace"]:\n                    data= await request.read()\n                    if urls[urls_patarn][\'addition_parameters\'][1] >0:\n                        pass\n                    #data= CommunicationPipeClient.verify(data)\n                    if urls[urls_patarn][\'addition_parameters\'][0]==\'json\':\n                        request.data = ujson.loads(data)\n                    rs = await urls[urls_patarn]["callback"](request)\n                else:\n                    args = dict(request.match_info)\n                    #print(args)\n                    data= await request.read()#request.query_string\n                    #data= CommunicationPipeClient.verify(data)\n                    if urls[urls_patarn][\'addition_parameters\'][1] >0:\n                        pass\n                    if urls[urls_patarn][\'addition_parameters\'][0]==\'json\':\n                        request.data = ujson.loads(data)\n                    request.data = data\n                    rs = await urls[urls_patarn]["callback"](request,**args)\n                if  type(rs) is tuple:#len(rs) == 2:\n                    #keys = \'111\'\n                    #data  =  CommunicationPipeClient.send_data(data,keys)\n                    #await lock.acquire()\n                    if self.is_debuging:\n                        try:\n                            self.replay_time_per_sec[urls_patarn].append(time.time()-current_time)\n                        except:\n                            self.replay_time_per_sec[urls_patarn]=[]\n                            self.replay_time_per_sec[urls_patarn].append(time.time()-current_time)\n\n\n\n                    #return web.json_response(data = data,status = status, dumps= ujson.dumps)\n                    async with lock:\n                        self.no_request_processing_per_url[urls_patarn]-=1\n                    if urls[urls_patarn][\'addition_parameters\'][0]==\'json\':\n                        return web.json_response(data = rs[0],status = rs[1], dumps= ujson.dumps)\n\n                    return web.Response(body = rs[0],status = rs[1])\n\n\n                    #return await self.preprocess_return_obj(rs[0],rs[1])\n                else:\n                    #keys = \'111\'\n                    #data  =  CommunicationPipeClient.send_data(data,keys)\n                    #await lock.acquire()\n                    if self.is_debuging:\n                        try:\n                            self.replay_time_per_sec[urls_patarn].append(time.time()-current_time)\n                        except:\n                            self.replay_time_per_sec[urls_patarn]=[]\n                            self.replay_time_per_sec[urls_patarn].append(time.time()-current_time)\n\n                    #return web.json_response(data = data,status = status, dumps= ujson.dumps)\n                    async with lock:\n                        self.no_request_processing_per_url[urls_patarn]-=1\n                        \n                    if urls[urls_patarn][\'addition_parameters\'][0]==\'json\':\n                        return web.json_response(data = rs[0],status = rs[1], dumps= ujson.dumps)\n\n                    return web.Response(body = rs)\n\n                    #return await self.preprocess_return_obj(rs)\n            else:\n\n\n                ws = web.WebSocketResponse()\n                print(\'in socket\')\n\n                await ws.prepare(request)\n                temp_msg = None \n                async for msg in ws:\n                    if msg.type == WSMsgType.ERROR:\n                        break\n                    if msg.type == WSMsgType.TEXT:\n                        if msg.data == \'in_close\':\n                            break\n                    if temp_msg is None:\n                        temp_msg = msg.data\n                    else:\n                        temp_msg +=msg.data\n\n                if temp_msg is not None:\n                    if type(temp_msg) is bytes:\n                        temp_msg = temp_msg.decode()\n                    #print(temp_msg)\n                    temp_msg = ujson.loads(temp_msg)\n                request.data = temp_msg\n\n                #print(self.test_re)\n\n                \n\n                rs =await urls[urls_patarn]["callback"](request)\n                #print(rs)\n                ml= len(rs)\n                if router_obj.buffer_size >= ml:\n                    await ws.send_str(rs)\n                    print(rs)\n                    await ws.send_str(\'out_close\')\n                else:\n                    for x in range(ml/router_obj.buffer_size):\n                        await ws.send_str(rs[x*router_obj.buffer_size:(x+1)*router_obj.buffer_size])\n                        ws.send_str("out_close")\n                        \n                await ws.close()\n                #await lock.acquire()\n                if self.is_debuging:\n                    try:\n                        self.replay_time_per_sec[urls_patarn].append(time.time()-current_time)\n                    except:\n                        self.replay_time_per_sec[urls_patarn]=[]\n                        self.replay_time_per_sec[urls_patarn].append(time.time()-current_time)\n\n                async with lock:\n                    self.no_request_processing_per_url[urls_patarn]-=1\n\n                return ws\n        #print(urls)\n        for x in urls:\n            method = None\n            if urls[x]["method"] == "get":\n                method = web.get\n            elif urls[x]["method"] == "post":\n                method = web.post\n            elif urls[x]["method"] == "put":\n                method = web.put\n            elif urls[x]["method"] == "socket":\n                method = web.get\n\n            rout_method.append(method(x,preprocess_json_request))\n        \n           #     return coroutine,app,handler\n\n        #    async def preprocess_return_obj(self,data=None,status=200):\n#        keys = \'111\'\n#        data  =  CommunicationPipeClient.send_data(data,keys)\n#        return web.json_response(data = data,status = status, dumps= ujson.dumps)\n        server = None\n        \n        #try:\n\n\n\n        app.add_routes(rout_method)\n        if init_work is not None:\n            app.on_startup.append(init_work)\n        app.on_shutdown.append(exception_handler)\n        runner = web.AppRunner(app)\n        self.runner = runner\n        #self.app = app\n\n        \n\n    async def start_microservice(self,loop,router_obj,host):\n        self.start_microservice_server(router_obj,loop)\n        runner = self.runner\n        #asyncio.set_event_loop(loop)\n        st = loop.create_task(runner.setup())\n        #loop.run_until_complete(runner.setup())\n        while not st.done():\n            await asyncio.sleep(0.5)\n        site = web.TCPSite(runner, host, self.port)\n        server_start_status =  loop.create_task(site.start())\n        while not server_start_status.done():\n            await asyncio.sleep(0.5)\n        \n            \n        \n    async def stop_microservice(self,loop):\n                \n        server_closing_status = loop.create_task(self.runner.cleanup())\n        while not server_closing_status:\n            await asyncio.sleep(0.5)\n        self.is_stop = True\n        self.status = \'Done\'\n\n        \n\n    def pre_process_router(self,router_obj,urls):\n        if urls == [\'*\']:\n            return router_obj\n\n        for x in router_obj.urls:\n            if x not in url:\n                del router_obj.urls[x]\n        return router_obj\n\n    ###will be moved to common_api\n    \n\n#def container_supervisor():\n#    import asyncio\n#    loop = asyncio.get_event_loop()\n#    global th_obj\n#    async def get_task(request):\n#        data = request.json()\n#        url = data["resource_url"]\n#        th_obj.create_thread(True,url)\n#    \n#    app = web.Application(loop=loop) \n#    app.add_routes([web.post(\'create_thread\',get_task)])\n#eb.run_app(app)\n\n', 'created_at': '2020-11-09 12:41:54.172717', 'time_stamp': 1604905914.1727374, 'import_parameters': '{"aiohttp": ["web,WSMsgType", "sys"], "common_models.common_api_lib": ["router", "custom"], "asyncio": ["sys"], "ujson": ["sys"], "time": ["sys"]}'}
{'namespace': 'thread_models.coroutine_manager_only_one_thread', 'created_by': 'sys', 'source_code': '#from kafka_manager import kafka_obj\n\n#### Corouting is a helper task to thread manager ######\n\nclass coroutine_manager:\n    ### This is the corouting manager for controlling access of many object ####\n    \n    MAXIMUM_LIMIT = 3\n    MINIMUM_LIMIT = 0\n    REDUCE_FLAG = []\n    DEBAGING = False \n    \n\n    waiting_list = []\n    #loop_list = []\n    \n    ALL_CORO_USED = True\n    REDUCING_FLAGE = False\n    STOP = False\n    num_of_corouting_in_loop = {} ### {1:9,2:0}\n\n    corouting_panding_work_list = [] ### {\'service\':\'service_name\',\'service_fun\':\'fun_name\'} \n    waiting_corouting_list = {} ### {1:3,2:0}\n    service_list = {} \n    task_list = {} ## {\'service\':{fun1:0,fun2:0}}\n    #loop_list\n    #corouting_work_list = {} ### corouting work list for each user \n    \n    def __init__(self,thread_obj):\n        self.thread_obj = thread_obj\n        self.loop_list = thread_obj.loop_list\n        \n    def multiple_register_service(self,service,service_fun_dic):\n        self.task_list[service] = {x:0 for x in service_fun_dic}\n        self.service_list[service] = service_fun_dic\n        \n\n    def register_service(self,service,service_fun,fun): \n        \'\'\' register_service(\'service\',\'service_fun\',fun_pointer)\'\'\'\n        self.service_list[service] = {service_fun:fun}\n        self.task_list[service] = {service_fun:0}\n\n\n    ## Add pending task list register the user request for farther process by the corouting ####\n\n    def add_pending_task_list(self,task_meta):\n        task_meta["status"] = "PANDDING" ### initialize trask ####\n        #self.corouting_work_list.append(task_meta) #### Add user task to the user task list ###\n        self.corouting_panding_work_list.append(task_meta) ####  #####\n        if self.ALL_CORO_USED:\n            self.add_coro_worker()\n\n    #### coro_worker function alway up and access the task que to process the user request ####\n\n    async def coro_worker(self,loop_index):\n       \n        while True:\n            await asyncio.sleep(0.5)\n            #print(\'Haii\')\n            if self.thread_obj.REDUCE_FLAG[loop_index]:\n                self.num_of_corouting_in_loop[loop_index] -=1\n                self.waiting_corouting_list[loop_index] -=1\n                #if self.num_of_corouting_in_loop[loop_index] ==0:\n                #    self.thread_obj.loop_list[loop_index].stop() \n                return\n\n\n            if self.num_of_corouting_in_loop[loop_index] < self.MAXIMUM_LIMIT :\n                if max(list(self.waiting_corouting_list.values())) == 0:\n                    self.ALL_CORO_USED = True\n\n                if self.corouting_panding_work_list == []:\n                    continue\n                with self.thread_obj.corouting_lock:\n                    async with self.thread_obj.loop_lock[loop_index]:\n                        if self.corouting_panding_work_list == []:\n                            continue\n                        self.waiting_corouting_list[loop_index] -=1\n                        loop = self.loop_list[loop_index]\n                        meta_coro_work = self.corouting_panding_work_list.pop()\n                        self.task_list[meta_coro_work[\'service\']][meta_coro_work[\'service_fun\']]+=1\n                        await self.service_list[meta_coro_work[\'service\']][meta_coro_work[\'service_fun\']](loop_index,meta_coro_work)\n                        self.task_list[meta_coro_work[\'service\']][meta_coro_work[\'service_fun\']]+=1\n                        self.waiting_corouting_list[loop_index] +=1\n\n            else:\n                self.num_of_corouting_in_loop[loop_index] -=1\n                self.waiting_corouting_list[loop_index] -=1\n                return\n\n\n        \n#### Findout most not used loop and assing corouting for it if posibale ####\n\n    def add_coro_worker(self):\n        loop_index = self.thread_manager_obj.loop_index\n\n        #if  temp> self.MAXIMUM_LIMIT-2:\n        #    self.ask_task_manager_for_help()\n        if temp > self.MAXIMUM_LIMIT:\n            return\n\n                    \n        if self.ALL_CORO_USED:\n            self.ALL_CORO_USED = False\n\n        self.num_of_corouting_in_loop[loop_index] +=1\n        self.waiting_corouting_list[loop_index] +=1\n        loop = self.loop_list[loop_index]\n        self.start_loop_coroutings(loop_index)\n        #self.thread_obj.thread_pending_task.append({\'service_type\':\'backup\'})\n        #loop.stop()\n                \n    def start_loop_coroutings(self,loop_index):\n        loop =self.loop_list[loop_index]\n        #print(\'haii\')\n        loop.create_task(self.coro_worker(loop_index))\n        #loop.run_forever()\n\n    def register_loop(self,loop_index):\n        #self.loop_list.append(loop)\n        self.num_of_corouting_in_loop[loop_index]=1\n        self.waiting_corouting_list[loop_index]=1\n        #self.start_loop_coroutings(loop_index)\n        #print(\'Haii  \'+str(self.num_of_corouting_in_loop[loop_index])+\'  \'+str(self.waiting_corouting_list[loop_index]))\n\n#    def ask_task_manager_for_help(self):\n#        self.thread_manager_obj.create_thread()\n#   \n\n#corouting_mager_obj = corouting_mager()\n', 'created_at': '2020-11-09 12:41:54.175908', 'time_stamp': 1604905914.175925, 'import_parameters': '{"asyncio": ["sys"]}'}
{'namespace': 'thread_models.one_only_thread', 'created_by': 'sys', 'source_code': 'class thread_manager:\n    """Assigns coroutines to threads running in a container based on the number of coroutines present under the available threads"""\n\n    thread_lock = threading.Lock() ### lock for handeling thread\n    corouting_lock = threading.Lock() ## lock for handeling corouting\n    loop_lock = {}\n    loop_status = []\n    \n    \n    thread_list = []\n    thread_inactive_list = []\n    thread_pending_task = [] ## [{service_type:[UI|backup],[url:]}]\n    thread_info = {} ## {\'thread_id\':[{service_type:[UI|backup],[url:],\'status\':\'\'}}\n    task_meta = {\'backup\':\'call_corouting\',\'UI\':\'call_micro_service\',\'kafka\':\'call_kafaka_backend_process\',\'data_base\':\'call_db_handeler\'} ## this will be store meta information for the task \n    socket_class = SocketManagerClass\n    socket_object = None\n    loop_list = []\n    #loop_status = []\n    services = {} ####{"microservice":[]}\n\n    restart_loop =[]\n    \n    MINIMUM_LIMIT = 1\n    MAXIMUM_LIMIT = 3\n    REDUCE_FLAG = []#False\n    \n    INIT_PORT =10 \n    current_port = 0\n    in_net = None\n    #microservice_list=[]\n        \n    def next_port(self):\n        """Generates port for assigning new microservice"""\n        \n        if self.current_port == 0:\n            self.current_port = self.INIT_PORT\n            \n        else:\n            self.current_port += 1\n        \n        return self.current_port\n\n   \n    def __init__(self, corouting_class = coroutine_manager):\n        """Initiates new thread"""\n        \n        self.corouting_manager_obj = corouting_class(self)\n        self.socket_object = self.socket_class(self)\n        self.corouting_manager_obj.register_loop(0)\n        self.socket_object.register_loop(0)\n\n    def register_thread(self):\n        """Creates new thread and appends to thread list"""\n        t = threading.current_thread()\n        self.thread_list.append(t)\n        self.thread_info[0] = {\'status\':\'created\'}\n\n        self.REDUCE_FLAG.append(False) \n        l = asyncio.get_event_loop()\n        loop_id = len(self.loop_list)\n        self.loop_index = 0\n        self.loop_list.append(l)\n        self.loop_status.append(\'stop\')\n        asyncio.set_event_loop(loop = l)\n        self.loop_lock[0] = asyncio.Lock(loop=l)\n        self.call_corouting(self.loop_index)\n\n\n\n    async def call_micro_service(self,loop_index,task):\n        m = None\n        if \'microservices\' in self.services:\n            for x in self.services[\'microservices\']:\n                if x.status == \'Done\':\n                    m = x\n                    break\n            if m is None:\n                m = MicroService(self.next_port(),loop_index,self)\n                self.services[\'microservices\'].append(m)\n            else:\n                m.loop_index = loop_index\n        else:\n            m = MicroService(self.next_port(),loop_index,self)\n            self.services[\'microservices\']=[m]\n        \n        task[\'host\'] =self.in_net\n\n        \n        try:\n            \n\n            await m.microservice(class_name = task[\'class_name\'],name_space = task[\'name_space\'],host = task[\'host\'],urls= task[\'urls\'])\n        except:\n            \n\n            await m.microservice(class_name = task[\'class_name\'],name_space = task[\'name_space\'],host = task[\'host\'])\n\n        return m.port\n\n            \n\t\t\t\t\n\n\n    def call_corouting(self,loop_index):\n        try:\n            self.corouting_manager_obj.num_of_corouting_in_loop[loop_index]\n        except:\n\n            self.corouting_manager_obj.register_loop(loop_index)\n        try:\n            self.services[\'backup\']\n        except:\n            self.services[\'backup\'] = self.corouting_manager_obj\n\n        self.corouting_manager_obj.start_loop_coroutings(loop_index)\n\n    def assign_loop(self):\n        \n        l = self.loop_list[0]\n        while True:\n           \n            if l.is_closed():\n                l = asyncio.new_event_loop()\n                self.loop_list[loop_id] = l\n                asyncio.set_event_loop(loop = l)\n                self.loop_lock[thread_id] = asyncio.Lock(loop=l)\n\n            if self.REDUCE_FLAG[0]:\n                return\n            else:\n                self.loop_status[0] = \'running\'\n                l.run_forever()\n               \n\n    def reduce_thread(self,thread_id):\n        self.REDUCE_FLAG[thread_id] = True\n\t\n\n', 'created_at': '2020-11-09 12:41:54.178431', 'time_stamp': 1604905914.178449, 'import_parameters': '{"threading": ["sys"], "asyncio": ["sys"], "network_models.socket_manager": ["SocketManagerClass", "custom"], "thread_models.coroutine_manager_only_one_thread": ["coroutine_manager", "custom"], "thread_models.support_only_one_thread": ["MicroService", "custom"]}'}
{'namespace': 'suport_thread.mytry', 'created_by': 'sys', 'source_code': 'class A:\n    pass\nclass B:\n    pass\n', 'created_at': '2020-11-09 12:41:54.180927', 'time_stamp': 1604905914.180948, 'import_parameters': '{}'}
{'namespace': 'suport_thread.import_model', 'created_by': 'sys', 'source_code': '\nroot_path = \'import_folder\'\n\nclass ImportManager:\n    \n    name_space_list = {} ### {namespace:{filename:str,is_imported:bool[True:False]}\n    root_folder = root_path\n    addition_info = {} ### {namespace:{}} This will containg addition infomation except source code + namespace from the data base\n    \n    def __init__(self):\n        self.name_space_list[\'config\'] = {\'filename\':\'../config\'}\n        p = pathlib.Path(self.root_folder)\n        self.config = importlib.import_module(\'base_model.config\')\n        if not p.exists():\n            os.makedirs(self.root_folder)\n\n        #self.config = config\n\n    class Comm:\n        pass\n\n\n    def add_namespace(self,name_space):\n        """Namespace face and add to local data structure"""\n        #print(name_space)\n        data=None\n        temp_str =""\n        try:\n            self.name_space_list[name_space]\n        except:\n           \n            #temp_str = ""\n            print(name_space)\n            data = MicroserviceResource.objects().filter(namespace = name_space).allow_filtering().first()\n            if data is None:\n                print(\'haiiiiiiiiiii\',name_space)\n                return\n#MicroserviceResource.objects.get(namespace = name_space)\n            #print(data)\n            import_parameter = ujson.loads(data.import_parameters)#[\'model\']\n                        #print(type(import_parameter))\n            #print(import_parameter)\n            if import_parameter !={}:\n                #print(import_parameter)\n\n                for x in import_parameter:\n\n                    if \'sys\' not in import_parameter[x]:\n                #        continue\n                        try:\n                            self.name_space_list[x]\n                        except:\n                #            pass\n                           \n                            self.add_namespace(x)\n                        finally:\n                ###        pass\n                            temp_str+=\'from base_model.suport_thread.import_model import import_manager_obj\\n\'\n                            if len(import_parameter[x])>1:\n                                for y in import_parameter[x][0].split(\',\'):\n                                    temp_str +=y+" = import_manager_obj."+x+"."+y+"\\n"\n                            else:\n                                temp_str+=\'import \'+x+\'\\n\'\n                    else:\n                        #try:\n                    ##        #self.name_space_list[x]\n                    ##    #except:\n                        if len(import_parameter[x])>1:\n                            temp_str +="from "+x+" import "+import_parameter[x][0]+"\\n"\n                        else:\n                            temp_str+=\'import \'+x+\'\\n\'\n\n                    ##        #pass\n                    #    \n                    #    #    return False\n\n                    #    #self.add_namespace(x,import_parameter[x],\'system\')\n                        #temp_str +="from "+x+" import ("\n                        \n            #print(temp_str)\n            #   pass\n            #for y in import_parameter[x]["model"]:\n            #    temp_str +=y+" = import_manager_obj."+x+"."+y+"\\n"\n            source_code  = temp_str + data.source_code\n            self.addition_info[name_space] ={}\n            self.addition_info[name_space][\'source_url\'] = data.source_url\n            self.addition_info[name_space][\'sub_url\'] = data.sub_url\n            self.addition_info[name_space][\'created_by\'] = data.created_by\n            self.addition_info[name_space][\'created_at\'] = data.created_at\n            self.addition_info[name_space][\'time_stamp\'] = data.time_stamp\n            self.addition_info[name_space][\'import_parameters\'] = data.import_parameters \n            #print(source_code)\n            #\n            snp = name_space.split(\'.\')\n            file_name = ""\n            ch = self\n            if len(snp)>1:\n\n                #print(snp)\n                sdir = self.root_folder+\'/\'+\'/\'.join(snp[:len(snp)-1])\n                #print(sdir)\n                if not pathlib.Path(sdir).exists():\n                    \n                    os.makedirs(sdir)\n                tf = self.root_folder+\'.\'+name_space\n                #print(tf)\n\n                file_name = sdir+\'/\'+snp[len(snp)-1]+\'.py\'\n                #print(file_name)    \n                for x in range(len(snp)-1):\n                #   \n                    try:\n                        getattr(ch,snp[x])\n                #        \n                    except:\n                        temp = self.Comm()\n                #        print(snp[x])\n                        setattr(ch,snp[x],temp)\n                    ch = getattr(ch,snp[x])\n                #file_name = self.root_folder+\'/\'+file_name\n\n                self.name_space_list[name_space] = file_name\n                f = open(file_name,\'w+\')\n                f.write(source_code)\n                f.close()\n                #print(tf)\n                ##\n                ##print(name_space)\n                #m = input(\'haii try\')\n                time.sleep(1/(10**2))\n                temp = importlib.import_module(tf)\n                #m = input(\'haii try\')\n                setattr(ch,snp[-1],temp)\n\n            else:\n                tf = self.root_folder+\'.\'+name_space\n                file_name = name_space+\'.py\'\n                file_name = self.root_folder+\'/\'+file_name\n                self.name_space_list[name_space] = file_name\n#                print(file_name)\n                f = open(file_name,\'w+\')\n                f.write(source_code)\n                f.close()\n                #print(tf)\n\n                #temp = importlib.import_module(tf)\n                #setattr(ch,name_space,temp)\n\n        \n        finally:\n            pass\n            \n\n    def walk(self,path): \n        \'\'\' This is the walk function it tarvel from top of the path to end. path is the namespace like "a.b.c.d" it will return d object.\'\'\'\n        array_str = path.split(\'.\')\n        temp_obj = getattr(self,array_str[0])\n        array_str = array_str[1:]\n        for x in array_str:\n            temp_obj = getattr(temp_obj,x)\n\n        return temp_obj\n\n    def update_namespace(self,namespace):\n        del self.name_space_list[namespace]\n        self.add_namespace(namespace)\n       \n       \n    def delete_namespace(self,namespace):\n        snp = namespace.split(\'.\')\n        \n        ch  = self\n        #print(getattr(ch,snp[0]))\n\n        for x in range(len(snp)-1):\n            ch = getattr(ch,snp[x])\n        m = getattr(ch,snp[len(snp)-1])\n        del m\n\n       \n    def list(self):\n        return self.name_space_list\n   \n   \n###will be moved to common_api\n\n    def reset(self):\n        m = list(self.name_space_list.keys())\n        m.remove(\'config\')\n        for x in m:\n            self.update_namespace(x)\n\n        \n    def preprocess_source_code(self,namespace):\n        """This function get the actual source code and compile the code"""\n        try:\n            self.name_space_list[namespace]\n        except:\n            self.add_namespace(namespace)\n                 \n\n', 'created_at': '2020-11-09 12:41:54.184740', 'time_stamp': 1604905914.184762, 'import_parameters': '{"sys": ["sys"], "importlib": ["custom"], "ujson": ["sys"], "uuid": ["sys"], "meta_model.container_db": ["MicroserviceResource", "custom"], "os": ["sys"], "pathlib": ["sys"], "time": ["sys"], "=": ["custom"]}'}
{'namespace': 'permission_model.permission_checker', 'created_by': 'sys', 'source_code': '\n\n#from base_model.PermissionPermissionCode.DBClient import PermissionPermissionCodeDBClient\n\nclass PermissionChecker:\n    permission_size = 256\n    owner_bit_position = 3\n\n    def __init__(self,thread_obj,loop_index):\n        self.user_group = PermissionUserGroupDBClient(thread_obj,loop_index)\n        self.group_bit_mapper = PermissionGroupBitsMapperDBClient(thread_obj,loop_index)\n        self.permission_for_model =PermissionPermissionForModelDBClient(thread_obj,loop_index)\n        self.group_permission_for_item = PermissionGroupPermissionForItemDBClient(thread_obj,loop_index)\n        self.item_owner = PermissionItemOwnerDBClient(thread_obj,loop_index)\n        self.permission_to_permission_code = PermissionPermissionCodeDBClient(thread_obj,loop_index)\n        self. item_default_permission_code = PermissionItemDefaultPermissionDBClient(thread_obj,loop_index)\n        permission_codes =  await self.permission_to_permission_code.list()\n        #dict_in_list0 = [{\'a\':12345,\'b\':987654},{\'c\':5,\'d\':98}]\n        self.permission_to_permission_code_mapping = {}\n        for x in permission_codes:\n            self.permission_to_permission_code_mapping[x[\'permission\']] = x[\'permission_code\']\n\n\n\n################## request.user, \'\', item_id,, \'\'\n\n\n\n\n    async def assign_permission(self,user,model,item,permission_string,is_permission_code = False,sp_permission=None):  #### Admin, Ds,P, U,Ug   #### ,/write+read+delete ############# for sp_permission = {\'user_id01\':\'permission /write+read+delete/7\'}\n\n        await self.item_owner.create({\'user\':user,\'item\':item,\'time_stamp\':time.time()})\n        permission_code=None\n        default_permission_code = await self. item_default_permission_code.retrieve({\'model\':model})[\'default_permission_code\']\n        if not is_permission_code:\n            tmp_p = permission_string.split(\',\') ### get permissions write+read+delete,write+read+delete\n            temp_code =[] ### temporal code\n             ###\n            for x in tmp_p: #### [\'write+read+delete\',\'write+read+delete\']\n                temp_code.append(sum([self.permission_to_permission_code_mapping[y] for y in x.split(\'+\')])) #### [\'write\',\'read\',\'delet\'] --->[6,2,1]\n\n            permission_code = \',\'.join(temp_code)\n        else:\n            permission_code = permission_string\n\n        total_permission_code  = default_permission_code +",{}".format(permission_code)\n        if sp_permission is None:\n            await self.group_permission_for_item.create({\n            \'permission_codes_string\':total_permission_code,\n            \'item_code\':item,\n            \'timestamp\':time.time(),\n            \'is_special_permission\':False\n            })\n\n        else:\n            if not is_permission_code:\n                sp_permission = sum([self.permission_to_permission_code_mapping[y] for y in sp_permission.split(\'+\')])\n            await self.group_permission_for_item.create({\n            \'permission_codes_string\':total_permission_code,\n            \'item_code\':item,\n            \'timestamp\':time.time(),\n            \'is_special_permission\':True,\n            \'special_permission\':sp_permission})\n\n\n\n\n\n    async def has_permission(self,user, model,permission):\n        permission_code = await self.permission_to_permission_code({\'permission\':permission})\n        permission_code, group_position = await self.common_op(user, model=model)  # permission value like 9,16,_, for the request model\n        permission_code_array = [int(x) for x in permission_code.split(\',\')]\n\n        return is_permission_match(permission_code_array[group_position],permission_code)  ## check for the permission\n\n\n\n    async def has_object_permission(self,user, obj,permission):\n        #permission_code, group_position = self.common_op(user,obj=obj)  #  permission value like 9,16,_,\n        permission_code = await self.permission_to_permission_code({\'permission\':permission})\n        return_data = await self.common_op(user,obj=obj)\n        permission_code_array = [int(x) for x in return_data[0].split(\',\')]\n\n        if len(return_data) ==2:\n            user_permission = permission_code_array[return_data[1]]\n\n        else:\n            sp_permission = return_data[1]\n            if user in sp_permission.keys():\n                return self.is_permission_match(permission_code,sp_permission[user])\n            else:\n                user_permission = permission_code_array[return_data[2]]\n\n        return self.is_permission_match(permission_code,user_permission)\n\n    def is_permission_match(self,permission_value,permission_code): ## permission_value will come from the request and permission_code stored in data base\n\n        result = permission_code & permission_value\n        if result == 0:\n            return False\n        else:\n            return True\n\n    async def common_op(self,user,model=None,obj=None):\n        user_group_code =await self.user_group.retrieve({\'user_id\':user})\n        group_position = await self.group_bit_mapper.retrieve({\'group_code\':user_group_code})[\'bit_position\']\n        if model is not None:\n            model_permission = await self.permission_for_model.retrieve(\'model_code\':model)[\'permission\']\n            return model_permission, group_position\n\n        elif obj is not None:\n            owner = await self.item_owner.retrieve({\'item_id\':obj})[\'user_id\']\n            if user == owner:\n                group_position = self.owner_bit_position\n            item_permission = await self.group_permission_for_item.retrieve({\'item_code\':obj})\n            if item_permission[\'is_special_permission\']:\n                return item_permission[\'permission_codes_string\'],item_permission[\'special_permission\'], group_position\n            return item_permission[\'permission_codes_string\'], group_position\n', 'created_at': '2020-11-09 12:41:54.188345', 'time_stamp': 1604905914.1883643, 'import_parameters': '{"PermissionUserGroup.DBClient": ["PermissionUserGroupDBClient", "custom"], "PermissionGroupBitsMapper.DBClient": ["PermissionGroupBitsMapperDBClient", "custom"], "PermissionPermissionForModel.DBClient": ["PermissionPermissionForModelDBClient", "custom"], "PermissionGroupPermissionForItem.DBClient": ["PermissionGroupPermissionForItemDBClient", "custom"], "PermissionPermissionCode.DBClient": ["PermissionPermissionCodeDBClient", "custom"], "PermissionItemOwner.DBClient": ["PermissionItemOwnerDBClient", "custom"], "PermissionItemDefaultPermission.DBClient": ["PermissionItemDefaultPermissionDBClient", "custom"], "time": ["sys"]}'}
{'namespace': 'permission_model.db_models', 'created_by': 'sys', 'source_code': "db_json_list = [\n        {\n        'model_name':'PermissionItemDefaultPermission',\n        'table_info':{\n            'default_permission_code':{'type':'string','required':True},\n            'model':{'type':'string','required':True},\n            'index':['model']\n            },\n        'data_base_type':['Cassandra','Prosgresql'],\n        'data_server':'default',\n        'backup_db':['Postgresql'],\n        'operation':['*']\n        },\n\n        {\n            'model_name':'PermissionPermissionBitMapper',\n            'table_info':{\n                'permission_code':{'type':'string','required':True},\n                'bits_position':{'type':'int','required':False},\n                'created_at':{'type':'datetime','required':False},\n                'index':['permission_code']\n                },\n            'redis_type':'not_list',\n            'data_base_type':['*'],\n            'data_server':'default',\n            'backup_db':['Postgresql'],\n            'operation':['*']\n            },\n        {\n            'model_name':'PermissionPermissionCode',\n            'table_info':{\n                'permission':{'type':'string','required':True},\n                'permission_code':{'type':'string','required':False},\n                'created_at':{'type':'datetime','required':False},\n                'index':['permission']\n                },\n            'redis_type':'not_list',\n            'data_base_type':['*'],\n            'data_server':'default',\n            'backup_db':['*'],\n            'operation':['*']\n            },\n        {\n            'model_name':'PermissionGroupPermissionForItem',\n            'table_info':{\n                'permission_codes_string':{'type':'string','required':True},\n                'item_code':{'type':'string','required':True},\n                'timestamp':{'type':'float','required':False},\n                'is_special_permission':{'type':'boolean','default':'False'},\n                'special_permission':{'type':'string','required':False}, #### {'user01':'write+read+delete=7'}\n                'index':['item_code']\n                },\n            'redis_type':'not_list',\n            'data_base_type':['*'],\n            'data_server':'default',\n            'backup_db':['*'],\n            'operation':['*']\n            },\n        {\n            'model_name':'PermissionPermissionForModel',\n            'table_info':{\n                'permission_codes_string':{'type':'string','required':True},\n                'model_code':{'type':'string','required':True},\n                'created_at':{'type':'datetime','required':False},\n                'resource_url':{'type':'float','required':True},\n                'index':['model_code']\n                },\n            'redis_type':'model_list',\n            'data_base_type':['*'],\n            'data_server':'default',\n            'backup_db':['*'],\n            'operation':['*']\n            },\n        {\n            'model_name':'PermissionGroupBitsMapper',\n            'table_info':{\n                'bits_position':{'type':'int','required':False},\n                'group_code':{'type':'string','required':True},\n                'created_at':{'type':'datetime','required':False},\n                'index':['group_code']\n                },\n            'redis_type':'model_list',\n            'data_base_type':['*'],\n            'data_server':'default',\n            'backup_db':['*'],\n            'operation':['*']\n            },\n        {\n            'model_name':'PermissionGroup',\n            'table_info':{\n                'group_code':{'type':'string','required':False},\n                'created_at':{'type':'datetime','required':False},\n                'group':{'type':'string','required':True},\n                'index':['group']\n                },\n            'redis_type':'model_list',\n            'data_base_type':['*'],\n            'data_server':'default',\n            'backup_db':['*'],\n            'operation':['*']\n            },\n        {\n            'model_name':'PermissionUserGroup',\n            'table_info':{\n                'userid':{'type':'string','required':True},\n                'group_code':{'type':'string','required':False},\n                'index':['userid']\n                },\n            'redis_type':'model_list',\n            'data_base_type':['*'],\n            'data_server':'default',\n            'backup_db':['*'],\n            'operation':['*']\n            },\n        {\n            'model_name':'PermissionItemOwner',\n                'table_info':{\n                'user':{'type':'string','required':True},\n                'item':{'type':'string','required':True},\n                'time_stamp':{'type':'float','required':True},\n                'index':['item_id']\n                },\n                'redis_type':'not_list',\n                'data_base_type':['*'],\n                'data_server':'default',\n                'backup_db':['Postgresql'],\n                'operation':['*']\n        },\n        {\n            'model_name':'PermissionItemOwner',\n                'table_info':{\n                'userid':{'type':'string','required':True},\n                'item_id':{'type':'string','required':False},\n                'index':['item_id']\n                },\n                'redis_type':'not_list',\n                'data_base_type':['*'],\n                'data_server':'default',\n                'backup_db':['Postgresql'],\n                'operation':['*']\n        }\n\n\n]\n", 'created_at': '2020-11-09 12:41:54.191355', 'time_stamp': 1604905914.1913726, 'import_parameters': '{}'}
{'namespace': 'authentication_model.database_models', 'created_by': 'sys', 'source_code': "db_json_list = [\n        {\n            'model_name':'PermissionPermissionBitMapper',\n            'table_info':{\n                'permission_code':{'type':'string','required':True},\n                'bits_position':{'type':'int','required':False},\n                'created_at':{'type':'datetime','required':False},\n                'index':['permission_code']\n                },\n            'redis_type':'not_list',\n            'data_base_type':['*'],\n            'data_server':'default',\n            'backup_db':['Postgresql'],\n            'operation':['*']\n            },\n        {\n            'model_name':'PermissionPermission',\n            'table_info':{\n                'permission':{'type':'string','required':True},\n                'permission_code':{'type':'string','required':False},\n                'created_at':{'type':'datetime','required':False},\n                'index':['permission']\n                },\n            'redis_type':'not_list',\n            'data_base_type':['*'],\n            'data_server':'default',\n            'backup_db':['*'],\n            'operation':['*']\n            },\n        {\n            'model_name':'PermissionGroupPermissionForItem',\n            'table_info':{\n                'permission_codes_string':{'type':'string','required':True},\n                'item_code':{'type':'string','required':True},\n                'created_at':{'type':'datetime','required':False},\n                'is_special_permission':{'type':'boolean','default':'False'},\n                'special_permission':{'type':'string','required':False},\n                'index':['item_code']\n                },\n            'redis_type':'not_list',\n            'data_base_type':['*'],\n            'data_server':'default',\n            'backup_db':['*'],\n            'operation':['*']\n            },\n        {\n            'model_name':'PermissionPermissionForModel',\n            'table_info':{\n                'permission_codes_string':{'type':'string','required':True},\n                'model_code':{'type':'string','required':True},\n                'created_at':{'type':'datetime','required':False},\n                'resource_url':{'type':'float','required':True},\n                'index':['model_code']\n                },\n            'redis_type':'model_list',\n            'data_base_type':['*'],\n            'data_server':'default',\n            'backup_db':['*'],\n            'operation':['*']\n            },\n        {\n            'model_name':'PermissionGroupBitsMapper',\n            'table_info':{\n                'bits_position':{'type':'int','required':False},\n                'group_code':{'type':'string','required':True},\n                'created_at':{'type':'datetime','required':False},\n                'index':['group_code']\n                },\n            'redis_type':'model_list',\n            'data_base_type':['*'],\n            'data_server':'default',\n            'backup_db':['*'],\n            'operation':['*']\n            },\n        {\n            'model_name':'PermissionGroup',\n            'table_info':{\n                'group_code':{'type':'string','required':False},\n                'created_at':{'type':'datetime','required':False},\n                'group':{'type':'string','required':True},\n                'index':['group']\n                },\n            'redis_type':'model_list',\n            'data_base_type':['*'],\n            'data_server':'default',\n            'backup_db':['*'],\n            'operation':['*']\n            },\n        {\n            'model_name':'PermissionUserGroup',\n            'table_info':{\n                'userid':{'type':'string','required':True},\n                'group_code':{'type':'string','required':False},\n                'index':['userid']\n                },\n            'redis_type':'model_list',\n            'data_base_type':['*'],\n            'data_server':'default',\n            'backup_db':['*'],\n            'operation':['*']\n            },\n        {\n            'model_name':'PermissionItemOwner',\n                'table_info':{\n                'userid':{'type':'string','required':True},\n                'item_id':{'type':'string','required':False},\n                'index':['item_id']\n                },\n                'redis_type':'not_list',\n                'data_base_type':['*'],\n                'data_server':'default',\n                'backup_db':['Postgresql'],\n                'operation':['*']\n        },\n        {\n            'model_name':'PermissionItemOwner',\n                'table_info':{\n                'userid':{'type':'string','required':True},\n                'item_id':{'type':'string','required':False},\n                'index':['item_id']\n                },\n                'redis_type':'not_list',\n                'data_base_type':['*'],\n                'data_server':'default',\n                'backup_db':['Postgresql'],\n                'operation':['*']\n        }\n                \n        \n]\n", 'created_at': '2020-11-09 12:41:54.194240', 'time_stamp': 1604905914.1942585, 'import_parameters': '{}'}
{'namespace': 'authentication_model.encrypcommunication', 'created_by': 'sys', 'source_code': '# RSA helper class for pycrypto\n# Copyright (c) Dennis Lee\n# Date 21 Mar 2017\n\n# Description:\n# Python helper class to perform RSA encryption, decryption, \n# signing, verifying signatures & keys generation\n\n# Dependencies Packages:\n# pycrypto \n\n# Documentation:\n# https://www.dlitz.net/software/pycrypto/api/2.6/\n\n### AES key generetor and sender ###\n\n\nhash = "SHA-256"\n\ndef newkeys(keysize):\n    random_generator = Random.new().read\n    key = RSA.generate(keysize, random_generator)\n    private, public = key, key.publickey()\n    return public, private\n\ndef importKey(externKey):\n    return RSA.importKey(externKey)\n\ndef getpublickey(priv_key):\n    return priv_key.publickey()\n\ndef encrypt(message, pub_key):\n    #RSA encryption protocol according to PKCS#1 OAEP\n    cipher = PKCS1_OAEP.new(pub_key)\n    return cipher.encrypt(message)\n\ndef decrypt(ciphertext, priv_key):\n    #RSA encryption protocol according to PKCS#1 OAEP\n    cipher = PKCS1_OAEP.new(priv_key)\n    return cipher.decrypt(ciphertext)\n\ndef sign(message, priv_key, hashAlg="SHA-256"):\n    global hash\n    hash = hashAlg\n    signer = PKCS1_v1_5.new(priv_key)\n    if (hash == "SHA-512"):\n        digest = SHA512.new()\n    elif (hash == "SHA-384"):\n        digest = SHA384.new()\n    elif (hash == "SHA-256"):\n        digest = SHA256.new()\n    elif (hash == "SHA-1"):\n        digest = SHA.new()\n    else:\n        digest = MD5.new()\n    digest.update(message)\n    return signer.sign(digest)\n\ndef verify(message, signature, pub_key):\n    signer = PKCS1_v1_5.new(pub_key)\n    if (hash == "SHA-512"):\n        digest = SHA512.new()\n    elif (hash == "SHA-384"):\n        digest = SHA384.new()\n    elif (hash == "SHA-256"):\n        digest = SHA256.new()\n    elif (hash == "SHA-1"):\n        digest = SHA.new()\n    else:\n        digest = MD5.new()\n    digest.update(message)\n    return signer.verify(digest, signature)\n\n\n\ndef aes_encrypt(payload, salt, key):\n    return AES.new(key, AES.MODE_CBC, salt).encrypt(r_pad(payload))\n\n\ndef aes_decrypt(payload, salt, key, length):\n    return AES.new(key, AES.MODE_CBC, salt).decrypt(payload)[:length]\n\n\ndef r_pad(payload, block_size=16):\n    length = block_size - (len(payload) % block_size)\n    return payload + chr(length) * length\n\n\n\n\ndef aes_generetor_and_send_with_rsa_public_keys():\n    m = secrets.token.bytes(32)\n    rsa_key_size  = 2048\n    rsa_key = newkeys(rsa_key_size) ### public key, private key\n\n\n\ndef main():\n    msg1 = b"Hello Tony, I am Jarvis!"\n    msg2 = b"Hello Toni, I am Jarvis!"\n\n    keysize = 2048\n\n    (public, private) = rsa.newkeys(keysize)\n\n#    # https://docs.python.org/3/library/base64.html\n#    # encodes the bytes-like object s\n#    # returns bytes\n    encrypted = b64encode(rsa.encrypt(msg1, private))\n#    # decodes the Base64 encoded bytes-like object or ASCII string s\n#    # returns the decoded bytes\n    decrypted = decrypt(b64decode(encrypted), private)\n    signature = b64encode(sign(msg1, private, "SHA-512"))\n#\n    verify1 = verify(msg1, b64decode(signature), public)\n#\n#    #print(private.exportKey(\'PEM\'))\n#    #print(public.exportKey(\'PEM\'))\n    print("Encrypted: " + encrypted.decode(\'ascii\'))\n    print("Decrypted: \'%s\'" % (decrypted))\n    print("Signature: " + signature.decode(\'ascii\'))\n    print("Verify: %s" % verify1)\n    verify(msg2, b64decode(signature), public)\n#\n#if __name__== "__main__":\n#    main()\n', 'created_at': '2020-11-09 12:41:54.197329', 'time_stamp': 1604905914.1973464, 'import_parameters': '{"Crypto.PublicKey": ["RSA", "custom"], "Crypto.Cipher": ["AES", "custom"], "Crypto.Signature": ["PKCS1_v1_5", "custom"], "Crypto.Hash": ["SHA512,", "custom"], "Crypto": ["Random", "custom"], "base64": ["b64encode,", "custom"], "rsa": ["custom"], "secrets": ["custom"]}'}
{'namespace': 'authentication_model.security_data_base', 'created_by': 'sys', 'source_code': "class SystemSignature(SimpleRedisManager):\n    table_name = 'SystemSignature'\n    pk = 'session_token'\n", 'created_at': '2020-11-09 12:41:54.199893', 'time_stamp': 1604905914.1999123, 'import_parameters': '{"service_models.data_base.database_viewsets.radis_model.radis_viewset": ["SimpleRedisManager", "custom"]}'}
{'namespace': 'authentication_model.secureity_dbclient', 'created_by': 'sys', 'source_code': "class SecurityDataBaseClient(DBClient):\n    data_type = ['redis']\n", 'created_at': '2020-11-09 12:41:54.202082', 'time_stamp': 1604905914.202102, 'import_parameters': '{"service_models.data_base.db_common_model.db_manager_client": ["", "custom"]}'}
{'namespace': 'authentication_model.security_db', 'created_by': 'sys', 'source_code': '\nclass Token(Model):\n    user = columns.Text(required=True)\n    session_token = columns.Text(required=True,primary_key = True)\n########    communication_token= columns.Text() for redis \n#class\n\n#class Signature(Model):\n#    user_ip = columns.Text(required=True, primary_key=True)\n#    system_private_key = columns.Text(required=True)\n#    user_public_key = columns.Text(required= True)\n#\n\n\nsetup([AUTHENTICAT_MODEL["DATA_BASE_STRING"]["CASSANDRA"]["IP"]], AUTHENTICAT_MODEL["DATA_BASE_STRING"]["CASSANDRA"]["KEY_SPACE"], retry_connect=True)\nsync_table(Token)\n\n', 'created_at': '2020-11-09 12:41:54.204333', 'time_stamp': 1604905914.2043505, 'import_parameters': '{"uuid": ["sys"], "cassandra.cqlengine": ["connection", "sys"], "datetime": ["datetime", "sys"], "cassandra.cqlengine.management": ["sync_table", "sys"], "cassandra.cqlengine.models": ["Model", "sys"], "cassandra.cqlengine.connection": ["setup", "sys"], "config": ["AUTHENTICAT_MODEL", "custom"]}'}
{'namespace': 'authentication_model.final', 'created_by': 'sys', 'source_code': '### Stp flows user done digital signatur------> generate and assing the authenticat token ---> generate and assing the session token -----> in logout time remove the token -----> login time reganaret regenerate the token ------> reset RSA key on reset device or password\n\ntoken_table = \n\n        {\n            \'model_name\':\'UserAuthenticationToken\',\n            \'table_info\':{\n                \'token\':{\'type\':\'string\',\'required\':True},\n                \'user\':{\'type\':\'string\',\'required\':True},\n                \'login_time\':{\'type\':\'float\',\'required\':True},\n                \'aes_key\':{\'type\':\'string\',\'required\':True},\n\n                \'index\':[\'token\']\n                },\n            \'redis_type\':\'not_list\',\n            \'data_base_type\':[\'*\'],\n            \'data_server\':\'default\',\n            \'backup_db\':[\'Postgresql\'],\n            \'operation\':[\'*\']\n            }\n\n\n\nclass CommonEncriptionOperation:\n    keysize = 32\n    def assing_session_token(self):\n        token = secrets.token_urlsafe(self.keysize)#secrets.token_bytes(self.keysize)\n\n        return token\n\n    def assing_aes_key(self):\n        return secrets.token_urlsafe(self.keysize)#secrets.token_bytes(self.keysize)\n\n\nclass AuthenticatUser(CommonEncriptionOperation):\n    rsa_key_size = 2048\n    aes_key_size = 1024\n    hash = "SHA-256"\n    table_client =SecurityDataBaseClient\n    table_client_obj = None\n    token_size = 32\n\n\n\n    def __init__(self,thread_obj,loop_index):\n        self.table_client_obj = self.table_client()\n\n    def rsa_newkeys(self):\n\n        random_generator = Random.new().read\n        key = RSA.generate(self.rsa_key_size, random_generator)\n        return key, key.publickey()\n        #return public, private\n\n    def exportkey(self,key):\n        \'\'\'Key to string converter\'\'\'\n\n        binkey =  key.exportKey(\'DER\')\n        return binkey\n\n    def importKey(self,externKey):\n        \'\'\'String to key converter\'\'\'\n\n        return RSA.importKey(externKey)\n\n    def encrypt(self,message, pub_key):\n        #RSA encryption protocol according to PKCS#1 OAEP\n        cipher = PKCS1_OAEP.new(pub_key)\n        return cipher.encrypt(message)\n\n    def decrypt(self,ciphertext, priv_key):\n        #RSA encryption protocol according to PKCS#1 OAEP\n        cipher = PKCS1_OAEP.new(priv_key)\n        return cipher.decrypt(ciphertext)\n\n    def sign(self,message, priv_key, hashAlg="SHA-256"):\n        \'\'\' Sign on the massage\'\'\'\n        global hash\n        hash = hashAlg\n        signer = PKCS1_v1_5.new(priv_key)\n        if (hash == "SHA-512"):\n            digest = SHA512.new()\n        elif (hash == "SHA-384"):\n            digest = SHA384.new()\n        elif (hash == "SHA-256"):\n            digest = SHA256.new()\n        elif (hash == "SHA-1"):\n            digest = SHA.new()\n        else:\n            digest = MD5.new()\n        digest.update(message)\n        return signer.sign(digest)\n\n    ############# User Authentication steps (web services list)  #######################\n\n\n    async def start_user_registation(self,request):\n        \'\'\' assing public key and token information to the user.\'\'\'\n        token = self.assing_session_token()\n        sys_public_key,sys_private_key = self.rsa_newkeys()\n        data = {\'sys_private_key\':self.exportkey(sys_private_key),\'session_token\':token}\n        self.table_client_obj.create(data)\n        data = {\'sys_public_key\':self.exportkey(sys_public_key),\'session_token\':token}\n        return data\n\n    async def recive_user_public_key(self,request):\n        #data = request.data\n        enrypted_data = request.data\n        token = request\n        sys_private_key = self.table_client_obj.retrive(token)[\'sys_private_key\'] ### store private_key\n        data = ujson.loads(self.decrypt(encrypted_data,private_key))\n        user_public_key = data[\'public_key\']\n        data = {\'sys_private_key\':self.exportkey(sys_private_key),\'session_token\':token,\'user_public_key\':user_public_key}\n        self.table_client_obj.create(data)\n\n    async def verify(self,request):\n        \'\'\'verify the user\'\'\'\n        token = request\n        sys_data = self.table_client_obj.retrive(token)\n        sys_private_key = sys_data[\'sys_private_key\']\n        user_public_key = sys_data[\'user_public_key\']\n\n        encrypted_data = request.data\n        data  = ujson.loads(self.decrypt(encrypted_data,private_key))\n        message = data[\'message\']\n        signature = data[\'signature\']\n        signer = PKCS1_v1_5.new(user_public_key)\n        if (hash == "SHA-512"):\n            digest = SHA512.new()\n        elif (hash == "SHA-384"):\n            digest = SHA384.new()\n        elif (hash == "SHA-256"):\n            digest = SHA256.new()\n        elif (hash == "SHA-1"):\n            digest = SHA.new()\n        else:\n            digest = MD5.new()\n        digest.update(message)\n        if signer.verify(digest, signature):\n            aes_key = self.assing_aes_key()\n            sys_data[\'aes_key\'] = aes_key\n            self.table_client_obj.create(data)\n            return \'Done\',200\n        else:\n            return \'Signature does not match\',400\n\n\n\nclass CommunicationPipe(CommonEncriptionOperation):\n    payload = "b"*16\n    rsa_key_size = 2048\n    aes_key_size = 1024\n    hash = "SHA-256"\n    table_client =SecurityDataBaseClient\n    table_client_obj = None\n    token_size = 32\n    def __init__(self,thread_obj,loop_index):\n        self.table_client_obj = self.table_client()\n\n    def aes_encrypt(self,payload, salt, key):\n        return AES.new(key, AES.MODE_CBC, salt).encrypt(r_pad(payload))\n\n\n    def aes_decrypt(self,payload, salt, key, length):\n        return AES.new(key, AES.MODE_CBC, salt).decrypt(payload)[:length]\n\n\n    def r_pad(self,payload, block_size=16):\n        length = block_size - (len(payload) % block_size)\n        return payload + chr(length) * length\n\n\n\n    async def send_data(self,data,aes_key)#,aes_key):\n        data[\'next_key\'] = secrets.token_hex(self.token_size)\n        aes_key = self.table_client_obj.retrive({\'user_token\':user_token})\n        return aes_encrypt(data,self.payload,aes_key)\n\n    async def recive_data(self,request): ### level = 0 or Non,1 or User authorization ,2 or model authorization,3 or request authorization\n        encrypted_data =request.data#,aes_key\n        user_token = None\n        if "Authorization" in req.headers:\n            user_token = request.headers["Authorization"]\n        else:\n            return False,\'Authrentication required\',400\n        sys_data = self.table_client_obj.retrive({\'user_token\':user_token})\n        if sys_data ==[]:\n            return \'Invalid user or user need to login\',400\n\n\n        data = aes_decrypt(encrypted_data,self.payload,sys_data[\'aes_key\'])\n\n        return True, data, sys_data[\'user\'], sys_data[\'aes_key\']\n', 'created_at': '2020-11-09 12:41:54.207653', 'time_stamp': 1604905914.2076757, 'import_parameters': '{"Crypto.PublicKey": ["RSA", "custom"], "Crypto.Cipher": ["AES", "custom"], "Crypto.Signature": ["PKCS1_v1_5", "custom"], "Crypto.Hash": ["SHA512,", "custom"], "Crypto": ["Random", "custom"], "base64": ["b64encode,", "custom"], "rsa": ["custom"], "secrets": ["custom"], "authentication_model": ["encrypcommunication", "custom"], "ujson": ["sys"], "authentication_model.secureity_dbclient": ["SecurityDataBaseClient", "custom"]}'}
{'namespace': 'back_end_processes.fanout_model.kafka_backend_process', 'created_by': 'sys', 'source_code': '#from base_model.config import NETWORK_INTERFACE\n#from base_model.common_models.common_api_lib import BackendProcess\n#import threading\nclass ProducerConsumerModel(BackendProcess):\n    MAXIMUM_PRODUCER = 3\n    MINIMUM_PRODUCER = 1\n    MAXIMUM_CONSUMER = 3\n    MINIMUM_CONSUMER = 0\n    #user_request_url = []\n    #request_list = []\n    STOP = False\n\n\n    loop_list = []\n    consumer_list = [] ### It will be 1d arry of [user id] ####\n\n    producer_list = {} ### It is a 2d array[loop][producer] of producer object for each loop ####\n    number_of_producer = {} ### It is a 1d array of int for count value of producer for each loop ####\n    number_of_consumer = {} ### It is a 1d array of int for count value of comsumer for each loop ####\n\n    #user_consumer = [] #### It is a array of  dic {user_id:consumer} #######\n    server_name = DEFAULT[\'KAFKA\']["BROKER"]#\'localhost:9092\'  #### default server. I will change it later ####\n    default_topic = DEFAULT[\'KAFKA\'][\'DEFAULT_TOPICS\'] #KAFKA_MODEL["DEFAULT_TOPICS"]\n    rule_fun_type = {\'system\':\'*\',\'user\':\'msg\',\'kafka_admin\':\'msg\'}\n    fun_list = [\'producer_send_msg\',\'conmuser_consume_msg\']\n    kafka_fun_list = {\'kafka\':{\'ProducerConsumerModel\':\'subscribe_topics\'},{\'ProducerConsumerModel\':\'unsubscribe_topics\'}} #{} ### {\'sub_topic\':..,\'fun\':..} {\'namespace\':{\'class\':{\'function\':function_pointer}}}\n    #allow_fun = [\'*\']\n\n\n    def __init__(self,thread_obj,loop_index):\n        self.thread_obj = thread_obj\n        self.loop_list = thread_obj.loop_list\n        self.ragister_loop(loop_index)\n        self.user_topics_client = UserTopicsListDB_client(thread_obj,loop_index)\n\n\n    def ragister_loop(self,loop_index): ##### Initialize everyone #####\n        self.producer_list[loop_index]=[] ##### {[],[],[],.....}\n        self.number_of_producer[loop_index]=0 ##### {0,0,0,....}\n        self.number_of_consumer[loop_index]=0\n        #self.user_consumer.append({})   #### [{},{},{},.......]\n\n\n    def create_consumer(self,loop_index,user_id, topic_list = None):\n        #### This function will create a consumer for each user process ####\n        loop = self.loop_list[loop_index]\n        consumer_obj = None\n        if self.number_of_consumer[loop_index] <=self.MAXIMUM_CONSUMER:\n            if topic_list is None:\n                consumer_obj = AIOKafkaConsumer(self.default_topic,loop = loop, bootstrap_servers = self.server_name,group_id = user_id)\n            else:\n                consumer_obj = AIOKafkaConsumer(topic_list,loop = loop, bootstrap_servers = self.server_name,group_id = user_id)\n            #self.user_consumer[loop_index][user_id] = consumer_obj\n            self.number_of_consumer[loop_index] += 1\n            return consumer_obj\n        else:\n            return False\n    \n\n    def register_producer(msg,topic):\n        request_json_meta_data = {"servic":"kafka","service_type":"producer","msg": msg, "topic": topic}\n        return request_json_meta_data\n\n\n    def register_consumer(user_id,topics):\n        request_json_meta_data = {"servic":"kafka","service_type":"consumer","topics": topics,"user_id":user_id}\n        return request_json_meta_data\n\n\n    #async def kafka_service(self,loop_index,task):\n    #    if coro_worker_manager.corouting_mager_obj.corouting_work_list[user_id][task_id]["status"]  !="DONE":\n    #        task = coro_worker_manager.corouting_mager_obj.corouting_work_list[user_id][task_id]\n    #        if task["service_type"] == "producer":\n    #            await self.producer_send_msg(loop_index,task["msg"],task["topic"])\n    #        if task["service_type"] == "consumer":\n    #            await self.conmuser_consume_msg(loop_index,task["user_id"],task["topics"])\n    #        if task["service_type"]=="consumer_del":\n    #            await self.consumer_del(loop_index,user_id)\n\n\n    async def subscribe_topics(self,user_id,consumer_obj,msg):\n        #### This function will update user topics list ####\n        #cache_manager.cache_manager_obj.create("user_topic_list",topics_list,user_id)\n        topics_list = await self.user_topics_client.retrive({\'user\':user_id})\n        topic = msg[\'topic\']\n        if type(topic) is list:\n            topic_list = topic_list + topic\n        else:\n            topic_list.append(topic)\n        consumer_obj.subscribe(topic_list)\n\n\n    async def unsubscribe_topics(self,user_id,consumer_obj,msg):\n        topics_list = await self.user_topics_client.retrive({\'user\':user_id})\n        topic = msg[\'topic\']\n        topic_list.remove(topic)\n        consumer_obj.subscribe(topic_list)\n\n\n    def assing_producer(self,loop_index):\n        if not self.producer_list[loop_index] == []:\n            return self.producer_list[loop_index].pop()\n        elif self.number_of_producer[loop_index] < self.MAXIMUM_PRODUCER:\n            temp_producer = AIOKafkaProducer(loop=self.loop_list[loop_index], bootstrap_servers=self.server_name)\n            self.number_of_producer[loop_index] += 1\n            return temp_producer\n        else:\n            return False\n\n\n    async def send_msg(self,msg,topic):\n        producer_obj = self.assing_producer(self.loop_index)\n        if not producer_obj:\n            return False\n        await producer_obj.start()\n        try:\n            await producer_obj.send_and_wait(topic, str.encode(msg))\n        finally:\n            # Wait for all pending messages to be delivered or expire.\n            await producer_obj.stop()\n            self.producer_list[self.loop_index].append(producer_obj)\n\n\n    async def producer_send_msg(self,loop_index,task): ######### by corouting\n        if self.STOP:\n            self.thread_obj.corouting_mager.add_pending_task_list(task_meta)\n            return False\n        msg=task[\'msg\']\n        topic = task[\'topic\']\n        producer_obj = self.assing_producer(loop_index)\n        if not producer_obj:\n            return False\n        await producer_obj.start()\n        try:\n            await producer_obj.send_and_wait(topic, str.encode(msg))\n        finally:\n            # Wait for all pending messages to be delivered or expire.\n            await producer_obj.stop()\n            self.producer_list[loop_index].append(producer_obj)\n\n\n    async def kafaka_stoping(self):\n        self.STOP = True\n\n\n    async def conmuser_consume_msg(self,loop_index,task):\n        user_id=task[\'user_id\']\n        topics = task[\'topics\']\n        self.consumer_list.append(user_id)\n        consumer_obj =self.create_consumer(loop_index,user_id,topics)#self.assing_consumer(user_id,loop_index,topics)\n        if not consumer_obj:\n            return False\n        await consumer_obj.start()\n        try:\n            async for msg in consumer_obj:\n                if self.kafka_fun_list[msg[\'namespace\']][\'class\'][\'function\'](user_id,consumer_obj,msg) is False:\n                    break\n                if self.STOP:\n                    break\n        finally:\n            await consumer_obj.stop()\n           # await consumer_obj.close()\n            self.number_of_consumer[loop_index] -=1\n            del consumer_obj\n\n\n    async def consumer_del(user_id,consumer_obj,msg):\n        if user_id == msg[\'user_id\']:\n            self.consumer_list.remove(user_id)\n            return False\n\n\nclass KafkaBackendProcessSuport(GenClass):\n    kafka_backend_class = ProducerConsumerModel\n    kafka_backend_process_obj = None\n    backend_func = None\n    allow_function = [\'*\']\n    function_dict = {\n            \'consumer_delete\':[\'init_consumer_delete\',\'socket\'],\n            \'create_consumer\':[\'init_create_consumer\',\'socket\'],\n            \'add_kafka_funcs_of_namespace\':[\'add_kafka_funcs_of_namespace\',\'socket\'],\n            \'delete_kafka_funcs\':[\'delete_kafka_funcs\',\'socket\'],\n            \'update_kafka_funcs\':[\'update_kafka_funcs\',\'socket\'],\n            \'init_kafka_backend_services\':[\'init_kafka_backend_services\',\'socket\']\n            }\n\n    def __init__(self,thread_obj,loop_index):\n        #print("Haii")\n        self.thread_obj = thread_obj\n        self.kafka_backend_process_obj = self.kafka_backend_class(thread_obj)\n        self.backend_func = self.kafka_backend_process_obj.distribut_users\n\n    \n    async def send_msg(self,request):\n        data = request.data\n        msg = data[\'msg\']\n        topic = data[\'topic\']\n        await self.kafka_backend_process_obj.send_msg(msg,topic)\n\n\n    async def init_consumer_delete(self,request):\n        data = request.data\n        msg = {\'sub_topic\':{\'fun_type\':\'system\',\'fun\':\'consumer_del\'},\'user_id\':data[\'user_id\']}\n        topic = \'system\'\n        task_meta = self.kafka_backend_process_obj.register_producer(msg,topic)\n        self.thread_obj.coroutin_manager.add_pending_task_list(task_meta)\n\n    \n    async def init_create_consumer(self,request):\n        data = request.data\n        user_id = data[\'user_id\']\n        topics = data[\'topics\']\n        task_meta = self.kafka_backend_process_obj.register_consumer(user_id,topics)\n        self.thread_obj.coroutin_manager.add_pending_task_list(task_meta)\n\n    \n    async def add_kafka_funcs_of_namespace(self,request):\n        data = request.data\n        self.add_kafka_function(data)\n\n\n    def add_kafka_function(data):\n        sub_topic = data[\'namespace\']\n        functions = data[\'functions\']\n        function_list = ujson.loads(functions.split)\n        for x in function_list:\n            tmp_namespace = getattr(import_manager_obj,sub_topic)\n            self.kafka_backend_process_obj.kafka_fun_list[sub_topic] = {sub_topic+"."+x:getattr(tmp_namespace,x)}\n\n\n    async def delete_kafka_funcs(self,request):\n        data = request.data\n        sub_topic = data[\'namespace\']\n        functions = data[\'functions\']\n        function_list = ujson.loads(functions)\n        for x in function_list:\n            #tmp_namespace = getattr(import_manager_obj,namespace)\n            del self.kafka_backend_process_obj.kafka_fun_list[sub_topic][sub_topic+"."+x]\n\n    \n    async def update_kafka_funcs(self,request):\n        data = request.data\n        self.add_kafka_function(data)\n\n    \n    async def init_kafka_backend_services(self,request):\n        broker_cluster = self.kafka_backend_class.broker_cluster\n        kafka_cluster_info = KafkaMsgMeta.objects().get(kafka_broker_cluster=broker_cluster)\n        fun_name_list =ujson.loads(kafka_cluster_info.fun_name_list) ## json array {namespace:[funtion_name]}\n        kafka_broker_list =kafka_cluster_info.kafka_broker_list ## json array\n        name_space_list = ujson.loads(kafka_cluster_info.name_space_list) ### json array [name_space]\n        for x in name_space_list:\n            data={}\n            data[\'namespace\'] = x\n            data[\'functions\'] = ujson.dumps(fun_name_list[x])\n            self.add_kafka_function(data)\n', 'created_at': '2020-11-09 12:41:54.212681', 'time_stamp': 1604905914.2127202, 'import_parameters': '{"config": ["DEFAULT,NETWORK_INTERFACE", "custom"], "common_models.common_api_lib": ["GenClass,BackendProcess", "custom"], "meta_model.container_db": ["MicroserviceResource", "custom"], "back_end_processes.fanout_model.kafka_db": ["KafkaMsgMeta", "custom"], "UserTopicsList.DB_client": ["UserTopicsListDB_client", "custom"], "ujson": ["sys"]}'}
{'namespace': 'back_end_processes.fanout_model.kafka_client_lib', 'created_by': 'sys', 'source_code': 'class KafkaServerClient:\n    method = \'post\'\n    sockect_obj = None\n    db = None\n    loop_index = None\n    data_type = [\'*\'] ## for all \'*\' or [\'cassandra\',\'redis\',\'postgresql\']\n    #db_interface_list = {} ## {\'db_type\':[\'db_type\':interface]}\n    url_header = NGINX[\'KAFKA\']\n    out_net = NETWORK_INTERFACE[\'OUT_NET\']\n\n    def __init__(self,thread_manager_obj,loop_index):\n        self.loop_index = loop_index\n        self.socket_obj = thread_manager_obj.socket_obj\n        self.thread_manager_obj = thread_manager_obj\n\n    async def send_msg(self,user,msg):\n        data = {\'user\':user,\'msg\':msg} ### {\'namespace\':\'\',\'class\':\'\',\'func\':\'\',\'msg\':\'90890890\'}\n        return await self.socket_obj.send_single_request(self.loop_index,self.out_net,self.method, "json", self.url_header+"/Kafka/"+"/send_msg/", data)\n\n    async def init_consumer_delete(self,user):\n        data = {\'user\':user}\n        return await self.socket_obj.send_single_request(self.loop_index,self.out_net,self.method, "json", self.url_header+"/Kafka/"+"/init_consumer_delete/",{\'user\':user})\n\n    async def init_create_consumer(self,user):\n        data = {\'user\':user}\n        return await self.socket_obj.send_single_request(self.loop_index,self.out_net,self.method, "json", self.url_header+"/Kafka/"+"/init_create_consumer/",{\'user\':user})\n', 'created_at': '2020-11-09 12:41:54.217161', 'time_stamp': 1604905914.2171824, 'import_parameters': '{}'}
{'namespace': 'back_end_processes.fanout_model.kafka_db', 'created_by': 'sys', 'source_code': "#from aiokafka import AIOKafkaConsumer,AIOKafkaProducer\n\n#### Loop is BigBoss for every where ############\n#### kafka manager manager producer and consumer object life cycle ###\n#### It is responcibale for data streaming to users.\n#### The producer object is created and mainted for each loop.\n####When an user want to send a massage he/she could take the produced object to send the data.#####\n#### It create equal amount of consumer for each user for each loop ####\n#### consumer object is created when user login and destroyed after user logout ###\n\n#from support.socket_manager import socket_manager_obj\n#from django_db import models\n \n#from cassandra.cqlengine import columns\n\nclass KafkaMsgMeta(Model):\n    kafka_broker_cluster = columns.Text(partition_key = True, required = True) ## cluster name by str like DEFULT \n    fun_name_list = columns.Text(partition_key = True, required = True) ## json array {namespace:[funtion_name]}\n    kafka_broker_list = columns.Text(partition_key = True, required = True) ## json array\n    name_space_list = columns.Text(required=True) ### json array [name_space]\n    extra_info = columns.Text()\n\n#setup([KAFKA_META_DATA['DATA_BASE_STRING']['CASSANDRA']['IP']],KAFKA_META_DATA['DATA_BASE_STRING']['CASSANDRA']['KEY_SPACE'] , retry_connect=True)\nsync_table(KafkaMsgMeta)\n\n#communication_methods = {'RR':{'subcribe':None,'logout':None},'OO':{'post_like':'post_like','story_like':'story_like','notification':'notifications'},'OM':{'post','story'}}##communication_db.objects.get.all()\n\n\nclass  KafkaMetadataService(GenClass):\n    allow_function =['*']\n    network_interface = None\n    function_dict= {\n            '/list':['list','get'],\n            '/create':['create','post'],\n            '/retrieve':['retrieve','get'],\n            '/update':['update','put'],\n            '/delete_kafka_broker_cluster':['delete_kafka_broker_cluster','put'],\n            #  'partial_update':['','put']\n            }\n    \n\n    async def list(self,request):\n        data = KafkaMsgMeta.objects.all()\n        data_list = []\n        for x in data:\n            data_list.append(dict(x))\n        return ujson.dumps(data_list)\n    async def retrieve(self,request): ## retrive by kafaka clust \n        pk = request['pk']\n        data = KafkaMsgMeta.objects.get(kafka_brocker_cluster = pk)\n\n        return ujson.data(dict(data))\n\n        \n\n    async def create(self,request):\n        data =request.data\n      #  \n      #          'kafka_broker_cluster':request['broker_cluster'],\n      #          'kafaka_broker_list':request['kafaka_broker_list'], ### kafka broker list of json objects\n      #          'name_space_list':request['name_space_list'], \n      #          'fun_name_list':request['fun_name_list'], ### {nanmespace:[function_name] }\n      #          'extra_info':request['extra_infor']\n      #          }\n        KafkaMsgMeta(**data).save()\n        return 'creatred'\n\n    async def update(self,request):\n        data = request.data\n       # {\n       #         'kafka_broker_cluster':request['broker_cluster'],\n       #         'kafaka_broker_list':request['kafaka_broker_list'], ### kafka broker list of json objects\n       #         'name_space_list':request['name_space_list'], \n       #         'fun_name_list':request['fun_name_list'], ### {namespace:[function_name]}\n       #         'extra_info':request['extra_infor']\n       #         }\n        KafkaMsgMeta(**data).save()\n        return 'Updated'\n#\n#        \n#\n    async def delete_kafka_broker_cluster(self,request):\n        cluster_name = request['cluster_name']\n        #broker = request['brocker']        \n        rs= KafkaMsgMeta(kafka_broker_cluster = cluster_name).delete()\n        return 'kafaka cluster is deleted successfully.'\n# \n\n#class  kafka_metadata_service(CassandraManager):\n#    keyspace =KafkaMsgMeta\n", 'created_at': '2020-11-09 12:41:54.220130', 'time_stamp': 1604905914.2201505, 'import_parameters': '{"common_models.common_api_lib": ["GenClass", "custom"], "config": ["KAFKA_META_DATA", "custom"], "uuid": ["sys"], "cassandra.cqlengine": ["columns,connection", "sys"], "datetime": ["datetime", "sys"], "cassandra.cqlengine.management": ["sync_table", "sys"], "cassandra.cqlengine.models": ["Model", "sys"], "cassandra.cqlengine.connection": ["setup", "sys"], "ujson": ["sys"]}'}
{'namespace': 'back_end_processes.fanout_model.kafka_server', 'created_by': 'sys', 'source_code': 'class KafkaMicroservice:\n    root_url = "kafaka_microservice"\n    def __init__(self,thread_obj):\n        pass\n\t\n    async def create_consumer(self,request):\n        user_id = request[\'user\']\n        topics = request[\'topics\'].split(\',\')\n#        await kafka_obj.conmuser_consume_msg(self,loop_index,user_id,topics)\n\n    async def send_msg(self,request):\n        msg = request[\'msg\']\n        topic = request[\'topic\']\n        m = kafka_obj.register_producer(msg,topic)\n #       await kafka_obj.producer_send_msg(loop_index,msg,topic)\n\n    async def kafka_stop(self,request):\n        kafka_obj.STOP = True\n', 'created_at': '2020-11-09 12:41:54.222890', 'time_stamp': 1604905914.2229083, 'import_parameters': '{}'}
{'namespace': 'back_end_processes.fanout_model.kafka_fun_namespace', 'created_by': 'sys', 'source_code': "class KafkaFuncClass:\n    service_func_list = []\n\n    def generate_sevice_fun(self):\n        class_name = type(self).__name__+'.{}'\n        func_list_dic = {}\n        for x in self.service_func_list:\n            func_list_dic[class_name.format(x)] = getattr(self,x)\n\n        return func_list_dic\n\n    def __init__(thread_obj):\n        self.thread_obj = thread_obj\n", 'created_at': '2020-11-09 12:41:54.225063', 'time_stamp': 1604905914.2250772, 'import_parameters': '{}'}
{'namespace': 'meta_model.container_db', 'created_by': 'sys', 'source_code': '#from base_model import config\n\n\n#### Container type to nginx mapper ###########\nContanerTypeToNginxMapping = {     \n    \'MANAGER\':{\'IN_NET\':\'14.0.0.2\',\'OUT_NET\':\'15.0.0.2\',\'url_type\':\'mn_con\'},\n    \'UI\':{\'IN_NET\':\'\',\'OUT_NET\':\'\',\'url_type\':\'ui_con\'}\n}\n\n\nclass KafkaMsgMeta(Model):\n    kafka_broker_cluster = columns.Text(primary_key=True, required = True) ## cluster name by str like DEFULT \n    fun_name_list = columns.Text(required = True) ## json array {namespace:{\'class_name\':[funtion_name]}}\n    kafka_broker_list = columns.Text() ## json array\n    name_space = columns.Text(partition_key=True,required=True) ### json array [name_space]\n    \n\n\nclass ServerInfo(Model):\n    container_name = columns.Text(required=True,primary_key=True)\n    containerip = columns.Text()\n    no_of_threads = columns.Integer()\n    master_port = columns.Text()\n\n#class ThreadInfo(Model):\n#    thread_url = columns.Text()\n#    container_name = columns.Text(required=True,partition_key=True)\n#    thread_port = columns.Text(required=True,partition_key = True)\n#\n#class ContainerUrlMapping(Model):\n#    container_name = columns.Text(required=True,partition_key = True)\n#    namespace = columns.Text(required=True,partition_key=True)\n\n\nclass UrlTypeProtocalMapping(Model): ## bridge information should be exist befor service has been created.\n    url_type = columns.Text(primary_key=True) ### ui\n    protocal = columns.Text()                 ### http\n\nclass UrlMethodMapping(Model): ### may not require\n    url_type = columns.Text()\n    method = columns.Text() ### not require\n    url = columns.Text(primary_key = True)\n\nclass ContainerIso(Model):\n    container_type = columns.Text(primary_key=True, max_length=255)\n    container_iso  = columns.Text(max_length=255)\n    container_dir = columns.Text()\n\nclass NetworkInterface(Model):\n    netmask = columns.Integer()\n    base_ip = columns.Text(max_length=255)\n    inter_face_type = columns.Text(max_length=255) ### UI_INNET, UI_OUTNET, MANAGER_INNET, MAN_OUT, MONITOR_NET,\n    inter_face_code = columns.Text(max_length=255, primary_key = True) # bridge name\n    inter_face_name = columns.Text(max_length=255)\n\n#class Naginx(Model):\n#    name = columns.Text() ### ui\n#    url_type = columns.Text()\n\nclass ContainerObject(Model):\n    container_name = columns.Text(primary_key= True)\n    container_type = columns.Text(max_length=255,partition_key=True)\n    status = columns.Text(max_length = 255) ####  running, not_running, stop\n\nclass ContainerObjectNetworking(Model):\n    ip_address = columns.Text(primary_key=True, max_length=50)\n    container_type = columns.Text(partition_key=True,max_length=255)\n    container_name = columns.Text(required= True,partition_key=True)\n    master_ip_address = columns.Text() ## X\n    inter_face_code = columns.Text(max_length=255) ### inter_face_type\n\nclass ContainerNetworking(Model):\n    container_type = columns.Text(max_length=255,partition_key=True)\n    inter_face_type = columns.Text(max_length=255,partition_key = True)\n\nclass MicroserviceResource(Model):\n    namespace = columns.Text(required=True,primary_key=True)\n    source_url = columns.Text()\n    sub_url = columns.Text() ###  [\'/create\',\'/update\',\'/delete\',\'/list\',\'/retrieve\']\n    source_code = columns.Text(required=True)\n    created_by = columns.Text(required=True)\n    created_at = columns.Text(required=False)\n    time_stamp = columns.Float(required=True)\n    import_parameters = columns.Text() ### import parameter list {"namespace":["model",.....]}###\n    microservice_class_name = columns.Text()\n    is_core = columns.Boolean(default=True,partition_key=True) \n    \n    #level = columns.Text() ### lvl_0 for system restart, lvl_1_1 for supervisor restart, lvl_1_2 for monitor restart, lvl_2 for supervisor microservice restart\n    container_type = columns.Text()\n    \n\nsetup([MICROSERVICE_API["MICROSERVICE_SOURCE"]["DATA_BASE_STRING"]["CASSANDRA"]["IP"]], MICROSERVICE_API["MICROSERVICE_SOURCE"]["DATA_BASE_STRING"]["CASSANDRA"]["KEY_SPACE"], retry_connect=True)\nsync_table(ContainerNetworking)\nsync_table(ContainerObject)\nsync_table(ContainerObjectNetworking)\nsync_table(NetworkInterface)\nsync_table(ContainerIso)\nsync_table(UrlTypeProtocalMapping)\nsync_table(UrlMethodMapping)\nsync_table(ServerInfo)\nsync_table(MicroserviceResource)\n', 'created_at': '2020-11-09 12:41:54.227460', 'time_stamp': 1604905914.227477, 'import_parameters': '{"uuid": ["sys"], "cassandra.cqlengine": ["columns,connection", "sys"], "datetime": ["datetime", "sys"], "cassandra.cqlengine.management": ["sync_table", "sys"], "cassandra.cqlengine.models": ["Model", "sys"], "cassandra.cqlengine.connection": ["setup", "sys"], "config": ["MICROSERVICE_API", "custom"]}'}
{'namespace': 'service_models.data_base.database_generators.cassandra_generator.cql_generator', 'created_by': 'sys', 'source_code': 'class CQLGenerator:\n    \n    __data_word = {\n            \'Text\':{\n                \'required\':\'required\',\n                \'max\':\'max_length\',\n                \'min\':\'min_length\',\n                \'default\':\'default\',\n                \'static\':\'static\',\n                \'order_by\':\'clustering_order\'\n                },\n            \'Integer\':{\n                \'required\':\'required\',\n                \'default\':\'default\',\n                \'static\':\'static\',\n                \'order_by\':\'clustering_order\'\n                },\n            \'Float\':{\n                \'required\':\'required\',\n                \'default\':\'default\',\n                \'static\':\'static\',\n                \'order_by\':\'clustering_order\'\n                },\n            \'Boolean\':{\n                \'required\':\'required\',\n                \'default\':\'default\',\n                \'static\':\'static\',\n                \'order_by\':\'clustering_order\'\n                },\n            \'DateTime\':{\n                \'required\':\'required\',\n                \'default\':\'default\',\n                \'static\':\'static\',\n                \'order_by\':\'clustering_order\'\n                },\n            \'BigInt\':{\n                \'required\':\'required\',\n                \'default\':\'default\',\n                \'static\':\'static\',\n                \'order_by\':\'clustering_order\'\n                }\n            }\n    \n    __db_type = {\'string\':\'Text\',\'int\':\'Integer\',\'float\':\'Float\',\'decimal\':\'Decimal\',\'boolean\':\'Boolean\',\'bigInt\':\'BigInt\',\'datetime\':\'DateTime\'}\n\n    def __init__(self,model_name,db_direct,data_server):\n        self.model_name = model_name\n        self.db_direct = db_direct\n        self.data_server = data_server\n    \n    def cql_str_generator(self):\n        top = \'\'\n        table_name = self.model_name+\'Cassandra\'\n        header = \'\\nclass \'+table_name+\'(Model):\'\n        body = ""\n        cluster_name = self.model_name+\'CassandraCluster\'\n        #footer = \'\\nsetup(["\'+self.data_server[\'IP\']+\'"],"\'+self.data_server[\'KEY_SPACE\']+\'", retry_connect=True)\\\n        #        \\nsync_table(\'+self.model_name+\'Cassandra)\'\n        #footer = \'\\n\\nsetup(["\'+self.data_server[\'IP\']+\'"],"\'+self.data_server[\'KEY_SPACE\']+\'", retry_connect=True)\\ \\nregister_connection("\'+cluster_name+\'", ["\'+self.data_server[\'IP\']+\'"])\\\n        footer =\'\\nsession = Cluster([\'+self.data_server+\'["IP"]]).connect(\'+self.data_server+\'["KEY_SPACE"])\\\n        \\nregister_connection("\'+cluster_name+\'", session=session)\\\n\t\\nsync_table(\'+table_name+\',connections=["\'+cluster_name+\'"])\\\n        \\ncluster_name ="\'+cluster_name+\'"\\\n        \\ntable_name = "\'+table_name+\'"\'\n        \n        index_list = None\n        l_dir = self.db_direct.copy()\n\t\n        if \'index\' in l_dir.keys():\n            index_list = l_dir[\'index\']\n            del l_dir[\'index\']\n\t\n        for x in index_list:\n            datatype = CQLGenerator.__db_type[l_dir[x][\'type\']]\n            body = body+"\\n\\t"+x+\' = columns.\'+datatype+\'(primary_key = True,\'#\'(partition_key = True,\'\n            \n            for y in CQLGenerator.__data_word[datatype]:\n                try:\n                    body = body + CQLGenerator.__data_word[datatype][y] + \' = \' + l_dir[x][y] + \',\'\n                except:\n                    pass\n            body = body + \')\'\n            del l_dir[x]\n            \t\n        for x in l_dir:\n            datatype = CQLGenerator.__db_type[l_dir[x][\'type\']]\n            body = body + "\\n\\t" + x + \' = columns.\' + datatype + \'(\'\n            \n            for y in CQLGenerator.__data_word[datatype]:\n                try:\n                    body = body + CQLGenerator.__data_word[datatype][y] + \' = \' + l_dir[x][y] + \',\'\n                except:\n                    pass\n            body = body + \')\'\n            \t\n        return top + header + body + footer\n    \n', 'created_at': '2020-11-09 12:41:54.230541', 'time_stamp': 1604905914.2305598, 'import_parameters': '{}'}
{'namespace': 'service_models.data_base.database_generators.postgres_generator.sql_generator', 'created_by': 'sys', 'source_code': 'class SQLGenerator:\n    \n    __postgresql_type = {\'string\':\'String\',\'int\':\'Integer\',\'float\':\'Float\',\'boolean\':\'Boolean\',\'bigInt\':\'BigInteger\',\'datetime\':\'DateTime\'}\n\n    def __init__(self, model_name, db_direct,data_server):\n        self.model_name = model_name\n        self.db_direct = db_direct\n        self.data_server = data_server\n    \n    def sql_str_generator(self):\n        top = \'\'\n        header = \'class \'+self.model_name+\'Postgresql(Table):\\n\'\n        #body = "\\n\\tserver="+"\'postgresql://"+self.data_server[\'USER_ID\']+":"+self.data_server[\'PASSWORD\']+"@"+self.data_server[\'IP\']+"/"+self.data_server[\'DATABASE\']+"\'"\n        body = ""\n        footer = "\\nserver = \'postgresql://\'+"+self.data_server+"[\'USER_ID\']+\':\'+"+self.data_server+"[\'PASSWORD\']+\'@\'+"+self.data_server+"[\'IP\']+\'/\'+"+self.data_server+"[\'DATABASE\']\\\n                \\nmytab = "+self.model_name+"Postgresql()\\\n                \\nmytab.server = server\\\n                \\nmytab.gen_table()"\n\n        l_dir = self.db_direct\n        \n        if \'index\' in l_dir.keys():\n            del l_dir[\'index\']\n            \n        for x in l_dir:\n            body = body + "\\n\\t" + x + \' = \' + SQLGenerator.__postgresql_type[self.db_direct[x][\'type\']] + \'( \'\n            pre = \'\'\n\t    \n            if \'max\' in l_dir[x].keys():\n                body = body + pre + \'maximum = \' + l_dir[x][\'max\']\n                pre = \', \' if pre ==\'\' else \'\'\n\t    \n            if \'min\' in l_dir[x].keys():\n                body = body + pre + \'minimum = \' + l_dir[x][\'min\']\n                pre = \', \' if pre ==\'\' else \'\'\n\t    \n            if \'max_len\' in l_dir[x].keys():\n                body = body + pre + \'maximum_length = \' + l_dir[x][\'max_len\']\n                pre = \', \' if pre ==\'\' else \'\'\n\t    \n            if \'min_len\' in l_dir[x].keys():\n                body = body + pre + \'minimum_length = \' + l_dir[x][\'min_len\']\n                pre = \', \' if pre ==\'\' else \'\'\n\t    \n            if \'default\' in l_dir[x].keys():\n                body = body + pre + \'default = \' + l_dir[x][\'default\']\n                pre = \', \' if pre ==\'\' else \'\'\n\n            body = body + \')\\n\\t\'\n\n        body += "\\n\\tserver="+"\'postgresql://\'+"+self.data_server+"[\'USER_ID\']+\':\'+"+self.data_server+"[\'PASSWORD\']+\'@\'+"+self.data_server+"[\'IP\']+\'/\'+"+self.data_server+"[\'DATABASE\']"\n       # print(body)\n\n        \n        return top + header + body + footer\n\n\n', 'created_at': '2020-11-09 12:41:54.233225', 'time_stamp': 1604905914.2332466, 'import_parameters': '{}'}
{'namespace': 'service_models.data_base.database_generators.postgres_generator.sql_creater', 'created_by': 'sys', 'source_code': "\nclass UDcolumn:\n    column_type = None\n\n    def __init__(self, maximum = None, minimum = None, allow_null = False,default=None):\n        self.allow_null= allow_null\n        self.maximum = maximum\n        self.minimum = minimum\n        self.default = default\n\n    def get_attr(self,name):\n        if self.maximum is not None:\n            rm = sqlalchemy.Column(name, type(self).column_type(self.maximum),default=self.default, nullable = self.allow_null)\n        else:\n            rm = sqlalchemy.Column(name, type(self).column_type, default = self.default,nullable = self.allow_null)\n\n        return rm\n\nclass Boolean(UDcolumn):\n    column_type = sqlalchemy.Boolean\n\n\nclass String(UDcolumn):\n    column_type = sqlalchemy.String\n    \n    def __init__(self, maximum = None, minimum = None, allow_null = False):\n        self.allow_null= allow_null\n        self.maximum = maximum\n        self.minimum = minimum\n\n    def get_attr(self,name):\n        if self.maximum is not None:\n            rm = sqlalchemy.Column(name, type(self).column_type(self.maximum), nullable = self.allow_null)\n        else:\n            rm = sqlalchemy.Column(name, type(self).column_type, nullable = self.allow_null)\n\n        return rm\n\n    \n\nclass Integer(UDcolumn):\n    column_type = sqlalchemy.Integer\n\n \nclass Float(UDcolumn):\n    column_type = sqlalchemy.Float\n\n\nclass DateTime(UDcolumn):\n    column_type = sqlalchemy.DateTime  \n\n\nclass BigInteger(UDcolumn):\n    column_type = sqlalchemy.BigInteger\n\n#class mytb(Table):\n#    m = String(True)\n#\n#m = mytb()\n#m.gen_table()\n\n\nclass Table:\n    table_name = None\n    server = None \n    default_atr = ['default_atr','server','table_name','gen_table']\n\n    def gen_table(self):\n        \n        if self.table_name is None:\n            self.table_name = type(self).__name__\n        \n        engine = sqlalchemy.create_engine(self.server)\n        connection = engine.connect()\n        metadata = sqlalchemy.MetaData()\n        \n        dirs= dir(self)\n        f_dirs = self.__filtering(dirs)\n        all_atr = [self.table_name, metadata]\n        attrs = f_dirs\n        \n        for x in attrs:\n            cl = getattr(self,x)\n            all_atr.append(cl.get_attr(x))\n        \n        l_attr = tuple(all_atr)\n        table = sqlalchemy.Table(*l_attr)\n        \n        try:\n            metadata.create_all(engine)\n        finally:\n            return table, connection,engine\n    \n        \n    def __filtering(self,atr):\n        \n        filter_list = []\n        pattern = '^(__\\w+__)$|^(_\\w+)$'\n        \n        for x in atr:\n            if not re.match(pattern,x):\n                filter_list.append(x)\n        \n        for y in self.default_atr:\n            filter_list.remove(y)\n        \n        return filter_list\n\n", 'created_at': '2020-11-09 12:41:54.235740', 'time_stamp': 1604905914.235758, 'import_parameters': '{"re": ["sys"], "sqlalchemy": ["sys"]}'}
{'namespace': 'service_models.data_base.generator.common_generator', 'created_by': 'sys', 'source_code': 'class CommonGenerator:\n    \n    \n    data_word = {\n            \'Text\':{\n                \'max\':\'max_length\',\n                \'min\':\'min_length\',\n                \'default\':\'default\',\n                \'static\':\'static\',\n                \'order_by\':\'clustering_order\'\n                },\n            \'Integer\':{\n                \'default\':\'default\',\n                \'static\':\'static\',\n                \'order_by\':\'clustering_order\'\n                },\n            \'Float\':{\n                \'default\':\'default\',\n                \'static\':\'static\',\n                \'order_by\':\'clustering_order\'\n                },\n            \'Boolean\':{\n                \'default\':\'default\',\n                \'static\':\'static\',\n                \'order_by\':\'clustering_order\'\n                },\n            \'DateTime\':{\n                \'default\':\'default\',\n                \'static\':\'static\',\n                \'order_by\':\'clustering_order\'\n                },\n            \'BigInt\':{\n                \'default\':\'default\',\n                \'static\':\'static\',\n                \'order_by\':\'clustering_order\'\n                }\n            }\n    \n    db_type = {\'string\':\'Text\',\'int\':\'Integer\',\'float\':\'Float\',\'decimal\':\'Decimal\',\'boolean\':\'Boolean\',\'bigInt\':\'BigInt\',\'datetime\':\'DateTime\'}\n\t\n    def __init__(self, model_name, db_direct):\n        self.model_name = model_name\n        self.db_direct = db_direct\n    \n    def str_generator(self):\n        top = \'from cassandra.cqlengine.models import Model,columns\\n\'\n        header = \'class \'+self.model_name+\'Cassandra(Model):\\n\'\n        body = ""\n        footer = \'\'\n        \n        index_list = None\n        l_dir = self.db_direct.copy()\n\t\n        if \'index\' in l_dir.keys():\n            index_list = l_dir[\'index\']\n            del l_dir[\'index\']\n\t\n        for x in index_list:\n            datatype = self.db_type[l_dir[x][\'type\']]\n            body = body+"\\n\\t"+x+\' = columns.\'+datatype+\'(partition_key = True,\'\n            \n            for y in self.data_word[datatype]:\n                try:\n                    body = body + self.data_word[datatype][y] + \' = \' + l_dir[x][y] + \',\'\n                except:\n                    pass\n            body = body + \')\'\n            del l_dir[x]\n            \t\n        for x in l_dir:\n            datatype = self.db_type[l_dir[x][\'type\']]\n            body = body + "\\n\\t" + x + \' = columns.\' + datatype + \'(\'\n            \n            for y in self.data_word[datatype]:\n                try:\n                    body = body + self.data_word[datatype][y] + \' = \' + l_dir[x][y] + \',\'\n                except:\n                    pass\n            body = body + \')\'\n            \t\n        return top+header+body\n \n', 'created_at': '2020-11-09 12:41:54.238637', 'time_stamp': 1604905914.238657, 'import_parameters': '{}'}
{'namespace': 'service_models.data_base.generator.db_models_generator_backup', 'created_by': 'sys', 'source_code': '\n\nclass DBGenerator:\n    import_parameter = {\n            "table":{\n                "Cassandra":{"cassandra.cqlengine.models":["Model,columns","sys"],"cassandra.cqlengine.connection":["setup","sys"],"cassandra.cqlengine.management":["sync_table","sys"]},\n                "Postgresql":{"service_models.data_base.database_generators.postgres_generator.sql_creater":["Table,String,Integer,Float,DateTime","custom"]}\n                },\n            "viewset":{\n                "Cassandra":{\n                    "service_models.data_base.database_viewsets.cassandra_model.cassandra_backend_supported_viewset":["CassandraManagerBackend,CassandraManagerForBackend","custom"],\n                    "service_models.data_base.database_viewsets.cassandra_model.cassandra_viewset":["CassandraManager","custom"]\n                    },\n                "Postgresql":{"service_models.data_base.database_viewsets.postgras_model.postgras_manager_viewset":["PostGrasManager","custom"]},\n                "Redis":{"service_models.data_base.database_viewsets.radis_model.radis_viewset":["SimpleRedisManager,RedisList","custom"]}\n                },\n            "client":{"service_models.data_base.db_common_model.db_manager_client":["DBClient","custom"]}\n            }\n    operations = [\'create\',\'update\',\'delete\',\'list\',\'retrieve\']\n\n    \n    def db_table_generator(data_dir):\n        table = {}\n        data_server = {}\n        \n        if data_dir[\'data_server\'] == \'default\':\n            data_server = config.DEFAULT \n        else:\n            data_server = data_dir[\'data_server\']\n\n\n        if data_dir[\'data_base_type\'] == [\'*\']:\n            table ={"Cassandra":"","Postgresql":""}\n            for x in table:\n                if x =="Cassandra":\n                    table[x] = CQLGenerator(data_dir[\'model_name\'],data_dir[\'table_info\'],data_server[\'CASSANDRA\']).cql_str_generator()\n                elif x =="Postgresql":\n                    table[x] = SQLGenerator(data_dir[\'model_name\'],data_dir[\'table_info\'],data_server[\'POSTGRESQL\']).sql_str_generator()\n\n        else:\n            for x in data_dir[\'data_base_type\']:\n                if x =="Cassandra":\n                    table[x] = CQLGenerator(data_dir[\'model_name\'],data_dir[\'table_info\'],data_server[\'CASSANDRA\']).cql_str_generator()\n                elif x =="Postgresql":\n                    table[x] = SQLGenerator(data_dir[\'model_name\'],data_dir[\'table_info\'],data_server[\'POSTGRESQL\']).sql_str_generator()\n        return table\n    \n    \n    def db_viewset_generator(data_dir):\n        table ={}\n        if data_dir[\'data_base_type\']==[\'*\']:\n            table ={"Cassandra":"","Redis":"","Postgresql":""}\n        else:\n            for x in data_dir[\'data_base_type\']:\n                table[x]=""\n\n        for x in table:\n            if x=="Cassandra":\n                if data_dir[\'backup_db\'] ==[\'*\']:\n                    db_backend = data_dir[\'model_name\']+"CassandraBackend"\n                    #### for backend class creation ####\n                    table[x] = "class "+db_backend+"(CassandraManagerBackend):\\n\\ttable="+data_dir[\'model_name\']+"Cassandra\\n\\n"\n                    ### for backend db viewset creation ####\n                    table[x]+="class "+data_dir[\'model_name\']+"CassandraViewset(CassandraManagerForBackend):\\n\\tbackend_class="+db_backend\n                else: \n                    table[x] = "class "+data_dir[\'model_name\']+"CassandraViewset(CassandraManager):\\n\\t"+"table="+data_dir[\'model_name\']+"Cassandra"\n            elif x=="Postgresql":\n                table[x] = "class "+data_dir["model_name"]+"PostgressqlViewset(PostGrasManager):\\n\\ttable_class="+data_dir["model_name"]+"Postgresql"\n\n            elif x =="Redis":\n                if data_dir[\'redis_type\']=="list":\n                    table[x] = "class "+data_dir["model_name"]+"RedisViewSet(SimpleRedisManager):\\n\\ttable_name=\'"+data_dir["model_name"]+"\'"\n                else:\n                    table[x] = "class "+data_dir["model_name"]+"RedisViewSet(RedisList):\\n\\ttable_name=\'"+data_dir["model_name"]+"\'"\n        return table\n\n\n    def db_client_generator(data_dir):\n        table ={}\n        if data_dir[\'data_base_type\']==[\'*\']:\n            table[\'\']=data_dir[\'model_name\']+"DBClient(DBClient):\\n\\tdata_type="+ujson.dumps([\'*\'])+"\\n\\tdb=\'"+data_dir[\'model_name\']+"\'"\n\n        else:\n            table[\'\']=data_dir[\'model_name\']+"DBClient(DBClient):\\n\\tdata_type="+ujson.dumps(data_dir[\'data_base_type\'])+"\\n\\tdb=\'"+data_dir[\'model_name\']+"\'"\n        return table\n\n   \n    def save_table(model_name,user,table,table_type,op):\n        for x in table:\n            if table_type == \'client\':\n                data = {\'namespace\':model_name+\'.\'+\'DBClient\',\'created_by\':user,\'source_code\':table[x],\'created_at\':str(datetime.now()),\'time_stamp\':time.time(),\'import_parameters\':ujson.dumps(DBGenerator.import_parameter[table_type])}\n                #data[\'source_url\'] = model_name+\'/\'+\'DBClient\'\n            \n            elif table_type == \'viewset\':\n                m = DBGenerator.import_parameter[table_type][x]\n                \n                if x != "Redis":\n                    m.update({model_name+\'.\'+x+\'_table\':[model_name+x,\'custom\']})\n                data = {\'namespace\':model_name+\'.\'+x+\'_\'+table_type,\'created_by\':user,\'source_code\':table[x],\'created_at\':str(datetime.now()),\'time_stamp\':time.time(),\'import_parameters\':ujson.dumps(m)}\n                \n                if op == [\'*\']:\n                    op = DBGenerator.operations\n                data[\'sub_url\'] = ujson.dumps(op)\n            \n            else:\n                data = {\'namespace\':model_name+\'.\'+x+\'_\'+table_type,\'created_by\':user,\'source_code\':table[x],\'created_at\':str(datetime.now()),\'time_stamp\':time.time(),\'import_parameters\':ujson.dumps(DBGenerator.import_parameter[table_type][x])}\n                data[\'source_url\'] = model_name+\'/\'+table_type\n\n            data[\'is_core\'] = False\n\n            ob = MicroserviceResource(**data)\n            ob.save()\n \n    def db_generator(data_dir,user=\'sys\'):\n        #if "redis" not in data_dir[\'data_base_type\']:\n        table = DBGenerator.db_table_generator(data_dir)\n        DBGenerator.save_table(data_dir[\'model_name\'],user,table,\'table\',data_dir[\'operation\'])\n\n        table_viewset = DBGenerator.db_viewset_generator(data_dir)\n        DBGenerator.save_table(data_dir[\'model_name\'],user,table_viewset,\'viewset\',data_dir[\'operation\'])\n        \n        table_client = DBGenerator.db_client_generator(data_dir)\n        DBGenerator.save_table(data_dir[\'model_name\'],user,table_client,\'client\',data_dir[\'operation\'])\n\n', 'created_at': '2020-11-09 12:41:54.241343', 'time_stamp': 1604905914.241362, 'import_parameters': '{"service_models.data_base.database_generators.postgres_generator.sql_generator": ["SQLGenerator", "custom"], "service_models.data_base.database_generators.cassandra_generator.cql_generator": ["CQLGenerator", "custom"], "meta_model.container_db": ["MicroserviceResource", "custom"], "base_model": ["config", "custom"], "datetime": ["datetime", "sys"], "time": ["sys"], "ujson": ["sys"]}'}
{'namespace': 'service_models.data_base.generator.serializer_generator', 'created_by': 'sys', 'source_code': 'class SerializerGenerator:\n    """\n    Input format:\n        db_direct = {\n                \'name\':{\'type\':\'string\',\'min_len\':\'0\',\'max_len\':\'255\',\'nullable\':False},\n                \'age\':{\'type\':\'int\',\'min\':\'20\',\'max\':\'50\',\'nullable\':True}\n                }\n    """\n    serializer_metadata = {\'string\':\'String\',\'int\':\'Integer\',\'float\':\'Float\',\'boolean\':\'Boolean\',\'bigInt\':\'BigInteger\',\'datetime\':\'DateTime\'}\n        \n    def __init__(self, model_name, db_direct):\n        self.model_name = model_name\n        self.db_direct = db_direct\n    \n    def serializer_str_generator(self):\n        top = \'\'\n        header = \'class \'+self.model_name+\'Serializer(Serializer):\'\n        body = ""\n        footer = \'\'\n        l_dir = self.db_direct\n        \n        if \'index\' in l_dir.keys():\n            del l_dir[\'index\']\n        \n        for x in l_dir:\n            body = body + "\\n\\t" + x + \' =  \' + self.serializer_metadata[self.db_direct[x][\'type\']] + \'(\'\n            pre = \'\'\n           \n            if \'max\' in l_dir[x].keys():\n                body = body + pre + \'maximum = \' + l_dir[x][\'max\']\n                pre = \', \' if pre ==\'\' else \'\'\n            \n            if \'min\' in l_dir[x].keys():\n                body = body + pre + \'minimum = \' + l_dir[x][\'min\']\n                pre = \', \' if pre ==\'\' else \'\'\n            \n            if \'max_len\' in l_dir[x].keys():\n                body = body + pre + \'maximum_length = \' + l_dir[x][\'max_len\']\n                pre = \', \' if pre ==\'\' else \'\'\n            \n            if \'min_len\' in l_dir[x].keys():\n                body = body + pre + \'minimum_length = \' + l_dir[x][\'min_len\']\n                pre = \',\' if pre ==\'\' else \'\'\n            \n            if \'default\' in l_dir[x].keys():\n                body = body + pre + \'default = \' + l_dir[x][\'default\']\n                pre = \',\' if pre ==\'\' else \'\'\n            \n            if \'allow_null\' in l_dir[x].keys():\n                body = body + pre + \'allow_empty = \' + l_dir[x][\'nullable\']\n                pre = \',\' if pre ==\'\' else \'\'\n            \n            body = body + \')\'\n            \n        return top+header+body\n\n', 'created_at': '2020-11-09 12:41:54.244200', 'time_stamp': 1604905914.244222, 'import_parameters': '{}'}
{'namespace': 'service_models.data_base.generator.db_models_generator', 'created_by': 'sys', 'source_code': '\n\n\n\nclass DBGenerator:\n    import_parameter = {\n            "table":{\n                "Cassandra":{"cassandra.cqlengine.models":["Model,columns","sys"],"cassandra.cqlengine.connection":["setup,register_connection","sys"],"cassandra.cqlengine.management":["sync_table","sys"],"cassandra.cluster":["Cluster","sys"],"config":[\'DEFAULT\',\'custom\']},\n                "Postgresql":{"service_models.data_base.database_generators.postgres_generator.sql_creater":["Table,String,Integer,Float,DateTime,Boolean","custom"],"config":[\'DEFAULT\',\'custom\']}\n                },\n            "viewset":{\n                "Cassandra":{\n                    "service_models.data_base.database_viewsets.cassandra_model.cassandra_backend_supported_viewset":["CassandraManagerBackend,CassandraManagerForBackend","custom"],\n                    "service_models.data_base.database_viewsets.cassandra_model.cassandra_viewset":["CassandraManager","custom"]\n                    },\n                "Postgresql":{"service_models.data_base.database_viewsets.postgras_model.postgras_manager_viewset":["PostGrasManager","custom"]},\n                "Redis":{"service_models.data_base.database_viewsets.radis_model.radis_viewset":["SimpleRedisManager,RedisList,RedisVariableManager","custom"],"config":[\'DEFAULT\',\'custom\']}\n                },\n            "client":{"service_models.data_base.db_common_model.db_manager_client":["DBClient","custom"]},\n            "serializer":{"service_models.data_base.db_common_model.serializer":["Serializer,Integer,String,Float,Email,DateTime","custom"]}\n            }\n    operations = [\'create\',\'update\',\'delete\',\'list\',\'retrieve\']\n\n    \n    def db_table_generator(data_dir):\n        data_dir = copy.deepcopy(data_dir)\n        table = {}\n        data_server = {}\n        \n        if data_dir[\'data_server\'] == \'default\':\n            \n            data_server = {\'CASSANDRA\':"DEFAULT[\'CASSANDRA\']","POSTGRESQL":"DEFAULT[\'POSTGRESQL\']"}#config.DEFAULT \n        else:\n            data_server = data_dir[\'data_server\']\n\n\n        if data_dir[\'data_base_type\'] == [\'*\']:\n            table ={"Cassandra":"","Postgresql":""}\n            for x in table:\n                if x =="Cassandra":\n                    table[x] = CQLGenerator(data_dir[\'model_name\'],data_dir[\'table_info\'],data_server[\'CASSANDRA\']).cql_str_generator()\n                elif x =="Postgresql":\n                    table[x] = SQLGenerator(data_dir[\'model_name\'],data_dir[\'table_info\'],data_server[\'POSTGRESQL\']).sql_str_generator()\n\n        else:\n            for x in data_dir[\'data_base_type\']:\n                if x =="Cassandra":\n                    table[x] = CQLGenerator(data_dir[\'model_name\'],data_dir[\'table_info\'],data_server[\'CASSANDRA\']).cql_str_generator()\n                elif x =="Postgresql":\n                    table[x] = SQLGenerator(data_dir[\'model_name\'],data_dir[\'table_info\'],data_server[\'POSTGRESQL\']).sql_str_generator()\n        return table\n    \n    \n    def db_viewset_generator(data_dir):\n        #print(data_dir)\n        table ={}\n        if data_dir[\'data_base_type\']==[\'*\']:\n            table ={"Cassandra":"","Redis":"","Postgresql":""}\n        else:\n            for x in data_dir[\'data_base_type\']:\n                table[x]=""\n\n        for x in table:\n            if x=="Cassandra":\n                if data_dir[\'backup_db\'] ==[\'*\']:\n                    db_backend = data_dir[\'model_name\']+"CassandraBackend"\n                    #### for backend class creation ####\n                    table[x] = "class "+db_backend+"(CassandraManagerBackend):\\n\\tpk=\'"+data_dir[\'table_info\'][\'index\'][0]+"\'\\n\\tcluster_name=cluster_name\\n\\ttable="+data_dir[\'model_name\']+"Cassandra\\n\\n"\n                    ### for backend db viewset creation ####\n                    table[x]+="class "+data_dir[\'model_name\']+"CassandraViewset(CassandraManagerForBackend):\\n\\tpk=\'"+data_dir[\'table_info\'][\'index\'][0]+"\'\\n\\tcluster_name =cluster_name\\n\\ttable="+data_dir[\'model_name\']+"Cassandra"+"\\n\\tbackend_class="+db_backend\n                else: \n                    table[x] = "class "+data_dir[\'model_name\']+"CassandraViewset(CassandraManager):\\n\\tpk=\'"+data_dir[\'table_info\'][\'index\'][0]+"\'\\n\\tcluster_name=cluster_name\\n\\t"+"table="+data_dir[\'model_name\']+"Cassandra"\n            elif x=="Postgresql":\n                table[x] = "class "+data_dir["model_name"]+"PostgresqlViewset(PostGrasManager):\\n\\ttable_class="+data_dir["model_name"]+"Postgresql"\n\n            elif x =="Redis":\n                db = "1"#data_dir[\'model_name\']\n                host = None\n                if data_dir[\'data_server\'] == \'default\':\n                    host = "DEFAULT[\'REDIS\'][\'IP\']" \n                else:\n                    host = data_dir[\'data_server\'][\'REDIS\'][\'IP\']\n\n                if data_dir[\'redis_type\']=="list":\n                    table[x] = "class "+data_dir["model_name"]+"RedisViewset(RedisList):\\n\\tpk=\'"+data_dir[\'table_info\'][\'index\'][0]+"\'\\n\\ttable_name=\'"+data_dir["model_name"]+"\'"\n                elif data_dir[\'redis_type\']=="variable":\n                    table[x] = "class "+data_dir["model_name"]+"RedisViewset(RedisVariableManager):\\n\\tpk=\'"+data_dir[\'table_info\'][\'index\'][0]+"\'\\n\\ttable_name=\'"+data_dir["model_name"]+"\'"\n                else:\n                    table[x] = "class "+data_dir["model_name"]+"RedisViewset(SimpleRedisManager):\\n\\tpk=\'"+data_dir[\'table_info\'][\'index\'][0]+"\'\\n\\ttable_name=\'"+data_dir["model_name"]+"\'"\n                table[x] +="\\n\\tdb=\'"+db+"\'\\n\\thost=\'"+host+"\'"\n        return table\n\n\n    def db_client_generator(data_dir):\n        table ={}\n        if data_dir[\'data_base_type\']==[\'*\']:\n            table[\'\']=\'class \'+data_dir[\'model_name\']+"DBClient(DBClient):\\n\\tdata_type="+ujson.dumps([\'*\'])+"\\n\\tdb=\'"+data_dir[\'model_name\']+"\'"\n\n        else:\n            table[\'\']=\'class \'+data_dir[\'model_name\']+"DBClient(DBClient):\\n\\tdata_type="+ujson.dumps(data_dir[\'data_base_type\'])+"\\n\\tdb=\'"+data_dir[\'model_name\']+"\'"\n        return table\n\n  \n    def db_serializer_generator(data_dir):\n        table = {}\n        table[\'\'] = SerializerGenerator(data_dir[\'model_name\'],data_dir[\'table_info\']).serializer_str_generator()\n        return table\n    \n\n    def save_table(model_name,user,table,table_type,op):\n        for x in table:\n            if table_type == \'client\':\n                data = {\'namespace\':model_name+\'.\'+\'DBClient\',\'created_by\':user,\'source_code\':table[x],\'created_at\':str(datetime.now()),\'time_stamp\':time.time(),\'import_parameters\':ujson.dumps(DBGenerator.import_parameter[table_type])}\n                #data[\'source_url\'] = model_name+\'/\'+\'DBClient\'\n            \n            elif table_type == \'serializer\':\n                data = {\'namespace\':model_name+\'.\'+\'Serializer\',\'created_by\':user,\'source_code\':table[x],\'created_at\':str(datetime.now()),\'time_stamp\':time.time(),\'import_parameters\':ujson.dumps(DBGenerator.import_parameter[table_type])}\n                \n            elif table_type == \'viewset\':\n                m = DBGenerator.import_parameter[table_type][x]\n                if x != "Redis":\n                    if x ==\'Cassandra\':\n                        m.update({model_name+\'.\'+x+\'_table\':[model_name+x+\',\'+\'cluster_name\',\'custom\']})\n                    else:\n                        m.update({model_name+\'.\'+x+\'_table\':[model_name+x,\'custom\']})\n\n\n                data = {\'namespace\':model_name+\'.\'+x+\'_\'+table_type,\'created_by\':user,\'source_code\':table[x],\'created_at\':str(datetime.now()),\'time_stamp\':time.time(),\'import_parameters\':ujson.dumps(m),\'microservice_class_name\':model_name+\'.\'+x+\'_\'+table_type+\'.ViewSet\'}\n                #print(data)\n                if op == [\'*\']:\n                    op = DBGenerator.operations\n                data[\'sub_url\'] = ujson.dumps(op)\n                \n                data[\'source_url\'] = \'/\'+model_name+\'/\'+table_type+\'/\'+x\n\n           \n            else:\n                data = {\'namespace\':model_name+\'.\'+x+\'_\'+table_type,\'created_by\':user,\'source_code\':table[x],\'created_at\':str(datetime.now()),\'time_stamp\':time.time(),\'import_parameters\':ujson.dumps(DBGenerator.import_parameter[table_type][x])}\n\n            data[\'is_core\'] =False\n                \n            ob = MicroserviceResource(**data)\n            ob.save()\n \n    def db_generator(data_dir,user=\'sys\'):\n        #if "redis" not in data_dir[\'data_base_type\']:\n        table = DBGenerator.db_table_generator(data_dir)\n        DBGenerator.save_table(data_dir[\'model_name\'],user,table,\'table\',data_dir[\'operation\'])\n\n        table_viewset = DBGenerator.db_viewset_generator(data_dir)\n        DBGenerator.save_table(data_dir[\'model_name\'],user,table_viewset,\'viewset\',data_dir[\'operation\'])\n        \n        table_client = DBGenerator.db_client_generator(data_dir)\n        DBGenerator.save_table(data_dir[\'model_name\'],user,table_client,\'client\',data_dir[\'operation\'])\n        \n        table_serializer = DBGenerator.db_serializer_generator(data_dir)\n        DBGenerator.save_table(data_dir[\'model_name\'],user,table_serializer,\'serializer\',data_dir[\'operation\'])\n\n', 'created_at': '2020-11-09 12:41:54.247615', 'time_stamp': 1604905914.2476373, 'import_parameters': '{"time": ["sys"], "ujson": ["sys"], "copy": ["sys"], "base_model": ["config", "custom"], "datetime": ["datetime", "sys"], "service_models.data_base.database_generators.postgres_generator.sql_generator": ["SQLGenerator", "custom"], "service_models.data_base.database_generators.cassandra_generator.cql_generator": ["CQLGenerator", "custom"], "service_models.data_base.generator.serializer_generator": ["SerializerGenerator", "custom"], "meta_model.container_db": ["MicroserviceResource", "custom"]}'}
{'namespace': 'service_models.data_base.db_common_model.db_manager_client', 'created_by': 'sys', 'source_code': '\n\nclass DBClient:\n    method = \'post\'\n    sockect_obj = None\n    db = None\n    loop_index = None\n    data_type = [\'*\'] ## for all \'*\' or [\'cassandra\',\'redis\',\'postgresql\']\n    #db_interface_list = {} ## {\'db_type\':[\'db_type\':interface]}\n    url_header = {\'cassandra\':NGINX[\'CASSANDRA\'],\'redis\':NGINX[\'REDIS\'],\'postgresql\':NGINX[\'POSTGRESQL\']}\n    out_net = NETWORK_INTERFACE[\'OUT_NET\']\n\n\n    def __init__(self,thread_manager_obj,loop_index):\n        self.loop_index = loop_index\n        self.socket_obj = thread_manager_obj.socket_obj\n        self.thread_manager_obj = thread_manager_obj\n\n   \n    async def list(self,data):\n        so = self.socket_obj.get_session_obj(self.loop_index,self.out_net)#self.db_interface_list[\'REDIS\'])\n\n        #so = None #self.socket_obj.get_session_obj(self.loop_index)\n        if self.data_type ==[\'*\'] or \'redis\' in self.data_type:\n            r = self.socket_obj.register_request(self.method,"json",\'http://\'+self.url_header[\'redis\']+"/"+self.db+"/viewset"+"/Redis/list",data)\n            d = await self.socket_obj.run_request(so,r)\n            if d.status == 200:\n                self.socket_obj.release_session_obj(self.loop_index,self.out_net,so)\n\n                return d.data\n        elif self.data_type==[\'*\'] or \'cassandra\' in self.data_type:\n            #so = self.socket_obj.get_session_obj(self.loop_index,self.out_net)#self.db_interface_list[\'CASSANDRA\'])\n            \n            r = self.socket_obj.register_request(self.method,"json",\'http://\'+self.url_header[\'cassandra\']+"/"+self.db+"/viewset"+"/Cassandra/list",data)\n            d = await self.socket_obj.run_request(so,r)\n            self.socket_obj.release_session_obj(self.loop_index,self.out_net,so)\n\n            return (d.data,d.status)\n        return \'Wrong Data Base Server Type\',500\n\n\n    async def retrieve(self,data):\n        so = self.socket_obj.get_session_obj(self.loop_index,self.out_net)#self.db_interface_list[\'REDIS\'])\n\n        #so = self.socket_obj.get_session_obj(self.loop_index)\n        if self.data_type ==[\'*\'] or \'redis\' in self.data_type:\n\n            r = self.socket_obj.register_request(self.method,"json",\'http://\'+self.url_header[\'redis\']+"/"+self.db+"/viewset"+"/Redis/retrive",data)\n            d = await self.socket_obj.run_request(so,r)\n            if d.status == 200:\n                self.socket_obj.release_session_obj(self.loop_index,self.out_net,so)\n\n                return d.data\n        elif self.data_type==[\'*\'] or \'cassandra\' in self.data_type:\n            so = self.socket_obj.get_session_obj(self.loop_index,self.out_net)#self.db_interface_list[\'CASSANDRA\'])\n\n            r = self.socket_obj.register_request(self.method,"json",\'http://\'+self.url_header[\'cassandra\']+"/"+self.db+"/viewset"+"/Cassandra/retrive",data)\n            d = await self.socket_obj.run_request(so,r)\n            self.socket_obj.release_session_obj(self.loop_index,self.out_net,so)\n\n            return (d.data,d.status)\n        return \'Worng Data Base Server Type\',500\n\n    async def create(self,data):\n        so = self.socket_obj.get_session_obj(self.loop_index,self.out_net)#self.db_interface_list[\'REDIS\'])\n\n        #so = self.socket_obj.get_session_obj(self.loop_index)\n        if self.data_type ==[\'*\'] or \'redis\' in self.data_type:\n\n            r = self.socket_obj.register_request(self.method,"json",\'http://\'+self.url_header[\'redis\']+"/"+self.db+"/viewset"+"/Redis/create",data)\n            d1 = await self.socket_obj.run_request(so,r)\n        elif self.data_type==[\'*\'] or \'cassandra\' in self.data_type:\n\n            r = self.socket_obj.register_request(self.method,"json",\'http://\'+self.url_header[\'cassandra\']+"/"+self.db+"/viewset"+"/Cassandra/create",data)\n            d2 = await self.socket_obj.run_request(so,r)\n        elif self.data_type==[\'*\'] or \'postgresql\' in self.data_type:\n\n            r = self.socket_obj.register_request(self.method,"json",\'http://\'+self.url_header[\'postgresql\']+"/"+self.db+"/viewset"+"/Postgrastedsql/create",data)\n            d3 = await self.socket_obj.run_request(so,r)\n            \n        self.socket_obj.release_session_obj(self.loop_index,self.out_net,so)\n\n\n        return (d1.data,d1.status)\n    \n    async def update(self,data):\n        so = self.socket_obj.get_session_obj(self.loop_index,self.out_net)#self.db_interface_list[\'REDIS\'])\n\n        if self.data_type ==[\'*\'] or \'redis\' in self.data_type:\n\n            r = self.socket_obj.register_request(self.method,"json",\'http://\'+self.url_header[\'redis\']+"/"+self.db+"/viewset"+"/Redis/update",data)\n            d1 = await self.socket_obj.run_request(so,r)\n        elif self.data_type==[\'*\'] or \'cassandra\' in self.data_type:\n\n            r = self.socket_obj.register_request(self.method,"json",\'http://\'+self.url_header[\'cassandra\']+"/"+self.db+"/viewset"+"/Cassandra/update",data)\n            d2 = await self.socket_obj.run_request(so,r)\n        elif self.data_type==[\'*\'] or \'postgresql\' in self.data_type:\n\n            r = self.socket_obj.register_request(self.method,"json",\'http://\'+self.url_header[\'postgresql\']+"/"+self.db+"/viewset"+"/Postgrastedsql/create",data)\n            d3 = await self.socket_obj.run_request(so,r)\n            \n        self.socket_obj.release_session_obj(self.loop_index,self.out_net,so)\n\n        return (d1.data,d1.status)\n    \n    async def delete(self,data):\n        so = self.socket_obj.get_session_obj(self.loop_index,self.out_net)#self.db_interface_list[\'REDIS\'])\n        if self.data_type ==[\'*\'] or \'redis\' in self.data_type:\n\n            r = self.socket_obj.register_request(self.method,"json",\'http://\'+self.url_header[\'redis\']+"/"+self.db+"/viewset"+"/Redis/delete",data)\n            d1 = await self.socket_obj.run_request(so,r)\n        elif self.data_type==[\'*\'] or \'cassandra\' in self.data_type:\n\n            r = self.socket_obj.register_request(self.method,"json",\'http://\'+self.url_header[\'cassandra\']+"/"+self.db+"/viewset"+"/Cassandra/delete",data)\n            d2 = await self.socket_obj.run_request(so,r)\n         self.socket_obj.release_session_obj(self.loop_index,self.out_net,so)\n            \n        return (d1.data,d1.status)\n\n', 'created_at': '2020-11-09 12:41:54.251361', 'time_stamp': 1604905914.251379, 'import_parameters': '{"config": ["NGINX,NETWORK_INTERFACE", "custom"]}'}
{'namespace': 'service_models.data_base.db_common_model.serializer', 'created_by': 'sys', 'source_code': "\nclass serializer_validator:\n    \n    validator_fun = None ### validators.string\n    none_value = None\n    \n    def __init__(self, allow_null):\n        self.allow_null = allow_null\n    \n    def is_valid(self, data):\n        if data is None:\n            data = type(self).none_value\n        try:\n            fun = type(self).validator_fun\n            if self.allow_null:\n                fun(data, True)\n            else:\n                fun(data, False)\n        except:\n            return False\n        return True\n\nclass String(serializer_validator):\n    \n    validator_fun = validators.string\n    none_value = ''\n    \n    def __init__(self, allow_null = False, minimum_length = 0, maximum_length = 255):\n        super().__init__(allow_null)\n        self.maximum_length = maximum_length\n        self.minimum_length = minimum_length\n            \n    def is_valid(self, data):\n        if not data:\n            data = type(self).none_value\n        try:\n            fun = type(self).validator_fun\n            if self.allow_null:\n                fun(data, True, minimum_length = self.minimum_length, maximum_length = self.maximum_length)\n            else:\n                fun(data,False, minimum_length = self.minimum_length, maximum_length = self.maximum_length)\n        except:\n            return False\n        return True\n\nclass Email(serializer_validator):\n\n    validator_fun = validators.email\n    none_value = ''\n    \n    def __init__(self, allow_null=True):\n        super().__init__(allow_null)\n\nclass Integer(serializer_validator):\n\n    validator_fun = validators.integer\n    none_value = None\n\n    def __init__(self, allow_null = False, minimum = None, maximum = None):\n        super().__init__(allow_null)\n        self.maximum = maximum\n        self.minimum = minimum\n    \n    def is_valid(self, data):\n        try:\n            fun = type(self).validator_fun\n            if self.allow_null:\n                fun(data, True, minimum = self.minimum, maximum = self.maximum)\n            else:\n                fun(data, False, minimum = self.minimum, maximum = self.maximum)\n        except:\n            return False\n        return True\n\nclass Float(Integer):\n    \n    validator_fun = validators.float\n    none_value = None\n    \n    def __init__(self, allow_null = False, minimum = None, maximum = None):\n        super().__init__(allow_null, minimum, maximum)\n\nclass DateTime(Integer):\n    \n    validator_fun = validators.datetime\n    none_value = None\n    \n    def __init__(self, allow_null = False, minimum = None, maximum = None):\n        super().__init__(allow_null, minimum, maximum)\n\n\nclass Serializer:\n    default_atr = ['data','default_atr','is_valid']\n\n    def __init__(self,data):\n        self.data = data\n    \n    def is_valid(self):\n        \n        if type(self.data) is not dict:\n            raise Exception('data must be in directory type')\n\n        dirs = dir(self)\n        f_dirs = self.__filtering(dirs)\n        temp = None\n        for x in f_dirs:\n            try:\n                temp = self.data[x]\n            except:\n                temp = None\n            if not getattr(self,x).is_valid(temp):\n                #print(self.data[x])\n                return False\n        return True\n\n        \n    def __filtering(self,atr):\n        re_list =[]\n        #print(atr)\n        pattern = 'r^(__\\w+__)$|^(_\\w+)$' \n       \n        for x in atr:\n            r = re.match(pattern,x)\n            if not r:\n                re_list.append(x)\n                \n        for y in self.default_atr:\n            re_list.remove(y)\n        return re_list\n\n", 'created_at': '2020-11-09 12:41:54.254378', 'time_stamp': 1604905914.2544014, 'import_parameters': '{"re": ["sys"], "validator_collection": ["validators", "sys"]}'}
{'namespace': 'service_models.data_base.db_common_model.db_manager_viewset', 'created_by': 'sys', 'source_code': "#from common_api_lib import GenClass\n\nclass DBManagerClass(GenClass):\n    url_type = 'manager'\n\n    function_dict= {\n            'list':['list','socket'],\n            'create':['create','socket'],\n            'retrieve':['retrieve','socket'],\n            'update':['update','socket'],\n            'destroy':['destroy','socket'],\n            'partial_update':['partial_update','socket']\n            }\n    async def list(self,request):\n        pass\n    async def retrieve(self,request):\n        pass\n    async def create(self,reuest):\n        pass\n    async def update(self,request):\n        pass\n    async def destroy(self,request):\n        pass\n    async def partial_update(self,request):\n        pass\n\n\n\n\n", 'created_at': '2020-11-09 12:41:54.256817', 'time_stamp': 1604905914.2568395, 'import_parameters': '{"common_models.common_api_lib": ["GenClass", "custom"]}'}
{'namespace': 'service_models.data_base.db_common_model.db_client', 'created_by': 'sys', 'source_code': 'async def db_manager_client(request,obj,fnc):\n    so = socket_manager_obj.get_session_obj(obj.loop_index)\n    method = \'post\'\n    r = socket_manager_obj.register_request(method,"json",obj.db+"/redis_db/"+fnc,request.data)\n    if obj.function_dict[fnc][1] == "get":\n        d = await socket_manager_obj.run_request(so,r)\n        if d.status == 200:\n            return d.data\n        else:\n            r = socket_manager_obj.register_request(method,"json",obj.db+"/cassandra_db/"+fnc,request.data)\n            d = await socket_manager_obj.run_request(so,r)\n            return (d.data,d.status)\n    else:\n        d1 = await socket_manager_obj.run_request(so,r)\n        r = socket_manager_obj.register_request(method,"json",obj.db+"/cassandra_db/"+fnc,request.data)\n        d2 = await socket_manager_obj.run_request(so,r)\n        if fnc == "create":\n            r = socket_manager_obj.register_request(method,"json",obj.db+"/postgrastedsql_db/"+fnc,request.data)\n            d3 = await socket_manager_obj.run_request(so,r)\n\n        return (d1.data,d1.status)\n\t\n', 'created_at': '2020-11-09 12:41:54.259131', 'time_stamp': 1604905914.2591498, 'import_parameters': '{}'}
{'namespace': 'service_models.data_base.database_viewsets.postgras_model.postgras_manager_viewset', 'created_by': 'sys', 'source_code': "\n#import corouting_manager_obj \n\nclass PostGrasManager(DBManagerClass,BackendProcess):\n    table_class = None\n    table = None\n    data = []\n    count = 0\n    corouting_obj = None\n    MAXIMUM_LIMIT = 10000000000000000000\n    allow_function = ['*']\n    fun_list = ['async_insert']\n    function_dict = {'create':['create','socket']}\n    is_insert = False\n    conn = None\n    need_backend = True\n    backend_func = 'async_insert'\n\n    def __init__(self,thread_obj,loop_index,task):\n        self.thread_obj = thread_obj\n        table_obj = self.table_class()\n        self.table = table_obj\n        self.corouting_manager_obj = thread_obj.corouting_manager_obj\n        self.name = type(self).__name__\n        self.corouting_manager_obj.register_service(self.name,'async_insert',self.async_insert)\n    \n    async def create(self,request):\n        #print('Haii')\n        data = request.data\n        self.data.append(data)\n        self.count += 1\n        if self.count > self.MAXIMUM_LIMIT:\n            self.is_insert = True\n            self.assign_task()\n        return 'ok'\n\n    def assign_task(self):\n        task_meta = {'service':self.name,'service_fun':'async_insert'}\n        self.corouting_manager_obj.add_pending_task_list(task_meta)\n        \n    async def async_insert(self,loop_index,meta_coro_work):\n\t#print('haii i am inserting')\n        data = self.data\n        self.data = []\n        self.count = 0\n        self.is_insert = False\n        table = self.table_class()\n        t,c,e = table.gen_table()\n        e.execute(t.insert(),data)#sql_creater.db.insert(self.table),data)\n\n", 'created_at': '2020-11-09 12:41:54.261555', 'time_stamp': 1604905914.2615755, 'import_parameters': '{"service_models.data_base.db_common_model.db_manager_viewset": ["DBManagerClass", "custom"], "common_models.common_api_lib": ["BackendProcess", "custom"]}'}
{'namespace': 'service_models.data_base.database_viewsets.redis_model.redis_count_manager', 'created_by': 'sys', 'source_code': '\nclass RedisCountManager(RedisManager):\n    \'\'\' For count related object in redis . \'\'\'\n    lookup_field = None\n    #time_varabel_name = \'timestamp\'\n    time_stamp_required = False   ### count initilize time. reset when reset call ###########\n    \n    def __init__(self,thread_obj,loop_index):\n        super().__init__(thread_obj)\n        self.function_dict.update({\'multiple_insert\':[\'multiple_insert\',\'socket\']})\n        if self.lookup_field is not None:\n            self.lookup_field  = self.lookup_field[::-1]\n\n    split_char = \',\'\n\n    async def retrieve(self,request): ### pk is come from request\n        pk = request.data[self.pk]\n        return self.select_by_id(self.table_name,pk).decode()\n\n    async def select_by_id(self,table_name, pk):\n        key = table_name+":"+pk\n        if self.semaphore: #### if not self.semaphore.locked():\n            await self.aquire() ### self.semaphore\n            values_in_byte = self.redis_connector.hget(table_name,key)\n            await self.release() ### self.semaphore\n            return values_in_byte\n        else:\n            await self.aquire() #### self.pipe_line_semaphore.aquire()\n            token = self.wait_count = self.wait_count + 1\n            self.pipeline.hget(table_name,key)  \n            values_in_byte = self.get_result_by_token(token) ## await \n            await self.release() #### self.pipe_line_semaphore.release()\n            return values_in_byte\n\n    async def select_all(self,table_name): ## select all\n        tmp_data = {}\n        if self.semaphore:\n            await self.aquire()\n            values_in_byte = self.redis_connector.hscan(table_name,0,table_name+":*")\n            await self.release()\n        else:\n            await self.aquire()\n            token = self.wait_count = self.wait_count + 1\n            self.pipeline.hscan(table_name,0,table_name+":*")\n            values_in_byte = self.get_result_by_token(token)\n            await self.release()\n\n        tmp_data = values_in_byte[1]\n        if values_in_byte[0]!=0:\n            while values_in_byte[0]!=0:\n                tmp = values_in_byte[0]\n                if self.semaphore:\n                    await self.aquire()\n                    values_in_byte = self.redis_connector.hscan(table_name, tmp, table_name + ":*")\n                    await self.release()\n                else:\n                    await self.aquire()\n                    token = self.wait_count = self.wait_count + 1\n                    self.pipeline.hscan(table_name, tmp, table_name + ":*")\n                    await self.release()\n                    values_in_byte = self.get_result_by_token(token)\n                tmp_data.update(values_in_byte[1])\n\n        return ujson.dumps(tmp_data.values())\n\n    async def list(self,request):\n        return self.select_all(self.table_name)\n\n    #async def create(self,request):\n    #    data = request.data\n    #    if is_list:\n    #        return create_list(self.table_name,data,self.lookup_field)\n    #    else:\n    #        return create_list(self.table_name,data,self.lookup_field)\n\n    async def create(self,request):\n        data = request.data\n        table = self.table_name\n        #print(type(data))\n        #return \'ok\'\n        if type(data) is str:\n            data = ujson.loads(data)\n            \n        pk = data[self.pk]#request[self.pk]\n        #string_data = ujson.dumps(data)\n        if self.lookup_field is not None:\n            for x in self.lookup_field:\n                table += \':{}\'.format(jdata[x])\n\n        if self.semaphore:\n            await self.aquire()\n            values_in_byte = self.redis_connector.hset(table,pk,0)\n            await self.release()\n            #print(values_in_byte)\n            if values_in_byte > 0:\n                return \'OK\'\n            else:\n                return \'Fail\'\n        else:\n            await self.aquire()\n            token = self.wait_count = self.wait_count + 1\n            self.pipeline.hset(table,pk,0)\n            await self.release()\n            #print(self.get_result_by_token(token))\n            if self.get_result_by_token(token) > 0:\n                return \'Ok\'\n            else:\n                return \'Fail\'\n\n\n    async def multiple_insert(self,request):\n        table_name = self.table_name\n        pk_list = request.data[\'pk_list\'].split(self.split_char)\n        data_list = request.data[\'data_list\']\n        size = len(data_list)\n        temp_dic = {}\n        for x in range(size):\n            temp_dic[table_name + ":" +pk_list[x]] = 0\n\n        if self.semaphore:\n            await self.aquire()\n            r = self.redis_connector.mset(table_name, temp_dic)\n            await self.release()\n            if r > 0:\n                return \'Ok\'\n            else:\n                return \'Fail\'\n        else:\n            await self.aquire()\n            token = self.wait_count = self.wait_count + 1\n            self.pipeline.mset(table_name, temp_dic)\n            await self.release()\n            if self.get_result_by_token(token) > 0:\n                return \'OK\'\n            else:\n                return \'Fail\'\n\n    # float_value = 1.0\n    # int_value = 1\n\n    # async def increase_by_int(self,request):\n    #     data = request.data\n    #     table = self.table_name\n    #     if type(data) is str:\n    #         data = ujson.loads(data)\n            \n    #     pk = data[self.pk]\n    #     if self.lookup_field is not None:\n    #         for x in self.lookup_field:\n    #             table += \':{}\'.format(data[x])\n    #     return await self.increase_by_value(table,pk,self.int_value)\n    #    #return await self.create(request)\n   \n    # async def increase_by_float(self,request):\n    #     table = self.table_name\n    #     data = request.data\n    #     if type(data) is str:\n    #         data = ujson.loads(data)\n           \n    #     pk = data[self.pk]\n    #     if self.lookup_field is not None:\n    #         for x in self.lookup_field:\n    #             table += \':{}\'.format(data[x])\n    #    return await self.increase_by_value(table,pk,self.float_value)\n\n    async def increase_by_value(self,request,value):\n        #HINCRBY myhash field 1\n        #print(type(data))\n        #return \'ok\'\n        \n        data = request.data\n        table = self.table_name\n        if type(data) is str:\n            data = ujson.loads(data)\n            \n        pk = data[self.pk]\n        if self.lookup_field is not None:\n            for x in self.lookup_field:\n                table += \':{}\'.format(data[x])\n\n        if self.semaphore:\n            await self.aquire()\n            values_in_byte =self.redis_connector.hincrby(table,table+\':\'+pk,value)\n            await self.release()\n            #print(values_in_byte)\n            if values_in_byte > 0:\n                return \'OK\'\n            else:\n                return \'Fail\'\n        else:\n            await self.aquire()\n            token = self.wait_count = self.wait_count + 1\n            self.pipeline.hincrby(table,table+\':\'+pk,value)\n            await self.release()\n            #print(self.get_result_by_token(token))\n            if self.get_result_by_token(token) > 0:\n                return \'Ok\'\n            else:\n                return \'Fail\'\n\n\n    async def reset(self,request):\n        #HINCRBY myhash field 1\n        #data  = request.data\n        #print(type(data))\n        #return \'ok\'\n        data = request.data\n        if type(data) is str:\n            data = ujson.loads(data)\n        \n        pk = data[self.pk]#request[self.pk]\n        if self.lookup_field is not None:\n            for x in self.lookup_field:\n                table += \':{}\'.format(jdata[x])\n\n        if self.semaphore:\n            await self.aquire()\n            values_in_byte =self.redis_connector.hset(table,table+\':\'+pk,0)\n            await self.release()\n            #print(values_in_byte)\n            if values_in_byte > 0:\n                return \'OK\'\n\n            else:\n                return \'Fail\'\n        else:\n            await self.aquire()\n            token = self.wait_count = self.wait_count + 1\n            self.pipeline.hset(table,table+\':\'+pk,0)\n            await self.release()\n            #print(self.get_result_by_token(token))\n            if self.get_result_by_token(token) > 0:\n                return \'Ok\'\n            else:\n                return \'Fail\'\n\n\n\n    async def destroy(self,request):\n        table_name = self.table_name\n        #pk = request.data[self.pk]\n        \n        data = request.data\n        if type(data) is str:\n            jdata = ujson.loads(data)\n            pk = jdata[self.pk]#request[self.pk]\n            #string_data = data\n            if self.lookup_field is not None:\n                for x in self.lookup_field:\n                    table += \':{}\'.format(jdata[x])\n\n        else:\n\n            pk = data[self.pk]#request[self.pk]\n            if self.lookup_field is not None:\n                for x in self.lookup_field:\n                    table += \':{}\'.format(jdata[x])\n\n        if self.semaphore:\n            await self.aquire()\n            r=self.redis_connector.hdel(table_name, table_name + ":" + pk)\n            await self.release()\n            if r >0:\n                return \'Ok\'\n            else:\n                return \'Fail\'\n\n        else:\n            await self.release()\n            token = self.wait_count = self.wait_count + 1\n            self.pipeline.hdel(table_name, table_name + ":" + pk)\n            await self.aquire()\n            if self.get_result_by_token(token) >0:\n                return \'Ok\'\n            else:\n                return \'Fail\'\n', 'created_at': '2020-11-09 12:41:54.264758', 'time_stamp': 1604905914.2647772, 'import_parameters': '{"ujson": ["sys"], "redis": ["sys"], "time": ["sys"], "service_models.data_base.database_viewsets.redis_model.redis_viewset": ["RedisManager", "custom"]}'}
{'namespace': 'service_models.data_base.database_viewsets.redis_model.redis_not_list_manager', 'created_by': 'sys', 'source_code': '\nclass SimpleRedisManager(RedisManager):\n\n    def __init__(self,thread_obj,loop_index):\n        super().__init__(thread_obj)\n        self.function_dict.update({\'multiple_insert\':[\'multiple_insert\',\'socket\']})\n        if self.lookup_field is not None:\n            self.lookup_field  = self.lookup_field[::-1]\n\n    splite_char = \',\'\n\n    \n    async def list(self,request):\n        return self.select_all(self.table_name)\n\n    #async def create(self,request):\n    #    data = request.data\n    #    if is_list:\n    #        return create_list(self.table_name,data,self.lookup_field)\n    #    else:\n    #        return create_list(self.table_name,data,self.lookup_field)\n\n    async def retrieve(self,request): ### pk is come from request\n        pk = request.data[self.pk]\n        return self.select_by_id(self.table_name,pk).decode()\n\n\n    def select_all(self,table_name): ## select all\n        #table_name = self.table_name\n        tmp_data = {}\n        if self.semaphore:\n            self.wait()\n            values_in_byte = self.redis_connector.hscan(table_name, 0, table_name + ":*")\n            tmp_data = values_in_byte[1]\n            #print(values_in_byte)\n            self.signal()\n        else:\n            self.wait_pipeline()\n            token = self.wait_count =self.wait_count + 1\n            self.pipeline.hscan(table_name,0,table_name+":*")\n            self.signal_pipeline()\n\n            values_in_byte =  self.get_result_by_token(token)\n            tmp_data = values_in_byte[1]\n\n        if values_in_byte[0] !=0:\n\n            while values_in_byte[0] !=0:\n                tmp =values_in_byte[0]\n                if self.semaphore:\n                    self.wait()\n                    values_in_byte = self.redis_connector.hscan(table_name, tmp, table_name + ":*")\n                    self.signal()\n                else:\n                    self.wait_pipeline()\n                    token = self.wait_count = self.wait_count + 1\n                    self.pipeline.hscan(table_name, tmp, table_name + ":*")\n                    self.signal_pipeline()\n                    values_in_byte = self.get_result_by_token(token)\n                tmp_data.update(values_in_byte[1])\n        #temp_rs = []\n        tmp_data = tmp_data.values()\n        return ujson.dumps(tmp_data)\n        #for x in  tmp_data:\n        #    temp_rs.append(ujson.loads(x))\n        #return tmp_rs\n\n    def select_by_id(self,table_name, pk):\n        key = table_name+":"+pk\n\n        if self.semaphore:\n            self.wait()\n\n\n            values_in_byte = self.redis_connector.hget(table_name, key)\n            self.signal()\n            return values_in_byte\n        else:\n            self.wait_pipeline()\n            token = self.wait_count = self.wait_count + 1\n            self.pipeline.hget(table_name, key)\n            values_in_byte = self.get_result_by_token(token)\n            self.signal_pipeline()\n\n            return values_in_byte\n\n    async def create(self,request):\n        data  = request.data\n        table_name = self.table_name\n        #print(type(data))\n        #return \'ok\'\n        if type(data) is str:\n            jdata = ujson.loads(data)\n            pk = jdata[self.pk]#request[self.pk]\n            string_data = data\n        else:\n\n            pk = data[self.pk]#request[self.pk]\n            string_data = ujson.dumps(data)\n\n\n\n        if self.semaphore:\n            self.wait()\n            values_in_byte =self.redis_connector.hset(table_name,table_name+":"+pk,string_data)\n            self.signal()\n            #print(values_in_byte)\n            if values_in_byte > 0:\n                return \'OK\'\n\n            else:\n                return \'Fail\'\n        else:\n            self.wait_pipeline()\n            token = self.wait_count = self.wait_count + 1\n            self.pipeline.hset(table_name, table_name + ":"+pk,string_data)\n            self.signal_pipeline()\n            #print(self.get_result_by_token(token))\n            if self.get_result_by_token(token) > 0:\n                return \'Ok\'\n            else:\n                return \'Fail\'\n\n\n    async def multiple_insert(self,request):\n        table_name = self.table_name\n        pk_list = request.data[\'pk_list\'].split(self.splite_char)\n        data_list = request.data[\'data_list\']\n        size = len(data_list)\n        temp_dic = {}\n        for x in range(size):\n            temp_dic[table_name + ":" +pk_list[x]] = ujson.dumps(data_list[x])\n\n        if self.semaphore:\n            self.wait()\n            r=self.redis_connector.hmset(table_name, temp_dic)\n            self.signal()\n            if r > 0:\n                return \'Ok\'\n            else:\n                return \'Fail\'\n\n        else:\n            self.wait_pipeline()\n            token = self.wait_count = self.wait_count + 1\n            self.pipeline.hmset(table_name, temp_dic)\n            self.signal_pipeline()\n\n            if self.get_result_by_token(token) > 0:\n                return \'OK\'\n            else:\n                return \'Fail\'\n    async def update(self,request):\n       return await self.create(request)\n\n    async def destroy(self,request):\n        table_name = self.table_name\n        pk = request.data[self.pk]\n        if self.semaphore:\n            self.wait()\n            r=self.redis_connector.hdel(table_name, table_name + ":" + pk)\n            self.signal()\n            if r >0:\n                return \'Ok\'\n            else:\n                return \'Fail\'\n\n        else:\n            self.signal_pipeline()\n            token = self.wait_count = self.wait_count + 1\n            self.pipeline.hdel(table_name, table_name + ":" + pk)\n            self.wait_pipeline()\n            if self.get_result_by_token(token) >0:\n                return \'Ok\'\n            else:\n                return \'Fail\'\n\n#cache_manager_obj = SimpleRedisManager()\n', 'created_at': '2020-11-09 12:41:54.268166', 'time_stamp': 1604905914.2681851, 'import_parameters': '{"ujson": ["sys"], "redis": ["sys"], "time": ["sys"], "service_models.data_base.database_viewsets.redis_model.redis_viewset": ["RedisManager", "custom"]}'}
{'namespace': 'service_models.data_base.database_viewsets.redis_model.redis_list_manager', 'created_by': 'sys', 'source_code': '\nclass RedisList(RedisManager):\n\n    #async def list(self,request):\n    #    return self.select_all(self.table_name)\n\n    #def select_all(self,table_name):\n    #    if self.semaphore:\n    #        self.wait()\n    #        values_in_byte = self.redis_connector. lrange(table_name+":*", 0, -1)\n    #        self.signal()\n    #        return ujson.loads(values_in_byte.values())\n    #    else:\n    #        self.wait_pipeline()\n    #        token = self.wait_count = self.wait_count + 1\n    #        self.pipeline.lrange(table_name+":*", 0, -1)\n    #        values_in_byte = self.get_result_by_token(token)\n    #        self.signal_pipeline()\n    #        return ujson.loads(values_in_byte.values())\n\n    function_dict = {\'create\':[\'create\',\'socket\'],\n            \'retrieve\':[\'retrieve\',\'socket\']}\n\n    def __init__(self,thread_obj,loop_index):\n        super().__init__(thread_obj)\n        self.function_dict.update({\'multiple_insert\':[\'multiple_insert\',\'socket\']})\n        if self.lookup_field is not None:\n            self.lookup_field  = self.lookup_field[::-1]\n\n    splite_char = \',\'\n\n    async def retrieve(self,request):\n        table_name = self.table_name\n        pk = request.data[self.pk]\n\n\n        if self.semaphore:\n            self.wait()\n            #print(self.redis_connector)\n\n            values_in_byte = self.redis_connector.lrange(table_name+":"+pk, 0, -1)\n\n            self.signal()\n            return ujson.dumps(values_in_byte)\n        else:\n            self.wait_pipeline()\n            token = self.wait_count = self.wait_count + 1\n            self.pipeline.lrange(table_name+":"+pk, 0, -1)\n            values_in_byte = self.get_result_by_token(token)\n            self.signal_pipeline()\n\n            return ujson.dumps(values_in_byte)\n\n    async def create(self,request):\n        table_name = self.table_name\n        data = request.data\n        pk = request.data[self.pk]\n\n        string_data_list = [ujson.dumps(data)]\n\n        #for x in data:\n        #    string_data_list.append(ujson.dumps(x))\n\n        if self.semaphore:\n            self.wait()\n            values_in_byte =self.redis_connector.lpush(table_name+":"+pk,*string_data_list)\n            self.signal()\n            y = values_in_byte\n\n        else:\n            self.wait_pipeline()\n            token = self.wait_count = self.wait_count + 1\n            self.pipeline.lpush(table_name + ":"+pk,*string_data_list)\n            self.signal_pipeline()\n            y = self.get_result_by_token(token)\n\n        if type(y) is int:\n            return \'ok\'', 'created_at': '2020-11-09 12:41:54.271046', 'time_stamp': 1604905914.2710679, 'import_parameters': '{"ujson": ["sys"], "redis": ["sys"], "time": ["sys"], "service_models.data_base.database_viewsets.redis_model.redis_viewset": ["RedisManager", "custom"]}'}
{'namespace': 'service_models.data_base.database_viewsets.redis_model.redis_variable_manager', 'created_by': 'sys', 'source_code': '\nclass RedisVariableManager(RedisManager):\n    \'\'\' For store variable. \'\'\'\n\n    time_varabel_name = \'timestamp\'\n    \n    def __init__(self,thread_obj,loop_index):\n        super().__init__(thread_obj)\n        self.function_dict.update({\'multiple_insert\':[\'multiple_insert\',\'socket\']})\n        if self.lookup_field is not None:\n            self.lookup_field  = self.lookup_field[::-1]\n\t\n    splite_char = \',\'\n\n\n    #async def create(self,request):\n    #    data = request.data\n    #    if is_list:\n    #        return create_list(self.table_name,data,self.lookup_field)\n    #    else:\n    #        return create_list(self.table_name,data,self.lookup_field)\n\n    async def retrieve(self,request): ### pk is come from request\n        pk = request.data[self.pk]\n        return self.select_by_id(self.table_name,pk).decode()\n\n\n\n    def select_by_id(self,table_name, pk):\n        key = table_name+":"+pk\n        if self.semaphore:\n            self.wait()\n            values_in_byte = self.redis_connector.hget(table_name,key)\n            self.signal()\n            return values_in_byte\n        else:\n            self.wait_pipeline()\n            token = self.wait_count = self.wait_count + 1\n            self.pipeline.hget(table_name,key)\n            values_in_byte = self.get_result_by_token(token)\n            self.signal_pipeline()\n\n            return values_in_byte\n\n\n\n    def select_all(self,table_name): ## select all\n        #table_name = self.table_name\n        tmp_data = {}\n        if self.semaphore:\n            self.wait()\n            values_in_byte = self.redis_connector.hscan(table_name, 0, table_name + ":*")\n            tmp_data = values_in_byte[1]\n            #print(values_in_byte)\n            self.signal()\n        else:\n            self.wait_pipeline()\n            token = self.wait_count =self.wait_count + 1\n            self.pipeline.hscan(table_name,0,table_name+":*")\n            self.signal_pipeline()\n\n            values_in_byte =  self.get_result_by_token(token)\n            tmp_data = values_in_byte[1]\n\n        if values_in_byte[0] !=0:\n\n            while values_in_byte[0] !=0:\n                tmp =values_in_byte[0]\n                if self.semaphore:\n                    self.wait()\n                    values_in_byte = self.redis_connector.hscan(table_name, tmp, table_name + ":*")\n                    self.signal()\n                else:\n                    self.wait_pipeline()\n                    token = self.wait_count = self.wait_count + 1\n                    self.pipeline.hscan(table_name, tmp, table_name + ":*")\n                    self.signal_pipeline()\n                    values_in_byte = self.get_result_by_token(token)\n                tmp_data.update(values_in_byte[1])\n        #temp_rs = []\n        tmp_data = tmp_data.values()\n        return ujson.dumps(tmp_data)\n\n    async def list(self,request):\n        return self.select_all(self.table_name)\n\n\n\n\n    async def create(self,request):\n        data  = request.data\n        table_name = self.table_name\n        #print(type(data))\n        #return \'ok\'\n        if type(data) is str:\n            jdata = ujson.loads(data)\n            pk = jdata[self.pk]#request[self.pk]\n            #string_data = data\n        else:\n\n            pk = data[self.pk]#request[self.pk]\n            #string_data = ujson.dumps(data)\n\n\n        if self.semaphore:\n            self.wait()\n            values_in_byte =self.redis_connector.hset(table,pk,0)\n            self.signal()\n            #print(values_in_byte)\n            if values_in_byte > 0:\n                return \'OK\'\n\n            else:\n                return \'Fail\'\n        else:\n            self.wait_pipeline()\n            token = self.wait_count = self.wait_count + 1\n            self.pipeline.hset(table,pk,0)\n            self.signal_pipeline()\n            #print(self.get_result_by_token(token))\n            if self.get_result_by_token(token) > 0:\n                return \'Ok\'\n            else:\n                return \'Fail\'\n\n\n    async def multiple_insert(self,request):\n        table_name = self.table_name\n        pk_list = request.data[\'pk_list\'].split(self.splite_char)\n        data_list = request.data[\'data_list\']\n        size = len(data_list)\n        temp_dic = {}\n        for x in range(size):\n            temp_dic[table_name + ":" +pk_list[x]] = 0\n\n        if self.semaphore:\n            self.wait()\n            r=self.redis_connector.mset(table_name, temp_dic)\n            self.signal()\n            if r > 0:\n                return \'Ok\'\n            else:\n                return \'Fail\'\n\n        else:\n            self.wait_pipeline()\n            token = self.wait_count = self.wait_count + 1\n            self.pipeline.mset(table_name, temp_dic)\n            self.signal_pipeline()\n\n            if self.get_result_by_token(token) > 0:\n                return \'OK\'\n            else:\n                return \'Fail\'\n\n    float_value = 1.0\n    int_value = 1\n\n\n    async def increase_by_int(self,request):\n        data = request.data\n        if type(data) is str:\n            jdata = ujson.loads(data)\n            pk = jdata[self.pk]#request[self.pk]\n            #string_data = data\n        else:\n\n            pk = data[self.pk]#request[self.pk]\n            #string_data = ujson.dumps(data)\n        return await self.increase_by_value(pk,self.int_value)\n\n       #return await self.create(request)\n   async def increase_by_float():\n       data = request.data\n       if type(data) is str:\n           jdata = ujson.loads(data)\n           pk = jdata[self.pk]#request[self.pk]\n           #string_data = data\n       else:\n\n           pk = data[self.pk]#request[self.pk]\n           #string_data = ujson.dumps(data)\n       return await self.increase_by_value(pk,self.float_value)\n\n   async def increase_by_value(self,pk,value):\n\n       #HINCRBY myhash field 1\n\n       #data  = request.data\n       table_name = self.table_name\n       #print(type(data))\n       #return \'ok\'\n\n\n\n       if self.semaphore:\n           self.wait()\n           values_in_byte =self.redis_connector.hincrby(table,table+\':\'+pk,value)\n           self.signal()\n           #print(values_in_byte)\n           if values_in_byte > 0:\n               return \'OK\'\n\n           else:\n               return \'Fail\'\n       else:\n           self.wait_pipeline()\n           token = self.wait_count = self.wait_count + 1\n           self.pipeline.hincrby(table,table+\':\'+pk,value)\n           self.signal_pipeline()\n           #print(self.get_result_by_token(token))\n           if self.get_result_by_token(token) > 0:\n               return \'Ok\'\n           else:\n               return \'Fail\'\n\n\n    async def reset(self,request):\n        #HINCRBY myhash field 1\n\n        #data  = request.data\n\n        #print(type(data))\n        #return \'ok\'\n        data = request.data\n        if type(data) is str:\n            jdata = ujson.loads(data)\n            pk = jdata[self.pk]#request[self.pk]\n            #string_data = data\n        else:\n\n            pk = data[self.pk]#request[self.pk]\n\n        if self.semaphore:\n            self.wait()\n            values_in_byte =self.redis_connector.hset(table,table+\':\'+pk,0)\n            self.signal()\n            #print(values_in_byte)\n            if values_in_byte > 0:\n                return \'OK\'\n\n            else:\n                return \'Fail\'\n        else:\n            self.wait_pipeline()\n            token = self.wait_count = self.wait_count + 1\n            self.pipeline.hset(table,table+\':\'+pk,0)\n            self.signal_pipeline()\n            #print(self.get_result_by_token(token))\n            if self.get_result_by_token(token) > 0:\n                return \'Ok\'\n            else:\n                return \'Fail\'\n\n\n\n    async def destroy(self,request):\n        table_name = self.table_name\n        pk = request.data[self.pk]\n        if self.semaphore:\n            self.wait()\n            r=self.redis_connector.hdel(table_name, table_name + ":" + pk)\n            self.signal()\n            if r >0:\n                return \'Ok\'\n            else:\n                return \'Fail\'\n\n        else:\n            self.signal_pipeline()\n            token = self.wait_count = self.wait_count + 1\n            self.pipeline.hdel(table_name, table_name + ":" + pk)\n            self.wait_pipeline()\n            if self.get_result_by_token(token) >0:\n                return \'Ok\'\n            else:\n                return \'Fail\'\n', 'created_at': '2020-11-09 12:41:54.273759', 'time_stamp': 1604905914.2737746, 'import_parameters': '{"ujson": ["sys"], "redis": ["sys"], "time": ["sys"], "service_models.data_base.database_viewsets.redis_model.redis_viewset": ["RedisManager", "custom"]}'}
{'namespace': 'service_models.data_base.database_viewsets.redis_model.redis_viewset', 'created_by': 'sys', 'source_code': '\n\nclass RedisManager(DBManagerClass):\n    host = None\n    port = 32770\n    db=None\n    #password = None\n    #Redis_DB = None\n    is_list = False\n    table_name = None\n    pk = "id"\n\n    #connector = None redis.Redis(host=u\'\'+host, port=port, db=db, password=password)\n\n    redis_connector = None ##REDIS_CONNECTOR #### As redis is single thread\n\n    pipeline = None\n    result = None\n    wait_count = -1\n\n    pipe_line_semaphore = None ### redis pipe semaphore\n\n    """\n    > This is pipline semaphore to handle access pipeline objects of redis\n    > It is a binary semaphore\n        > True for signal for the resource\n        > False for wait for the resource\n    > Redis is a single threaded server. So if the resource is scheduled for an operation, all other operations are blocked\n    > Blocked operations are stored in pipeline list and are rescheduled when server is available again\n    """\n    semaphore = None  ### server semaphore \n\n    def __init__(self,thread_obj,loop_index):\n        #print(\'Haii\')\n        h = u\'\'+self.host\n        #self.table_name = table\n        #self.redis_connector = redis_connector\n        loop = thread_obj.loop_list[loop_index]\n        self.redis_connector =redis.Redis(host = h,port = self.port, db=1) #redis.Redis(host = u\'\'+self.host,port = self.port, db=self.db)\n        self.pipeline = self.redis_connector.pipeline()\n        self.semaphore = asyncio.Semaphore(1,loop = loop)\n        self.pipe_line_semaphore = asyncio.Semaphore(1,loop = loop)\n\n        #m = self.redis_connector.lrange(\'t01:01\', 0, -1)\n        #print(m)\n\n    """\n    As redis is a single thread server we need to manage redis connection so that for each blocked operation\n    pipeline will be used to store the requested operation\n\n\n    """\n    is_processing = False\n    multi_pip = False\n\n    async def get_result_by_token(self,token):\n        \n        while self.result is not None:\n            continue\n\n\n        while not self.is_processing:\n            if self.semaphore.locked:\n                continue\n            else:\n                await self.semaphore.acquire()\n                await self.pipe_line_semaphore.acquire()\n                self.temp_wait_count = self.wait_count\n                self.wait_count = -1\n                \n                break\n \n       \n        \n        self.temp_wait_count = self.temp_wait_count -1 ##reduce waiting list\n        if self.result is None:\n            \n    \n            self.result = self.pipeline.execute() ##initialize result\n            await self.semaphore.release() ##allow other to enter in sharing section\n            await self.pipe_line_semaphore.release()\n            self.is_processing = True\n\n        if self.temp_wait_count == -1:\n            self.is_processing = False\n            r = self.result[token]\n            self.result = None\n            ###await self.pipe_line_semaphore.release()\n            return r\n\n        return self.result[token]\n\n', 'created_at': '2020-11-09 12:41:54.276271', 'time_stamp': 1604905914.2762878, 'import_parameters': '{"ujson": ["sys"], "redis": ["sys"], "time": ["sys"], "service_models.data_base.db_common_model.db_manager_viewset": ["DBManagerClass", "custom"]}'}
{'namespace': 'service_models.data_base.database_viewsets.cassandra_model.cassandra_viewset', 'created_by': 'sys', 'source_code': '\n\nclass CassandraManager(DBManagerClass):\n    #network_interface = ""\n    lookup_field =None \n    table = None\n    order_field = None\n    pk = \'id\'\n    lookup_field = [pk]\n    separator = \',\'\n    need_backend = False\n    cluster_name = None\n\n    async def converter(self,cassandra_data):\n        out_data  = []\n        for x in cassandra_data:\n            out_data.append(dict(x))\n        return ujson.dumps(out_data)\n\n\n    async def list(self,request):\n        cassandra_data = self.table.using(connection=self.cluster_name).all()\n        \n        return await self.converter(cassandra_data)\n\n\n        #if self.order_field is None:\n        #    cassendra_data = self.table.objects.filter(**{self.lookup_field:request[self.lookup_field]}).order_by(\'-\' + self.order_field)\n        #else:\n        #    cassendra_data = self.table.objects.filter(**{self.lookup_field:request[self.lookup_field]})\n        #return cassendra_data\n\n    async def retrieve(self,request): ## pk come from request\n        if self.lookup_field is not None:\n            try:\n                cassandra_data = self.table.using(connection=self.cluster_name).filter(**data)\n            except:\n                cassandra_data = self.table.using(connection=self.cluster_name).filter(**data).allow_filtering()\n                \n            return await self.converter(cassandra_data)\n        else:\n            try:\n                data = request.data\n                pk = data[self.pk]\n                cassandra_data = self.table.using(connection=self.cluster_name).get(**{self.pk:pk})\n                data = dict(cassandra_data)\n            except:\n                data = request.data\n                pk = data[self.pk]\n                cassandra_data = self.table.using(connection=self.cluster_name).get(**{self.pk:pk}).allow_filtering()\n                data = dict(cassandra_data)\n                \n            return ujson.dumps(data)\n\n\n    async def multiple_selection(self,request): ## pklist come from request\n        data = request.data\n        pk_list = data[self.pk]\n        pks = pk_list.split(self.separator)\n        try:\n            cassandra_data = self.table.using(connection=self.cluster_name).filter(**{self.pk+\'__in\': pks})\n\n        except:\n            \n            cassandra_data = self.table.using(connection=self.cluster_name).filter(**{self.pk+\'__in\': pks})\n\n\n        return await self.converter(cassandra_data)\n        \n    async def create(self,request):\n        data = request.data\n        try:\n            self.table.using(connection=self.cluster_name).save(**data)\n            return \'Created Successfully\'\n        except:\n            return \'Fail to create\'\n\n    async def update(self,request): ### pk come from request\n        data = request.data\n        try:\n            self.table.using(connection=self.cluster_name).save(**data)\n            return \'Update successfull\'\n        except:\n            return \'Fail to successfully update\'\n\n    async def partial_update(self,request): ### pk come from request\n        #data = request.data\n        return await self.update(request) \n\n\n    async def destroy(self,request): ### pk come from request obj\n        pk = request.data[self.pk]\n        c = 0\n        if lookup_field is not None:\n            try:\n                rs = self.table.objects.using(connection=self.cluster_name).filter(**{self.pk:pk})\n                c = 0\n                for x in rs:\n                    x.delete()\n                    c+=1\n            except:\n                rs = self.table.objects.using(connection=self.cluster_name).filter(**{self.pk:pk}).allow_filtering()\n                c = 0\n                for x in rs:\n                    x.delete()\n                    c+=1\n        else:\n            try:\n                rs = self.table.objects.using(connection=self.cluster_name).filter(**{self.pk:pk})\n                c=0\n                for x in rs:\n                    x.delete()\n                    c+=1\n            except:\n                pass\n                    \n        if c>0:\n            return \'Delete record succefull\'\n        else:\n            return \'Fail to delete\'\n\n\n', 'created_at': '2020-11-09 12:41:54.278669', 'time_stamp': 1604905914.2786858, 'import_parameters': '{"ujson": ["sys"], "service_models.data_base.db_common_model.db_manager_viewset": ["DBManagerClass", "custom"]}'}
{'namespace': 'service_models.data_base.database_viewsets.cassandra_model.cassandra_backend_supported_viewset', 'created_by': 'sys', 'source_code': '\nclass CassandraManagerForBackend(CassandraManager): #### for microservice support ####\n    need_backend = True\n    backend_class = None\n    #cluster_name = None\n    task_name = None\n    \n    def __init__(self,thread_obj,loop_index,task):\n        self.thread_obj = thread_obj\n        self.corouting_manager_obj = thread_obj.corouting_manager_obj\n        #if self.backend_class is None:\n        #    self.name = type(self).__name__+\'Backend\'\n        #else:\n        #    self.name = self.backend_class.__name__\n        obj = self.backend_class.__name__()\n        services = obj.get_services()\n        self.task_name = task[\'namespace\']\n\n        self.corouting_manager_obj.register_service(task[\'namespace\'],services)\n            \n    async def create(self,request):\n        task_meta = {\'service\':self.task_name,\'service_fun\':\'create\',\'data\':request.data}\n        self.thread_obj.corouting_manager_obj.add_pending_task_list(task_meta)\n        return \'OK\'\n\n    async def update(self,request): ### pk come from request\n        task_meta = {\'service\':self.task_name,\'service_fun\':\'update\',\'data\':request.data}\n        self.thread_obj.corouting_manager_obj.add_pending_task_list(task_meta)\n        return \'OK\'\n\n    async def partial_update(self,request): ### pk come from reques\n        task_meta = {\'service\':self.task_name,\'service_fun\':\'update\',\'data\':request.data}\n        self.thread_obj.corouting_manager_obj.add_pending_task_list(task_meta)\n        return \'OK\'\n        #data[self.pk] = pk\n        #return self.keyspace.save(data)\n\n    async def destroy(self,request): ### pk come from request obj\n        task_meta = {\'service\':self.task_name,\'service_fun\':\'destroy\',\'data\':request.data}\n        self.thread_obj.corouting_manager_obj.add_pending_task_list(task_meta)\n        return \'OK\'\n        \n\n    \n\n#cassandra_manager_obj = CassandraManager()\n\nclass CassandraManagerBackend(BackendProcess): #### for backend support\n    #network_interface = ""\n    lookup_field = "user"\n    table = None\n    cluster_name = None\n    order_field = None\n    pk = \'id\'\n    separator = \',\'\n    fun_list = [\'create\',\'update\',\'destroy\',\'partial_update\']\n    allow_fun = [\'*\']\n    \n    #def __init__(self,thread_obj):\n    #    self.thread_obj = thread_obj\n            \n    \n    async def create(self,loop_index,metadata):\n        data = metadata[\'data\']\n        obj = self.table(**data)\n        return obj.using(connection=self.cluster_name).save()\n\n    async def update(self,loop_index,metadata): ### pk come from request\n        data =  metadata[\'data\']\n        obj = self.table(**data)\n        return self.obj.using(connection=self.cluster_name).save()\n\n    async def partial_update(self,loop_index,metadata): ### pk come from request\n        data = metadata[\'data\']\n        #data[self.pk] = pk\n        obj = self.table(**data)\n        return obj.using(connection=self.cluster_name).save()\n\n    async def destroy(self,loop_index,metadata): ### pk come from request obj\n        data = metadata[\'data\']\n        #print(data)\n        pk = data[self.pk]\n        self.table.objects(**{self.pk:pk}).using(connection=self.cluster_name).delete()\n        \n        return \n\n', 'created_at': '2020-11-09 12:41:54.281133', 'time_stamp': 1604905914.28115, 'import_parameters': '{"service_models.data_base.database_viewsets.cassandra_model.cassandra_viewset": ["CassandraManager", "custom"], "common_models.common_api_lib": ["BackendProcess", "custom"]}'}
